# Diffusion-Based Neural Network Weights Generation
Bedionita Soro, Bruno Andreis, Hayeon Lee, Wonyong Jeong, Song Chong, Frank Hutter, Sung Ju Hwang
[text](https://arxiv.org/abs/2402.18153)
Transfer learning has gained significant attention in recent deep learning research due to its ability to accelerate convergence and enhance performance on new tasks. However, its success is often contingent on the similarity between source and target data, and training on numerous datasets can be costly, leading to blind selection of pretrained models with limited insight into their effectiveness. To address these challenges, we introduce D2NWG, a diffusion-based neural network weights generation technique that efficiently produces high-performing weights for transfer learning, conditioned on the target dataset. Our method extends generative hyper-representation learning to recast the latent diffusion paradigm for neural network weights generation, learning the weight distributions of models pretrained on various datasets. This allows for automatic generation of weights that generalize well across both seen and unseen tasks, outperforming state-of-the-art meta-learning methods and pretrained models. Moreover, our approach is scalable to large architectures such as large language models (LLMs), overcoming the limitations of current parameter generation techniques that rely on task-specific model collections or access to original training data. By modeling the parameter distribution of LLMs, D2NWG enables task-specific parameter generation without requiring additional fine-tuning or large collections of model variants. Extensive experiments show that our method consistently enhances the performance of diverse base models, regardless of their size or complexity, positioning it as a robust solution for scalable transfer learning.

迁移学习因其能够加速收敛并提升新任务性能，在近年深度学习研究中备受关注。然而，其成功往往依赖于源数据与目标数据之间的相似性，且在多数据集上训练成本高昂，导致预训练模型的选择盲目，对其有效性缺乏深入理解。为解决这些挑战，我们提出了 D2NWG，一种基于扩散的神经网络权重生成技术，它能够高效地为迁移学习生成高性能权重，且以目标数据集为条件。我们的方法扩展了生成式超表示学习，重新构建了神经网络权重生成的潜在扩散范式，学习在不同数据集上预训练模型的权重分布。这使得自动生成的权重在已见及未见任务上均能良好泛化，超越了最先进的元学习方法及预训练模型。 此外，我们的方法可扩展至大型架构，如大型语言模型（LLMs），克服了当前依赖于任务特定模型集合或原始训练数据访问的参数生成技术的局限性。通过建模LLMs的参数分布，D2NWG 实现了无需额外微调或大量模型变体集合的任务特定参数生成。大量实验表明，无论基础模型的规模或复杂性如何，我们的方法均能持续提升其性能，使其成为可扩展迁移学习的稳健解决方案。
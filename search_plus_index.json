{"./":{"url":"./","title":"Introduction","keywords":"","body":"My Notes for nearly Everything, by hhw Learn: Record learning resources, environment configuration, common questions and precautions when learning something or a certain skill Linux: Linux related notes, including Linux distribution, display, user management, input method, etc. PythonLibs - Programming: Programming related notes, about design patterns and so on. ReadingNotes: Personal reading notes, including books and articles. Network: Network related notes, including network protocols, network architecture, etc. Papers: Papers I read and want to remember. ConfigExamples: Configuration examples for different tools and editors. Git: Git related notes, including basic usage, branch management, etc. Misc: Miscellaneous notes, uncategorized notes. ⭐ If you think this repository is helpful to you, please give me a star! 😚 "},"Learn/Learn.html":{"url":"Learn/Learn.html","title":"Learn","keywords":"","body":"Learn Notes "},"Learn/LearnAIMA/":{"url":"Learn/LearnAIMA/","title":"Learn AIMA","keywords":"","body":"第一部分 人工智能基础 第 1 章 绪论 1.1 什么是人工智能 1.1.1 类人行为：图灵测试方法 1.1.2 类人思考：认知建模方法 1.1.3 理性思考：“思维法则”方法 1.1.4 理性行为：理性智能体方法 1.1.5 益机 1.2 人工智能的基础 1.2.1 哲学 1.2.2 数学 1.2.3 经济学 1.2.4 神经科学 1.2.5 心理学 1.2.6 计算机工程 1.2.7 控制理论与控制论 1.2.8 语言学 1.3 人工智能的历史 1.3.1 人工智能的诞生（1943—1956） 1.3.2 早期热情高涨，期望无限（1952—1969） 1.3.3 一些现实（1966—1973） 1.3.4 专家系统（1969—1986） 1.3.5 神经网络的回归（1986—现在） 1.3.6 概率推理和机器学习（1987—现在） 1.3.7 大数据（2001—现在） 1.3.8 深度学习（2011—现在） 1.4 目前的先进技术 1.5 人工智能的风险和收益 第 2 章 智能体 2.1 智能体和环境 2.2 良好行为：理性的概念 2.2.1 性能度量 2.2.2 理性 2.2.3 全知、学习和自主 2.3 环境的本质 2.3.1 指定任务环境 2.3.2 任务环境的属性 2.4 智能体的结构 2.4.1 智能体程序 2.4.2 简单反射型智能体 2.4.3 基于模型的反射型智能体 2.4.4 基于目标的智能体 2.4.5 基于效用的智能体 2.4.6 学习型智能体 2.4.7 智能体程序的组件如何工作 第二部分 问题求解 第 3 章 通过搜索进行问题求解 3.1 问题求解智能体 3.1.1 搜索问题和解 3.1.2 问题形式化 3.2 问题示例 3.2.1 标准化问题 3.2.2 真实世界问题 3.3 搜索算法 3.3.1 最佳优先搜索 3.3.2 搜索数据结构 3.3.3 冗余路径 3.3.4 问题求解性能评估 3.4 无信息搜索策略 3.4.1 广度优先搜索 3.4.2 Dijkstra 算法或一致代价搜索 3.4.3 深度优先搜索与内存问题 3.4.4 深度受限和迭代加深搜索 3.4.5 双向搜索 3.4.6 无信息搜索算法对比 3.5 有信息（启发式）搜索策略 3.5.1 贪心最佳优先搜索 3.5.2 A*搜索 3.5.3 搜索等值线 3.5.4 满意搜索：不可容许的启发式函数与加权 A*搜索 3.5.5 内存受限搜索 3.5.6 双向启发式搜索 3.6 启发式函数 3.6.1 启发式函数的准确性对性能的影响 3.6.2 从松弛问题出发生成启发式函数 3.6.3 从子问题出发生成启发式函数：模式数据库 3.6.4 使用地标生成启发式函数 3.6.5 学习以更好地搜索 3.6.6 从经验中学习启发式函数 第 4 章 复杂环境中的搜索 4.1 局部搜索和最优化问题 4.1.1 爬山搜索 4.1.2 模拟退火 4.1.3 局部束搜索 4.1.4 进化算法 4.2 连续空间中的局部搜索 4.3 使用非确定性动作的搜索 4.3.1 不稳定的真空吸尘器世界 4.3.2 与或搜索树 4.3.3 反复尝试 4.4 部分可观测环境中的搜索 4.4.1 无观测信息的搜索 4.4.2 部分可观测环境中的搜索 4.4.3 求解部分可观测问题 4.4.4 部分可观测环境中的智能体 4.5 在线搜索智能体和未知环境 4.5.1 在线搜索问题 4.5.2 在线搜索智能体 4.5.3 在线局部搜索 4.5.4 在线搜索中的学习 第 5 章 对抗搜索和博弈 5.1 博弈论 5.2 博弈中的优化决策 5.2.1 极小化极大搜索算法 5.2.2 多人博弈中的最优决策 5.2.3 剪枝 5.2.4 移动顺序 5.3 启发式树搜索 5.3.1 评价函数 5.3.2 截断搜索 5.3.3 前向剪枝 5.3.4 搜索和查表 5.4 蒙特卡罗树搜索 5.5 随机博弈 机会博弈的评价函数 5.6 部分可观测博弈 5.6.1 四国军棋：部分可观测的国际象棋 5.6.2 纸牌游戏 5.7 博弈搜索算法的局限性 第 6 章 约束满足问题 6.1 定义约束满足问题 6.1.1 问题示例：地图着色 6.1.2 问题示例：车间作业调度 6.1.3 CSP 形式体系的变体 6.2 约束传播：CSP 中的推断 6.2.1 节点一致性 6.2.2 弧一致性 6.2.3 路径一致性 6.2.4 k 一致性 6.2.5 全局约束 6.2.6 数独 6.3 CSP 的回溯搜索 6.3.1 变量排序和值排序 6.3.2 交替进行搜索和推理 6.3.3 智能回溯：向后看 6.3.4 约束学习 6.4 CSP 的局部搜索 6.5 问题的结构 6.5.1 割集调整 6.5.2 树分解 6.5.3 值对称 第三部分 知识、推理和规划 第 7 章 逻辑智能体 7.1 基于知识的智能体 7.2 wumpus 世界 7.3 逻辑 7.4 命题逻辑：一种非常简单的逻辑 7.4.1 语法 7.4.2 语义 7.4.3 一个简单的知识库 7.4.4 一个简单的推断过程 7.5 命题定理证明 7.5.1 推断与证明 7.5.2 通过归结证明 7.5.3 霍恩子句与确定子句 7.5.4 前向链接与反向链接 7.6 高效命题模型检验 7.6.1 完备的回溯算法 7.6.2 局部搜索算法 7.6.3 随机 SAT 问题概览 7.7 基于命题逻辑的智能体 7.7.1 世界的当前状态 7.7.2 混合智能体 7.7.3 逻辑状态估计 7.7.4 用命题推断进行规划 第 8 章 一阶逻辑 8.1 回顾表示 8.1.1 思想的语言 8.1.2 结合形式语言和自然语言的优点 8.2 一阶逻辑的语法和语义 8.2.1 一阶逻辑模型 8.2.2 符号与解释 8.2.3 项 8.2.4 原子语句 8.2.5 复合语句 8.2.6 量词 8.2.7 等词 8.2.8 数据库语义 8.3 使用一阶逻辑 8.3.1 一阶逻辑的断言与查询 8.3.2 亲属关系论域 8.3.3 数、集合与列表 8.3.4 wumpus 世界 8.4 一阶逻辑中的知识工程 8.4.1 知识工程的过程 8.4.2 电子电路论域 第 9 章 一阶逻辑中的推断 9.1 命题推断与一阶推断 约简为命题推断 9.2 合一与一阶推断 9.2.1 合一 9.2.2 存储与检索 9.3 前向链接 9.3.1 一阶确定子句 9.3.2 简单的前向链接算法 9.3.3 高效前向链接 9.4 反向链接 9.4.1 反向链接算法 9.4.2 逻辑编程 9.4.3 冗余推断和无限循环 9.4.4 Prolog 的数据库语义 9.4.5 约束逻辑编程 9.5 归结 9.5.1 一阶逻辑的合取范式 9.5.2 归结推断规则 9.5.3 证明范例 9.5.4 归结的完备性 9.5.5 等词 9.5.6 归结策略 第 10 章 知识表示 10.1 本体论工程 10.2 类别与对象 10.2.1 物理组成 10.2.2 量度 10.2.3 对象：事物和物质 10.3 事件 10.3.1 时间 10.3.2 流和对象 10.4 精神对象和模态逻辑 其他模态逻辑 10.5 类别的推理系统 10.5.1 语义网络 10.5.2 描述逻辑 10.6 用缺省信息推理 10.6.1 限定与缺省逻辑 10.6.2 真值维护系统 第 11 章 自动规划 11.1 经典规划的定义 11.1.1 范例领域：航空货物运输 11.1.2 范例领域：备用轮胎问题 11.1.3 范例领域：积木世界 11.2 经典规划的算法 11.2.1 规划的前向状态空间搜索 11.2.2 规划的反向状态空间搜索 11.2.3 使用布尔可满足性规划 11.2.4 其他经典规划方法 11.3 规划的启发式方法 11.3.1 领域无关剪枝 11.3.2 规划中的状态抽象 11.4 分层规划 11.4.1 高层动作 11.4.2 搜索基元解 11.4.3 搜索抽象解 11.5 非确定性域的规划和行动 11.5.1 无传感器规划 11.5.2 应变规划 11.5.3 在线规划 11.6 时间、调度和资源 11.6.1 时间约束和资源约束的表示 11.6.2 解决调度问题 11.7 规划方法分析 第四部分 不确定知识和不确定推理 第 12 章 不确定性的量化 12.1 不确定性下的动作 12.1.1 不确定性概述 12.1.2 不确定性与理性决策 12.2 基本概率记号 12.2.1 概率是关于什么的 12.2.2 概率断言中的命题语言 12.2.3 概率公理及其合理性 12.3 使用完全联合分布进行推断 12.4 独立性 12.5 贝叶斯法则及其应用 12.5.1 应用贝叶斯法则：简单实例 12.5.2 应用贝叶斯法则：合并证据 12.6 朴素贝叶斯模型 使用朴素贝叶斯进行文本分类 12.7 重游 wumpus 世界 第 13 章 概率推理 13.1 不确定域的知识表示 13.2 贝叶斯网络的语义 13.2.1 贝叶斯网络中的条件独立性关系 13.2.2 条件分布的高效表示 13.2.3 连续变量的贝叶斯网络 13.2.4 案例研究：汽车保险 13.3 贝叶斯网络中的精确推断 13.3.1 通过枚举进行推断 13.3.2 变量消元算法 13.3.3 精确推断的复杂性 13.3.4 聚类算法 13.4 贝叶斯网络中的近似推理 13.4.1 直接采样方法 13.4.2 通过马尔可夫链模拟进行推断 13.4.3 编译近似推断 13.5 因果网络 13.5.1 表示动作：do 操作 13.5.2 后门准则 第 14 章 时间上的概率推理 14.1 时间与不确定性 14.1.1 状态与观测 14.1.2 转移模型与传感器模型 14.2 时序模型中的推断 14.2.1 滤波与预测 14.2.2 平滑 14.2.3 寻找最可能序列 14.3 隐马尔可夫模型 14.3.1 简化矩阵算法 14.3.2 隐马尔可夫模型示例：定位 14.4 卡尔曼滤波器 14.4.1 更新高斯分布 14.4.2 简单的一维示例 14.4.3 一般情况 14.4.4 卡尔曼滤波的适用范围 14.5 动态贝叶斯网络 14.5.1 构建动态贝叶斯网络 14.5.2 动态贝叶斯网络中的精确推断 14.5.3 动态贝叶斯网络中的近似推断 第 15 章 概率编程 15.1 关系概率模型 15.1.1 语法与语义 15.1.2 实例：评定玩家的技能等级 15.1.3 关系概率模型中的推断 15.2 开宇宙概率模型 15.2.1 语义与语法 15.2.2 开宇宙概率模型的推断 15.2.3 示例 15.3 追踪复杂世界 15.3.1 示例：多目标跟踪 15.3.2 示例：交通监控 15.4 作为概率模型的程序 15.4.1 示例：文本阅读 15.4.2 语法与语义 15.4.3 推断结果 15.4.4 结合马尔可夫模型改进生成程序 15.4.5 生成程序的推断 第 16 章 做简单决策 16.1 在不确定性下结合信念与愿望 16.2 效用理论基础 16.2.1 理性偏好的约束 16.2.2 理性偏好导致效用 16.3 效用函数 16.3.1 效用评估和效用尺度 16.3.2 金钱的效用 16.3.3 期望效用与决策后失望 16.3.4 人类判断与非理性 16.4 多属性效用函数 16.4.1 占优 16.4.2 偏好结构与多属性效用 16.5 决策网络 16.5.1 使用决策网络表示决策问题 16.5.2 评估决策网络 16.6 信息价值 16.6.1 简单示例 16.6.2 完美信息的一般公式 16.6.3 价值信息的性质 16.6.4 信息收集智能体的实现 16.6.5 非短视信息收集 16.6.6 敏感性分析与健壮决策 16.7 未知偏好 16.7.1 个人偏好的不确定性 16.7.2 顺从人类 第 17 章 做复杂决策 17.1 序贯决策问题 17.1.1 时间上的效用 17.1.2 最优策略与状态效用 17.1.3 奖励规模 17.1.4 表示 MDP 17.2 MDP 的算法 17.2.1 价值迭代 17.2.2 策略迭代 17.2.3 线性规划 17.2.4 MDP 的在线算法 17.3 老虎机问题 17.3.1 计算基廷斯指数 17.3.2 伯努利老虎机 17.3.3 近似最优老虎机策略 17.3.4 不可索引变体 17.4 部分可观测 MDP POMDP 的定义 17.5 求解 POMDP 的算法 17.5.1 POMDP 的价值迭代 17.5.2 POMDP 的在线算法 第 18 章 多智能体决策 18.1 多智能体环境的特性 18.1.1 单个决策者 18.1.2 多决策者 18.1.3 多智能体规划 18.1.4 多智能体规划：合作与协调 18.2 非合作博弈论 18.2.1 单步博弈：正则形式博弈 18.2.2 社会福利 18.2.3 重复博弈 18.2.4 序贯博弈：扩展形式 18.2.5 不确定收益与辅助博弈 18.3 合作博弈论 18.3.1 联盟结构与结果 18.3.2 合作博弈中的策略 18.3.3 合作博弈中的计算 18.4 做集体决策 18.4.1 在合同网中分配任务 18.4.2 通过拍卖分配稀缺资源 18.4.3 投票 18.4.4 议价 第五部分 机器学习 第 19 章 样例学习 19.1 学习的形式 19.2 监督学习 问题示例：餐厅等待问题 19.3 决策树学习 19.3.1 决策树的表达能力 19.3.2 从样例中学习决策树 19.3.3 选择测试属性 19.3.4 泛化与过拟合 19.3.5 拓展决策树的适用范围 19.4 模型选择与模型优化 19.4.1 模型选择 19.4.2 从错误率到损失函数 19.4.3 正则化 19.4.4 超参数调整 19.5 学习理论 PAC 学习示例：学习决策列表 19.6 线性回归与分类 19.6.1 单变量线性回归 19.6.2 梯度下降 19.6.3 多变量线性回归 19.6.4 带有硬阈值的线性分类器 19.6.5 基于逻辑斯谛回归的线性分类器 19.7 非参数模型 19.7.1 最近邻模型 19.7.2 使用 k-d 树寻找最近邻 19.7.3 局部敏感哈希 19.7.4 非参数回归 19.7.5 支持向量机 19.7.6 核技巧 19.8 集成学习 19.8.1 自助聚合法 19.8.2 随机森林法 19.8.3 堆叠法 19.8.4 自适应提升法 19.8.5 梯度提升法 19.8.6 在线学习 19.9 开发机器学习系统 19.9.1 问题形式化 19.9.2 数据收集、评估和管理 19.9.3 模型选择与训练 19.9.4 信任、可解释性、可说明性 19.9.5 操作、监控和维护 第 20 章 概率模型学习 20.1 统计学习 20.2 完全数据学习 20.2.1 最大似然参数学习：离散模型 20.2.2 朴素贝叶斯模型 20.2.3 生成模型和判别模型 20.2.4 最大似然参数学习：连续模型 20.2.5 贝叶斯参数学习 20.2.6 贝叶斯线性回归 20.2.7 贝叶斯网络结构学习 20.2.8 非参数模型密度估计 20.3 隐变量学习：EM 算法 20.3.1 无监督聚类：学习混合高斯 20.3.2 学习带隐变量的贝叶斯网络参数值 20.3.3 学习隐马尔可夫模型 20.3.4 EM 算法的一般形式 20.3.5 学习带隐变量的贝叶斯网络结构 第 21 章 深度学习 21.1 简单前馈网络 21.1.1 网络作为复杂函数 21.1.2 梯度与学习 21.2 深度学习的计算图 21.2.1 输入编码 21.2.2 输出层与损失函数 21.2.3 隐藏层 21.3 卷积网络 21.3.1 池化与下采样 21.3.2 卷积神经网络的张量运算 21.3.3 残差网络 21.4 学习算法 21.4.1 计算图中的梯度计算 21.4.2 批量归一化 21.5 泛化 21.5.1 选择正确的网络架构 21.5.2 神经架构搜索 21.5.3 权重衰减 21.5.4 暂退法 21.6 循环神经网络 21.6.1 训练基本的循环神经网络 21.6.2 长短期记忆 RNN 21.7 无监督学习与迁移学习 21.7.1 无监督学习 21.7.2 迁移学习和多任务学习 21.8 应用 21.8.1 视觉 21.8.2 自然语言处理 21.8.3 强化学习 第 22 章 强化学习 22.1 从奖励中学习 22.2 被动强化学习 22.2.1 直接效用估计 22.2.2 自适应动态规划 22.2.3 时序差分学习 22.3 主动强化学习 22.3.1 探索 22.3.2 安全探索 22.3.3 时序差分 Q 学习 22.4 强化学习中的泛化 22.4.1 近似直接效用估计 22.4.2 近似时序差分学习 22.4.3 深度强化学习 22.4.4 奖励函数设计 22.4.5 分层强化学习 22.5 策略搜索 22.6 学徒学习与逆强化学习 22.7 强化学习的应用 22.7.1 在电子游戏中的应用 22.7.2 在机器人控制中的应用 第六部分 沟通、感知和行动 第 23 章 自然语言处理 23.1 语言模型 23.1.1 词袋模型 23.1.2 n 元单词模型 23.1.3 其他 n 元模型 23.1.4 n 元模型的平滑 23.1.5 单词表示 23.1.6 词性标注 23.1.7 语言模型的比较 23.2 文法 E0 的词典 23.3 句法分析 23.3.1 依存分析 23.3.2 从样例中学习句法分析器 23.4 扩展文法 23.4.1 语义解释 23.4.2 学习语义文法 23.5 真实自然语言的复杂性 23.6 自然语言任务 第 24 章 自然语言处理中的深度学习 24.1 词嵌入 24.2 自然语言处理中的循环神经网络 24.2.1 使用循环神经网络的语言模型 24.2.2 用循环神经网络进行分类 24.2.3 自然语言处理任务中的 LSTM 模型 24.3 序列到序列模型 24.3.1 注意力 24.3.2 解码 24.4 Transformer 架构 24.4.1 自注意力 24.4.2 从自注意力到 Transformer 24.5 预训练和迁移学习 24.5.1 预训练词嵌入 24.5.2 预训练上下文表示 24.5.3 掩码语言模型 24.6 最高水平（SOTA） 第 25 章 计算机视觉 25.1 引言 25.2 图像形成 25.2.1 无透镜成像：针孔照相机 25.2.2 透镜系统 25.2.3 缩放正交投影 25.2.4 光线与明暗 25.2.5 颜色 25.3 简单图像特征 25.3.1 边缘 25.3.2 纹理 25.3.3 光流 25.3.4 自然图像分割 25.4 图像分类 25.4.1 基于卷积神经网络的图像分类 25.4.2 卷积神经网络对图像分类问题有效的原因 25.5 物体检测 25.6 三维世界 25.6.1 多个视图下的三维线索 25.6.2 双目立体视觉 25.6.3 移动摄像机给出的三维线索 25.6.4 单个视图的三维线索 25.7 计算机视觉的应用 25.7.1 理解人类行为 25.7.2 匹配图片与文字 25.7.3 多视图重建 25.7.4 单视图中的几何 25.7.5 生成图片 25.7.6 利用视觉控制运动 第 26 章 机器人学 26.1 机器人 26.2 机器人硬件 26.2.1 机器人的硬件层面分类 26.2.2 感知世界 26.2.3 产生运动 26.3 机器人学解决哪些问题 26.4 机器人感知 26.4.1 定位与地图构建 26.4.2 其他感知类型 26.4.3 机器人感知中的监督学习与无监督学习 26.5 规划与控制 26.5.1 构形空间 26.5.2 运动规划 26.5.3 轨迹跟踪控制 26.5.4 最优控制 26.6 规划不确定的运动 26.7 机器人学中的强化学习 26.7.1 利用模型 26.7.2 利用其他信息 26.8 人类与机器人 26.8.1 协调 26.8.2 学习做人类期望的事情 26.9 其他机器人框架 26.9.1 反应式控制器 26.9.2 包容架构 26.10 应用领域 第七部分 总结 第 27 章 人工智能的哲学、伦理和安全性 27.1 人工智能的极限 27.1.1 由非形式化得出的论据 27.1.2 由能力缺陷得出的论据 27.1.3 数学异议 27.1.4 衡量人工智能 27.2 机器能真正地思考吗 27.2.1 中文房间 27.2.2 意识与感质 27.3 人工智能的伦理 27.3.1 致命性自主武器 27.3.2 监控、安全与隐私 27.3.3 公平与偏见 27.3.4 信任与透明度 27.3.5 工作前景 27.3.6 机器人权利 27.3.7 人工智能安全性 第 28 章 人工智能的未来 28.1 人工智能组件 28.2 人工智能架构 附录 A 数学背景知识 A.1 复杂性分析和$O()$记号 A.1.1 渐近分析 A.1.2 NP 困难和固有的难题 A.2 向量，矩阵和线性代数 A.3 概率分布 读者服务 "},"Learn/LearnConda/Conda.html":{"url":"Learn/LearnConda/Conda.html","title":"Learn Conda","keywords":"","body":"pypi: https://pypi.tuna.tsinghua.edu.cn/simple https://mirrors.tuna.tsinghua.edu.cn/help/pypi/ anaconda: https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ conda config --set custom_channels.auto https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ "},"Learn/LearnCUDA/CUDA.html":{"url":"Learn/LearnCUDA/CUDA.html","title":"Learn CUDA","keywords":"","body":"CUDA Learn NVIDIA CUDA documentation NVIDIA cuda-education cuda-samples CUDA Programming Course – High-Performance Computing with GPUs d_what_are_some_good_resources_to_learn_cuda What-are-some-of-the-best-resources-to-learn-CUDA-C cuda-training-series even-easier-introduction-cuda demystifying-gpu-architectures-for-deep-learning numba GPU-Puzzles CUDA 编程入门极简教程 GPU 编程 nvvm-ir-spec NVIDIA CUDA (Compute Unified Device Architecture) The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application. NVIDIA® CUDA® 工具包为构建 GPU 加速应用程序的 C 和 C++ 开发人员提供了一个全面的开发环境。借助 CUDA 工具包，您可以在 GPU 加速的嵌入式系统、桌面工作站、企业数据中心、基于云的平台和 HPC 超级计算机上开发、优化和部署您的应用程序。该工具包包括 GPU 加速库、调试和优化工具、C/C++ 编译器以及用于部署应用程序的运行时库。 Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs. 使用内置功能在多 GPU 配置之间分配计算，科学家和研究人员可以开发从单个 GPU 工作站扩展到具有数千个 GPU 的云安装的应用程序。 CUDA C++ Programming Guide v12.6 CUDA C++ Programming Guide CUDA 编程模型的关键组成部分 (GPT) CUDA 编程模型使开发者能够编写代码，充分利用 NVIDIA GPU 的强大并行计算能力。它基于单指令多线程（SIMT）架构，其中多个线程同时执行相同的指令，但处理不同的数据。CUDA 通过分层的线程结构和内存管理系统，高效组织计算任务。 线程层次结构： 线程（Thread）：执行特定任务的最小执行单元。 线程块（Thread Block）：线程的集合，线程块中的线程共同执行任务。一个线程块最多包含 1024 个线程（具体取决于 GPU 架构）。 网格（Grid）：线程块的集合。网格可以是 1D、2D 或 3D，以便更方便地将线程映射到数据上。 通过唯一的索引（如 threadIdx、blockIdx、blockDim 和 gridDim），每个线程可以访问特定的数据部分。 内存层次结构： 全局内存（Global Memory）：所有线程都可以访问，但访问延迟较高。 共享内存（Shared Memory）：线程块内的线程共享的一种快速、低延迟的内存。 局部内存（Local Memory）：每个线程的私有内存，但由于位于全局内存中，访问速度较慢。 寄存器（Registers）：速度极快，但数量有限，用于存储线程的临时变量。 内核（Kernel）： CUDA 内核是运行在 GPU 上的函数，使用 C/C++ 语言编写并带有特殊的语法标记。内核从 CPU 发起，并由 GPU 的线程并行执行。 Introduction The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores. The CUDA parallel programming model is designed to overcome this challenge while maintaing a low learning curve for programmers familiar with C. Its core is three key abstractions: a hierarchy of thread groups: 层级线程组 shared memories: 共享内存 barrier synchronization: 障碍同步 These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block. 这些抽象提供了细粒度数据并行性和线程并行性，嵌套在粗粒度数据并行性和任务并行性中。它们引导程序员将问题划分为可以由线程块独立并行解决的粗略子问题，并将每个子问题划分为可以由块内的所有线程并行协作解决的更精细的部分。 This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count. 这种分解通过允许线程在解决每个子问题时进行合作来保留语言表达能力，同时实现自动可扩展性。事实上，每个线程块都可以以任何顺序（同时或顺序）调度到 GPU 内的任何可用多处理器上，以便编译后的 CUDA 程序可以在任意数量的多处理器上执行，如图 3 所示，并且仅运行时系统需要知道物理多处理器数量。 A GPU is built around an array of Streaming Multiprocessors (SMs) GPU 由流式多处理器 (SM) 阵列构建 Programming Model 编程模型 Kernels: 内核函数 Thread Hierarachy: 线程层次结构 Memory Hierarachy: 内存层次结构 Heteroheneous Programming: 异构编程 Asynchronous SIMT Programming Model: 异步 SIMT 编程模型 Compute Capability: 计算能力 Kernels: 内核函数 CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. CUDA C++ 通过允许程序员定义称为内核的 C++ 函数来扩展 C++，这些函数在调用时由 N 个不同的 CUDA 线程并行执行 N 次，而不是像常规 C++ 函数那样只能执行一次。 A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new >> execution configuration syntax (see C++ Language Extensions). Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. 使用 __global__ 声明说明符定义内核，并使用新的 >> 执行配置语法指定为给定内核调用执行该内核的 CUDA 线程数（请参阅 C++语言扩展）。每个执行内核的线程都会被赋予一个唯一的线程 ID ，该 ID 可以在内核中通过内置变量进行访问。 Thread Hierarachy: 线程层次结构 grids - blocks - threads 块内线程驻留在同一个 core 上，共享内存 blocks, threads 由三维下标索引 threadIdx.x, .y, .z blockIdx.x, .y, .z block 尺寸: blockDim.x, .y, .z For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy). There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads. 每个块的线程数量是有限的，因为块的所有线程都应该驻留在同一个流式多处理器核心上，并且必须共享该核心的有限内存资源。在当前的 GPU 上，一个线程块最多可以包含 1024 个线程。 However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks. 然而，一个内核可以由多个形状相同的线程块来执行，因此线程总数等于每个块的线程数乘以块数。 Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4. The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system. 块被组织成一维、二维或三维线程块网格，如图 4 所示。网格中线程块的数量通常由正在处理的数据大小决定，该数据通常超过系统中处理器的数量。 Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows. // Kernel definition __global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) { int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; if (i >>(A, B, C); ... } Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. In addition to __syncthreads(), the Cooperative Groups API provides a rich set of thread-synchronization primitives. 块内的线程可以通过某些共享内存共享数据并同步其执行来协调内存访问来进行协作。更准确地说，可以通过调用__syncthreads()内部函数来指定内核中的同步点； __syncthreads()充当屏障，块中的所有线程都必须等待，然后才允许任何线程继续进行。除了__syncthreads()之外，协作组 API 还提供了一组丰富的线程同步原语。 For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight. 为了高效合作，共享内存应该是每个处理器核心附近的低延迟内存（很像 L1 缓存），并且__syncthreads()应该是轻量级的。 Memory Hierarachy: 内存层次结构 CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory. All threads have access to the same global memory. CUDA 线程在执行期间可以访问多个内存空间中的数据，如图 6 所示。每个线程都有私有本地内存。每个线程块都有对该块的所有线程可见的共享内存，并且与该块具有相同的生命周期。线程块簇中的线程块可以对彼此的共享内存执行读、写和原子操作。所有线程都可以访问相同的全局内存。 There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory). 还有两个可供所有线程访问的附加只读内存空间：常量内存空间和纹理内存空间。全局、常量和纹理内存空间针对不同的内存使用进行了优化（请参阅设备内存访问）。纹理内存还为某些特定的数据格式提供不同的寻址模式以及数据过滤（请参阅纹理和表面内存）。 The global, constant, and texture memory spaces are persistent across kernel launches by the same application. 全局、常量和纹理内存空间在同一应用程序的内核启动过程中是持久的。 Heteroheneous Programming: 异构编程 As illustrated by Figure 7, the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU. 如图 7 所示，CUDA 编程模型假设 CUDA 线程在物理上独立的设备上执行，该设备作为运行 C++ 程序的主机的协处理器运行。例如，当内核在 GPU 上执行而 C++ 程序的其余部分在 CPU 上执行时，就会出现这种情况。 The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface). This includes device memory allocation and deallocation as well as data transfer between host and device memory. CUDA 编程模型还假设主机和设备都在 DRAM 中维护自己独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用 CUDA 运行时（在编程接口中描述）来管理内核可见的全局、常量和纹理内存空间。这包括设备内存分配和释放以及主机和设备内存之间的数据传输。 Unified Memory provides managed memory to bridge the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. See Unified Memory Programming for an introduction to Unified Memory. 统一内存提供托管内存来桥接主机和设备内存空间。托管内存可作为具有公共地址空间的单个一致内存映像从系统中的所有 CPU 和 GPU 进行访问。此功能可实现设备内存的超额订阅，并且无需在主机和设备上显式镜像数据，从而大大简化移植应用程序的任务。有关统一内存的介绍，请参阅统一内存编程。 Serial code executes on the host while parallel code executes on the device. 串行代码在主机上执行，而并行代码在设备上执行。 Asynchronous SIMT Programming Model: 异步 SIMT 编程模型 In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads. 在 CUDA 编程模型中，线程是执行计算或内存操作的最低抽象级别。从基于 NVIDIA Ampere GPU 架构的设备开始，CUDA 编程模型通过异步编程模型提供内存操作加速。异步编程模型定义了与 CUDA 线程相关的异步操作的行为。 The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads. The model also explains and defines how cuda::memcpy_async can be used to move data asynchronously from global memory while computing in the GPU. 异步编程模型定义了用于 CUDA 线程之间同步的异步屏障的行为。该模型还解释并定义了如何使用cuda::memcpy_async在 GPU 中计算时从全局内存异步移动数据。 2.5.1. Asynchronous Operations 2.5.1.异步操作 An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread. In a well formed program one or more CUDA threads synchronize with the asynchronous operation. The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads. 异步操作被定义为由 CUDA 线程发起并像由另一个线程一样异步执行的操作。在格式良好的程序中，一个或多个 CUDA 线程与异步操作同步。启动异步操作的 CUDA 线程不需要位于同步线程中。 Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation. An asynchronous operation uses a synchronization object to synchronize the completion of the operation. Such a synchronization object can be explicitly managed by a user (e.g., cuda::memcpy_async) or implicitly managed within a library (e.g., cooperative_groups::memcpy_async). 这样的异步线程（as-if 线程）始终与启动异步操作的 CUDA 线程相关联。异步操作使用同步对象来同步操作的完成。这样的同步对象可以由用户显式管理（例如， cuda::memcpy_async ）或在库中隐式管理（例如， cooperative_groups::memcpy_async ）。 A synchronization object could be a cuda::barrier or a cuda::pipeline. These objects are explained in detail in Asynchronous Barrier and Asynchronous Data Copies using cuda::pipeline. These synchronization objects can be used at different thread scopes. A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each. 同步对象可以是cuda::barrier或cuda::pipeline 。这些对象在使用 cuda::pipeline 的异步屏障和异步数据副本中详细解释。这些同步对象可以在不同的线程范围内使用。范围定义了可以使用同步对象来与异步操作同步的线程集。下表定义了 CUDA C++ 中可用的线程范围以及可以与每个线程同步的线程。 Compute Capability: 计算能力 Programming Interface 编程接口 Hardware Implementation 硬件实现 Performance Guidelines 性能指南 PTX Parallel Thread Execution PTX: a low-level parallel thread execution virtual machine and instruction set architecture. PTX 是一种低级并行线程执行虚拟机和指令集体系结构。 PTX exposes the GPU as data-parallel computing device. Numba Overview 概述  Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically. Numba 通过将 Python 代码的受限子集直接编译为遵循 CUDA 执行模型的 CUDA 内核和设备函数来支持 CUDA GPU 编程。用 Numba 编写的内核似乎可以直接访问 NumPy 数组。 NumPy 数组在 CPU 和 GPU 之间自动传输。 Install CUDA CUDA Toolkit Archive NVIDIA CUDA Installation Guide for Linux Other Resources Programming Massively Parallel Processors: A Hands-on Approach Programming Massively Parallel Processors: A Hands-on Approach, Second Edition, teaches students how to program massively parallel processors. It offers a detailed discussion of various techniques for constructing parallel programs. Case studies are used to demonstrate the development process, which begins with computational thinking and ends with effective and efficient parallel programs. This guide shows both student and professional alike the basic concepts of parallel programming and GPU architecture. Topics of performance, floating-point format, parallel patterns, and dynamic parallelism are covered in depth. This revised edition contains more parallel programming examples, commonly-used libraries such as Thrust, and explanations of the latest tools. It also provides new coverage of CUDA 5.0, improved performance, enhanced development tools, increased hardware support, and more; increased coverage of related technology, OpenCL and new material on algorithm patterns, GPU clusters, host programming, and data parallelism; and two new case studies (on MRI reconstruction and molecular visualization) that explore the latest applications of CUDA and GPUs for scientific research and high-performance computing. This book should be a valuable resource for advanced students, software engineers, programmers, and hardware engineers. Programming Massively Parallel Processors： A Hands-on Approach，第二版，教授学生如何对大规模并行处理器进行编程。它详细讨论了用于构建并行程序的各种技术。案例研究用于演示开发过程，该过程从计算思维开始，以有效和高效的并行程序结束。本指南向学生和专业人士展示了并行编程和 GPU 架构的基本概念。深入介绍了性能、浮点格式、并行模式和动态并行性等主题。此修订版包含更多并行编程示例、常用库（如 Thrust）以及最新工具的解释。它还提供了 CUDA 5.0 的新覆盖范围、改进的性能、增强的开发工具、增强的硬件支持等;增加了相关技术、OpenCL 和有关算法模式、GPU 集群、主机编程和数据并行性的新材料的覆盖范围;以及两个新的案例研究（关于 MRI 重建和分子可视化），探索 CUDA 和 GPU 在科学研究和高性能计算中的最新应用。这本书应该是高级学生、软件工程师、程序员和硬件工程师的宝贵资源。 CUDA Examples #include #include // CUDA kernel function for vector addition __global__ void vectorAdd(const float* A, const float* B, float* C, int N) { int i = blockIdx.x * blockDim.x + threadIdx.x; // Calculate global thread index if (i (i); h_B[i] = static_cast(i * 2); } // Allocate device memory float *d_A, *d_B, *d_C; cudaMalloc((void**)&d_A, size); cudaMalloc((void**)&d_B, size); cudaMalloc((void**)&d_C, size); // Copy data from host to device cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); // Launch kernel int threadsPerBlock = 256; int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; vectorAdd>>(d_A, d_B, d_C, N); // Copy result back to host cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Print some results for (int i = 0; i "},"Learn/LearnCV/ComputerVisoin.html":{"url":"Learn/LearnCV/ComputerVisoin.html","title":"Learn Computer Vision","keywords":"","body":"Computer Vision Image Classification Object Detection: bounding box Image Segmentation: pixel-wise classification, cutting edges "},"Learn/LearnDocker/docker.html":{"url":"Learn/LearnDocker/docker.html","title":"Learn Docker","keywords":"","body":"# Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update # Install Docker Engine: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin https://docs.docker.com/desktop/install/debian/ docker stop $(docker ps -a -q) docker image pull name:tag docker search image_name docker pull image_name:tag docker images # list all images docker rmi image_name:tag # remove image docker run -it image_name:tag /bin/bash # run container # start docker desktop systemctl --user start docker-desktop # start on sign in systemctl --user enable docker-desktop systemctl --user stop docker-desktop https://medium.com/@SrvZ/docker-proxy-and-my-struggles-a4fd6de21861 Docker Compose Docker 组合 Docker Compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience. Docker Compose 是一个用于定义和运行多容器应用程序的工具。它是解锁精简高效的开发和部署体验的关键。 Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file. Then, with a single command, you create and start all the services from your configuration file. Compose 简化了对整个应用程序堆栈的控制，使您可以在单个易于理解的 YAML 配置文件中轻松管理服务、网络和卷。然后，使用单个命令，您可以从配置文件创建并启动所有服务。 Compose works in all environments; production, staging, development, testing, as well as CI workflows. It also has commands for managing the whole lifecycle of your application: Compose 适用于所有环境；生产、登台、开发、测试以及 CI 工作流程。它还具有用于管理应用程序整个生命周期的命令： Start, stop, and rebuild services 启动、停止和重建服务 View the status of running services 查看正在运行的服务的状态 Stream the log output of running services 流式传输正在运行的服务的日志输出 Run a one-off command on a service 在服务上运行一次性命令 "},"Learn/LearnDrones/Note.html":{"url":"Learn/LearnDrones/Note.html","title":"Learn Drones","keywords":"","body":"Learn Drones HKUST-Aerial-Robotics ZJU-FAST-Lab Fast-Drone-250 ego-planner-swarm ego-planner multi_uav_simulator Tools RealSense Driver github source installation The shared object will be installed in /usr/local/lib, header files in /usr/local/include. sudo apt install libpcl-dev Mavros: Micro Air Vehicle Robot Operating System MAVLink extendable communication node for ROS with proxy for Ground Control Station. This package provides communication driver for various autopilots with MAVLink communication protocol. Additional it provides UDP MAVLink bridge for ground control stations (e.g. QGroundControl). mavros用于无人机通信，可以将飞控与主控的信息进行交换 ceres-solver: googlesource An open source lib for modeling and solving large, complicated optimization problems. It can be used to solve Non-linear Least Squares problems with bounds constraints and general unconstrained optimization problems. 它可用于求解具有边界约束的非线性最小二乘法问题和一般的无约束优化问题。 glog C++ implementation of the Google logging module. pip uninstall em pip install empy # install ros noetic dependencies # [ddynamic_reconfigure](https://github.com/pal-robotics/ddynamic_reconfigure) catkin_make_isolated --install -DPYTHON_EXECUTABLE=/usr/bin/python3 --force-cmake # must be: dont be higher than 0.10.0 Package: liblog4cxx-dev Version: 0.10.0-15ubuntu2 "},"Learn/LearnGameDev/Note.html":{"url":"Learn/LearnGameDev/Note.html","title":"Learn GameDev","keywords":"","body":"Learning Game Development links awesome-gamedev youtube: godot tutorials videos Learn GDScript From Zero app godotengine "},"Learn/LearnGameTheory/GameTheory.html":{"url":"Learn/LearnGameTheory/GameTheory.html","title":"Learn Game Theory","keywords":"","body":"Game Theory Coalitional Games: 联盟博弈 "},"Learn/LearnLalrpop/Lalrpop.html":{"url":"Learn/LearnLalrpop/Lalrpop.html","title":"Learn Lalrpop (Rust)","keywords":"","body":"Lalrpop LR(1) parser generator for Rust. LALRPOP 拉尔波普 LALRPOP is a parser generator, similar in principle to YACC, ANTLR, Menhir, and other such programs. In general, it has the grand ambition of being the most usable parser generator ever. This ambition is most certainly not fully realized: right now, it's fairly standard, maybe even a bit subpar in some areas. But hey, it's young. For the most part, this README is intended to describe the current behavior of LALRPOP, but in some places it includes notes for planned future changes. LALRPOP 是一个解析器生成器，原理类似于 YACC、ANTLR、Menhir 和其他此类程序。总的来说，它有一个宏伟的雄心壮志，那就是成为有史以来最有用的解析器生成器。这个雄心壮志肯定没有完全实现：现在，它相当标准，甚至在某些领域可能有点低于标准。但是，嘿，它很年轻。在大多数情况下，本 README 旨在描述 LALRPOP 的当前行为，但在某些地方，它包括计划的未来更改的说明。 add lalrpop to rust project define grammar in lalrpop file define asts, build ast use macros use errors and error recovery pass param to parser Tutorial : means extract the value of the expression inside the angle brackets, here is ... Macros four built-in macros: ?: Expr? get Option> *: Expr* get Vec>, minimum 0 +: Expr+ get Vec>, minimum 1 (...): short for creating an nonterminal, ( \",\")?, mean an \"optionally parse an Expr followed by a comma\" Note the angle brackets (<>) around Expr: these ensures that the value of the ( \",\")is the value of the expression, and not a tuple of the expression and the comma. get type Option> Grammar Example // Term: Num | '(' Term ')' pub Term: i32 = { => n, \"(\" \")\" => t, } // Num: r\"[0-9]+\" Num: i32 = => i32::from_str(&s).unwrap(); Precedence and associativity can be specified using attributes: pub Expr: i32 = { #[precedence(level=\"0\")] // Highest precedence Term, #[precedence(level=\"1\")] #[assoc(side=\"left\")] \"*\" => l * r, \"/\" => l / r, #[precedence(level=\"2\")] #[assoc(side=\"left\")] \"+\" => l + r, \"-\" => l - r, }; Use AST to represent the parsed tree: // ast.rs use std::fmt::{Debug, Error, Formatter}; pub enum Expr { Number(i32), Op(Box, Opcode, Box), Error, } pub enum ExprSymbol { NumSymbol(&'input str), Op(Box>, Opcode, Box>), Error, } #[derive(Copy, Clone)] pub enum Opcode { Mul, Div, Add, Sub, } impl Debug for Expr { fn fmt(&self, fmt: &mut Formatter) -> Result { use self::Expr::*; match *self { Number(n) => write!(fmt, \"{:?}\", n), Op(ref l, op, ref r) => write!(fmt, \"({:?} {:?} {:?})\", l, op, r), Error => write!(fmt, \"error\"), } } } impl Debug for ExprSymbol { fn fmt(&self, fmt: &mut Formatter) -> Result { use self::ExprSymbol::*; match *self { NumSymbol(n) => write!(fmt, \"{:?}\", n), Op(ref l, op, ref r) => write!(fmt, \"({:?} {:?} {:?})\", l, op, r), Error => write!(fmt, \"error\"), } } } impl Debug for Opcode { fn fmt(&self, fmt: &mut Formatter) -> Result { use self::Opcode::*; match *self { Mul => write!(fmt, \"*\"), Div => write!(fmt, \"/\"), Add => write!(fmt, \"+\"), Sub => write!(fmt, \"-\"), } } } // grammar.lalrpop use std::str::FromStr; use crate::ast::{Expr, Opcode}; grammar; pub Expr: Box = { // (1) Expr ExprOp Factor => Box::new(Expr::Op(<>)), // (2) Factor, }; ExprOp: Opcode = { // (3) \"+\" => Opcode::Add, \"-\" => Opcode::Sub, }; Factor: Box = { Factor FactorOp Term => Box::new(Expr::Op(<>)), Term, }; FactorOp: Opcode = { \"*\" => Opcode::Mul, \"/\" => Opcode::Div, }; Term: Box = { Num => Box::new(Expr::Number(<>)), \"(\" \")\" }; Num: i32 = { r\"[0-9]+\" => i32::from_str(<>).unwrap() }; Lexer Num: i32 = r\"[0-9]+\" => i32::from_str(<>).unwrap(); // ~~~ ~~~ ~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~ // | | | Action code // | | Symbol(s) that should match // | Return type // Name of nonterminal +-------------------+ +---------------------+ Text -> | Lexer | -> | Parser | | | | | | Applies regex to | | Consumes terminals, | | produce terminals | | executes your code | +-------------------+ | as it recognizes | | nonterminals | +---------------------+ 词法分析优先级问题：使用 match 显式指定优先级。 Simple match declarations 简单 match 声明 A match declaration lets you explicitly give the precedence between terminals. In its simplest form, it consists of just ordering regular expressions and string literals into groups, with the higher precedence items coming first. So, for example, we could resolve our conflict above by giving r\"[0-9]+\" precedence over r\"\\w+\", thus saying that if something can be lexed as a number, we'll do that, and otherwise consider it to be an identifier. match 声明允许您显式指定终端之间的优先级。在最简单的形式中，它只包括将正则表达式和字符串文本排序到组中，优先级较高的项排在最前面。因此，例如，我们可以通过赋予 r“[0-9]+“优先于 r”\\w+“ 来解决上面的冲突，从而表示如果某个东西可以被解释为一个数字，我们就会这样做，否则就将其视为标识符。 match { r\"[0-9]+\" } else { r\"\\w+\", _ } The final _ indicates that other string literals and regular expressions that appear elsewhere in the grammar (e.g., \"(\" or \"22\") should be added into that final level of precedence (without an _, it is illegal to use a terminal that does not appear in the match declaration). 结尾的 _ 表示出现在语法中其他位置的字符串文字和正则表达式（例如，“（” 或 “22”）应该添加到最终的优先级中（如果没有 _，使用未出现在 match 声明中的终结符是非法的）。 // fixed literals get precedence over regular expressions match { r\"[0-9]+\", \"22\" } else { r\"\\w+\", _ } 使用 match 声明为正则表达式命名，这样我们就不必直接在语法中输入它们。 match { r\"[0-9]+\", \"22\" } else { r\"\\w+\" => ID, // pub Term = { Num, \"(\" \")\", \"22\" => \"Twenty-two!\".to_string(), ID => format!(\"Id({})\", <>), // Customizing skipping between tokens 自定义在标记之间跳过（用于跳过注释内容） match { r\"\\s*\" => { }, // The default whitespace skipping is disabled if an `ignore pattern` is specified r\"//[^\\n\\r]*[\\n\\r]*\" => { }, // Skip `// comments` r\"/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/\" => { }, // Skip `/* comments */` } "},"Learn/LearnPest/pest.html":{"url":"Learn/LearnPest/pest.html","title":"Learn Pest (Rust)","keywords":"","body":"pest. The Elegant Parser pest.rs pest.docs.rs pest.book pest is a general purpose parser written in Rust with a focus on accessibility , correctness , and performance . It uses parsing expression grammars (or PEG) as input, which are similar in spirit to regular expressions, but which offer the enhanced expressivity needed to parse complex languages. pest 是一个用 Rust 编写的通用解析器，重点关注可访问性、正确性和性能。它使用解析表达式语法（或 PEG）作为输入，其本质上与正则表达式类似，但提供了解析复杂语言所需的增强表达能力。 alpha = { 'a'..'z' | 'A'..'Z' } digit = { '0'..'9' } ident = { (alpha | digit)+ } ident_list = _{ !digit ~ ident ~ (\" \" ~ ident)+ } // ^ // ident_list rule is silent (produces no tokens or error reports) int main() { return 5; } - FuncDecl - int_t: \"int \" - Identifier: \"main\" - FormalParams: \"\" - Block > Stmt > Return > Expr > Integer: \"5\" "},"Learn/LearnMLIR/MLIR-LanRef.html":{"url":"Learn/LearnMLIR/MLIR-LanRef.html","title":"Learn MLIR","keywords":"","body":"MLIR Language Reference text text text MLIR 基本上是基于一种类图的数据结构，这种结构由称为操作 (Operations) 的节点和称为值 (Values) 的边组成。每个值都是由一个操作或块参数 (Block Argument) 产生的，并且具有由类型系统定义的值类型。操作包含在块 (Blocks) 中，块包含在区域 (Regions) 中。操作在其所属的块中是有顺序的，块在其所属的区域中也是有顺序的，尽管这种顺序在某些类型的区域中可能具有或不具有语义意义。操作还可以包含区域，从而能够表示层次结构。 操作可以表示许多不同的概念，从高层概念（如函数定义、函数调用、缓冲区分配、缓冲区的视图或切片以及进程创建）到低层概念（如与目标无关的算术、与目标相关的指令、配置寄存器和逻辑门）。这些不同的概念由 MLIR 中的不同操作表示，并且 MLIR 中可用的操作集可以任意扩展。 MLIR 还提供了一个可扩展的框架，用于对操作进行转换，使用了编译器 Passes 的常见概念。对任意操作集启用任意转换集会带来显著的扩展挑战，因为每个转换都必须可能考虑到任何操作的语义。MLIR 通过允许操作语义使用特性 (Traits) 和接口 (Interfaces) 进行抽象描述，解决了这一复杂性，从而使转换能够更通用地操作这些操作。特性通常描述了对有效中间表示 (IR) 的验证约束，从而能够捕获并检查复杂的不变量。（参见 Op vs Operation） MLIR 的一个明显应用是表示基于静态单赋值形式 (SSA) 的中间表示 (IR)，如 LLVM 核心 IR，使用适当的操作类型来定义模块、函数、分支、内存分配和验证约束，以确保 SSA 主导性 (Dominance) 属性。MLIR 包含了一组定义了这些结构的方言 (Dialects)。然而，MLIR 的设计足够通用，也可以表示其他类似编译器的数据结构，例如语言前端中的抽象语法树、目标特定后端中生成的指令或高级综合工具中的电路。 text 关于MLIR基本模块学习过程如下： Dialect, Attribute, Type, Operation；想象如果自己去实现，该怎么设计类； DialectConversion；想象在自己实现的前四个模块上，如何实现DialectConversion； Interface, Constraint, Trait；同样，想象自己会怎么增加这些功能； Transformation, Concalization； Region, Block：基于1. 设计的Operation，以及4. 增加的Transformation，想象如何对Operation进行抽象，提取出Region和Block的概念； Pass； 最后才是ODS和DRR。 MLIR中Dialects分类及关联 OpenXLA openxla An open ecosystem of performant, portable, and extensible machine learning (ML) infrastructure components that simplify ML development by defragmenting the tools between frontend frameworks and hardware backends. Built by industry leaders in AI modeling, software, and hardware. 一个由高性能、可移植和可扩展的机器学习 (ML) 基础设施组件组成的开放生态系统，可通过对前端框架和硬件后端之间的工具进行碎片整理来简化 ML 开发。由 AI 建模、软件和硬件领域的行业领导者构建。 XLA XLA (Accelerated Linear Algebra) is an open source compiler for machine learning. The XLA compiler takes models from popular frameworks such as PyTorch, TensorFlow, and JAX, and optimizes the models for high-performance execution across different hardware platforms including GPUs, CPUs, and ML accelerators. XLA（加速线性代数）是一个用于机器学习的开源编译器。 XLA 编译器采用来自 PyTorch、TensorFlow 和 JAX 等流行框架的模型，并优化模型，以便在不同的硬件平台（包括 GPU、CPU 和 ML 加速器）上实现高性能执行。 Key benefits 主要优点 Build anywhere : XLA is already integrated into leading ML frameworks such as TensorFlow, PyTorch, and JAX. 随处构建：XLA 已集成到领先的 ML 框架中，例如 TensorFlow、PyTorch 和 JAX。 Run anywhere : It supports various backends including GPUs, CPUs, and ML accelerators, and includes a pluggable infrastructure to add support for more. 随处运行：它支持各种后端，包括 GPU、CPU 和 ML 加速器，并包含可插拔基础设施以增加对更多后端的支持。 Maximize and scale performance : It optimizes a model's performance with production-tested optimization passes and automated partitioning for model parallelism. 最大化和扩展性能：它通过经过生产测试的优化过程和模型并行性的自动分区来优化模型的性能。 Eliminate complexity : It leverages the power of MLIR to bring the best capabilities into a single compiler toolchain, so you don't have to manage a range of domain-specific compilers. 消除复杂性：它利用了 MLIR将最佳功能引入单个编译器工具链，因此您无需管理一系列特定于域的编译器。 Future ready : As an open source project, built through a collaboration of leading ML hardware and software vendors, XLA is designed to operate at the cutting-edge of the ML industry. 面向未来：作为一个开源项目，通过领先的 ML 硬件和软件供应商的合作构建，XLA 旨在在 ML 行业的前沿运行。 StableHLO StableHLO is an operation set for high-level operations (HLO) in machine learning (ML) models. Essentially, it's a portability layer between different ML frameworks and ML compilers: ML frameworks that produce StableHLO programs are compatible with ML compilers that consume StableHLO programs. StableHLO 是机器学习 (ML) 模型中高级操作 (HLO) 的操作集。本质上，它是不同 ML 框架和 ML 编译器之间的可移植层：生成 StableHLO 程序的 ML 框架与使用 StableHLO 程序的 ML 编译器兼容。 Shardy Shardy is an MLIR-based tensor partitioning system for all dialects. Built from the collaboration of both the GSPMD and PartIR teams, it incorporates the best of both systems, and the shared experience of both teams and users. Shardy 是一个基于 MLIR 的张量划分系统，适用于所有方言。它由 GSPMD 和 PartIR 团队合作构建，融合了两个系统的优点以及团队和用户的共享体验。 PJRT PJRT is a hardware and framework independent interface for ML compilers and runtimes. It is currently included with the XLA distribution. See the XLA GitHub and documentation for more information on how to use and integrate PJRT. PJRT 是用于 ML 编译器和运行时的独立于硬件和框架的接口。目前它包含在 XLA 发行版中。有关如何使用和集成 PJRT 的更多信息，请参阅 XLA GitHub 和文档。 TF(TensorFlow) Dialect This dialect maps to TensorFlow operations. 该方言映射到 TensorFlow 操作。 Invariants: 不变量： All values are of Tensor type (in particular, scalars are represented using zero-dimensional tensors); 所有值都是 Tensor 类型（特别是标量使用零维张量表示）； TODO: Make invariants more structured so that we can reference them in ops.TODO： 使不变量更加结构化，以便我们可以在操作中引用它们。 "},"Learn/LearnRust/Resources.html":{"url":"Learn/LearnRust/Resources.html","title":"Learn Rust","keywords":"","body":"Rust Resources Rust The Rust Reference Rust Ref: zh ~~ The Rust Programming Language The Rust Programming Language: Experiment Type Rust 程序设计语言 (zh) - 2022 Rust 程序设计语言 (zh) - 2024-05-02 ~~ effective rust: 35 Specific Ways to Improve Your Rust Code effective-rust-cn ~~ The Rustonomicon: The Dark Arts of Unsafe Rust Rust 语言圣经(Rust Course) rust-by-example The Cargo Book The Little Book of Rust Macros ~~ Rust RFCs - RFC Book ~~ releases github releases ~~ Rust 语言中文社区 clippy Rust Design Patterns Newtype Index Pattern Embrace the newtype pattern -- Effective Rust Idiomatic tree and graph like structures in Rust Rust Compiler cranelift: a fast, secure, relatively simple and innovative compiler backend rustc_codegen_cranelift wasmtime: About A lightweight WebAssembly runtime that is fast, secure, and standards-compliant Rust OS NUDT-OS-Book rcore-os rCore-Tutorial-Book-v3 haibo_chen Others 一文读懂什么是进程、线程、协程 rust-cli Joel on Software git submodule update --init --recursive Commands # rustup: Install, manage, and update Rust toolchains. rustup install/default/update/show rustup self uninstall # cargo: Rust's package manager and build system. cargo new # create a new Rust project cargo build # build the current package cargo run # build and run the current package cargo check # check the current package for errors without building cargo test # run the tests in the current package cargo fmt # check formatting of the current package cargo build --release # build the current package with optimizations cargo doc --open # build all dependences doc and open in broswer RUST_BACKTRACE=1 cargo run # checkout backtrace OwnerShip Rust 中的每一个值都有一个 所有者 （ owner ）。 值在任一时刻有且只有一个所有者。 当所有者（变量）离开作用域，这个值将被丢弃。 引用 在任意给定时间，要么 只能有一个可变引用，要么 只能有多个不可变引用。 引用必须总是有效的。 Slice 类型 slice 允许你引用集合中一段连续的元素序列，而不用引用整个集合。slice 是一种引用，所以它没有所有权。 fn first_word(s: &String) -> &str { let bytes = s.as_bytes(); for (i, &item) in bytes.iter().enumerate() { if item == b' ' { return &s[0..i]; } } &s[..] } fn main() { let my_string = String::from(\"hello world\"); // `first_word` 适用于 `String`（的 slice），部分或全部 let word = first_word(&my_string[0..6]); let word = first_word(&my_string[..]); // `first_word` 也适用于 `String` 的引用， // 这等价于整个 `String` 的 slice let word = first_word(&my_string); let my_string_literal = \"hello world\"; // `first_word` 适用于字符串字面值，部分或全部 let word = first_word(&my_string_literal[0..6]); let word = first_word(&my_string_literal[..]); // 因为字符串字面值已经 **是** 字符串 slice 了， // 这也是适用的，无需 slice 语法！ let word = first_word(my_string_literal); } Struct 字段初始化简写语法（field init shorthand） fn build_user(email: String, username: String) -> User { User { active: true, username, email, sign_in_count: 1, } } 结构体更新语法（struct update syntax） 使用旧实例的大部分值但改变其部分值来创建一个新的结构体实例 fn main() { // --snip-- let user2 = User { email: String::from(\"another@example.com\"), ..user1 }; } 示例 5-7 中的代码也在 user2 中创建了一个新实例，但该实例中 email 字段的值与 user1 不同，而 username、 active 和 sign_in_count 字段的值与 user1 相同。..user1 必须放在最后，以指定其余的字段应从 user1 的相应字段中获取其值，但我们可以选择以任何顺序为任意字段指定值，而不用考虑结构体定义中字段的顺序。 请注意，结构更新语法就像带有 = 的赋值，因为它移动了数据，就像我们在“变量与数据交互的方式（一）：移动”部分讲到的一样。在这个例子中，总体上说我们在创建 user2 后就不能再使用 user1 了，因为 user1 的 username 字段中的 String 被移到 user2 中。如果我们给 user2 的 email 和 username 都赋予新的 String 值，从而只使用 user1 的 active 和 sign_in_count 值，那么 user1 在创建 user2 后仍然有效。active 和 sign_in_count 的类型是实现 Copy trait 的类型，所以我们在“变量与数据交互的方式（二）：克隆” 部分讨论的行为同样适用。 元组结构体（tuple structs） struct Color(i32, i32, i32); struct Point(i32, i32, i32); fn main() { let black = Color(0, 0, 0); let origin = Point(0, 0, 0); } 类单元结构体（unit-like structs） 没有任何字段的结构体 类单元结构体常常在你想要在某个类型上实现 trait 但不需要在类型中存储数据的时候发挥作用。 struct AlwaysEqual; fn main() { let subject = AlwaysEqual; } sdsd dsd "},"Learn/LearnROS/ROS.html":{"url":"Learn/LearnROS/ROS.html","title":"Learn ROS","keywords":"","body":"ROS: Robot Operating System index.ros.org/ wiki.ros: Tutorials wiki.ros: cn Introduction github: ros-infrastructure github: ROS core stacks github: ros-dpg rep: learn-ros blog: ros-tutorials ROS（机器人操作系统）提供库和工具来帮助软件开发人员创建机器人应用程序。它提供硬件抽象、设备驱动程序、库、可视化工具、消息传递、包管理等。 ROS Releases/Distributions ros: Distributions ros2: Releases ros - os version match ROS: ROS Box Turtle 2010.03.02 ROS C Turtle 2010.08.02 ROS Diamondback 2011.03.01 ROS Electric Emys 2011.08.30 ROS Fuerte Turtle 2012.04.23 ROS Groovy Galapagos 2012.12.31 ROS Hydro Medusa 2013.09.04 ROS Indigo Igloo 2014.07.22 ROS Jade Turtle 2015.05.23 ROS Kinetic Kame 2016.05.23 ROS Lunar Loggerhead 2017.05.23 ROS Melodic Morenia 2018.05.23 ROS Noetic Ninjemys 2020.05.23 ROS2: ROS 2 Ardent Apalone 2017.12.08 ROS 2 Bouncy Bolson 2018.05.31 ROS 2 Crystal Clemmys 2018.12.12 ROS 2 Dashing Diademata 2019.05.31 ROS 2 Eloquent Elusor 2019.12.12 ROS 2 Foxy Fitzroy 2020.06.05 ROS 2 Galactic Geochelone 2021.05.23 ROS 2 Humble Hawksbill 2022.05.23 ROS 2 Iron Irwini 2023.05.23 ROS 2 Jazzy Jalisco 2024.05.23 There is a new ROS 2 distribution released yearly on May 23rd (World Turtle Day). ROS Noetic Ninjemys wiki ROS Noetic Ninjemys is the thirteenth ROS distribution release. It was released on May 23rd, 2020. Concepts REP: ROS Enhancement Proposals REPs are documents that define standards, conventions, and best practices for the ROS ecosystem. They are similar to RFCs (Request for Comments) in the internet protocol community or PEPs (Python Enhancement Proposals) in the Python community. Filesystem Level Packages Metapackages Package Manifests Repositories Message (msg) types: Message Description, stored in my_package/msg/MyMessageType.msg Service (srv) types: Service Description, stored in my_package/srv/MyServiceType.srv Graph Level Nodes: process that performs computation Master: provides name registration and lookup to the rest of the Computation Graph Parameter Server: allows data to be stored by key in a central location Messages: data structure for communication Topics: messages are routed via a transport system with publish/subscribe semantics a node sends a message by publishing it to a given topic a node receives a message by subscribing to the appropriate topic Services: request/reply is done via a service a node offers a service under a specific name a client uses the service by sending the request message and awaiting the reply Bags: a format for saving and playing back ROS message data mechanism for storing ROS message data, such as sensor data Community Level Distributions Repositories ROS Wiki ... names: Package Resource Names and Graph Resource Names Graph Resource Names: provides a hierarchical naming structure that is used for all resources in ROS Computation Graph Graph Resource Names are an important mechanism in ROS for providing encapsulation. Each resource is defined within a namespace, which it may share with many other resources. In general, resources can create resources within their namespace and they can access resources within or above their own namespace. Connections can be made between resources in distinct namespaces, but this is generally done by integration code above both namespaces. This encapsulation isolates different portions of the system from accidentally grabbing the wrong named resource or globally hijacking names. 每个资源都在一个命名空间中定义，该命名空间可以与许多其他资源共享。 通常，资源可以在其命名空间中创建资源，并且可以访问其自己的命名空间内或之上的资源。 可以在不同命名空间中的资源之间建立连接，但这通常是通过两个命名空间上方的集成代码完成的。 这种封装将系统的不同部分与意外获取错误的命名资源或全局劫持名称隔离开来。 /: global namespace four types of Graph Resource Names: base, relative, global, and private base relative/name /global/name ~private/name Package Resource Names \"std_msgs/String\" refers to the \"String\" message type in the \"std_msgs\" Package. Higher-Level Concepts wiki Coordinate Frames/Transforms The tf package provides a distributed, ROS-based framework for calculating the positions of multiple coordinate frames over time. Actions/Tasks The actionlib package defines a common, topic-based interface for preemptible tasks in ROS. Message Ontology common_msgs stack provides a set of common message types for interacting with robots. actionlib_msgs: messages for representing actions diagnostic_msgs: messages for sending diagnostic data. geometry_msgs: messages for representing common geometric primitives. nav_msgs: messages for navigation. sensor_msgs: messages for representing sensor data. Plugins pluginlib package provides tools for writing and dynamically loading plugins using the ROS build system. Filters filters package provides a set of filters for processing data streams. Robot Model The urdf package defines an XML format for representing a robot model and provides a C++ parser. Client Libraries A ROS client library is a collection of code that eases the job of the ROS programmer. It takes many of the ROS concepts and makes them accessible via code. In general, these libraries let you write ROS nodes, publish and subscribe to topics, write and call services, and use the Parameter Server. Such a library can be implemented in any programming language, though the current focus is on providing robust C++ and Python support. ROS 客户端库是简化 ROS 程序员工作的代码集合。它采用了许多 ROS 概念，并使其可以通过代码访问。通常，这些库允许您编写 ROS 节点、发布和订阅主题、编写和调用服务以及使用 Parameter Server。这样的库可以用任何编程语言实现，尽管目前的重点是提供强大的 C++ 和 Python 支持。 roscpp rospy roslisp ... Technical Overview Tools rosdep rosdep is a command-line tool for installing system dependencies. # install sudo apt-get install python3-rosdep # or pip install rosdep # source install git clone https://github.com/ros-infrastructure/rosdep cd rosdep source setup.sh # init rosdep, needs to call only once after installation sudo rosdep init # update rosdep update # install system dependencies # install dependency of a package rosdep install AMAZING_PACKAGE # install dependency of all packages in the workspace # cd into the catkin workspace, run: rosdep install --from-paths src --ignore-src -r -y catkin Low-level build system macros and infrastructure for ROS. wiki.ros: catkin wiki.ros: catkin conceptual overview catkin ros: rep-0128 catkin is the official build system of ROS and the successor to the original ROS build system, rosbuild. catkin combines CMake macros and Python scripts to provide some functionality on top of CMake's normal workflow. catkin was designed to be more conventional than rosbuild, allowing for better distribution of packages, better cross-compiling support, and better portability. catkin's workflow is very similar to CMake's but adds support for automatic 'find package' infrastructure and building multiple, dependent projects at the same time. catkin 是 ROS 的官方构建系统，也是原始 ROS 构建系统 rosbuild 的继承者。catkin 结合了 CMake 宏和 Python 脚本，在 CMake 的正常工作流程之上提供了一些功能。Catkin 的设计比 rosbuild 更传统，允许更好的包分发、更好的交叉编译支持和更好的可移植性。catkin 的工作流程与 CMake 的工作流程非常相似，但增加了对自动“查找包”基础设施的支持，并同时构建多个依赖的项目。 # debian 12 bookworm catkin/stable 0.8.10-9 all # python3 python3-catkin/stable,now 0.8.10-9 al Usage: cd path/to/your/catkin_workspace # will build any packages in /catkin_workspace/src catkin_make # equivalent to cd path/to/your/catkin_workspace cd src catkin_init_workspace cd .. mkdir build cd build cmake ../src -DCMAKE_INSTALL_PREFIX=../install -DCATKIN_DEVEL_PREFIX=../devel make # build specific package catkin_make -DCATKIN_WHITELIST_PACKAGES=\"package1;package2\" # revert back to building all packages: catkin_make -DCATKIN_WHITELIST_PACKAGES=\"\" # generate build and devel dir under workspace root # install catkin_make install # specific source catkin_make --source my_src catkin_make install --source my_src rosinstall_generator generattes .rosinstall files containing information about repositories with ROS packages/stacks. # usage rosinstall_generator PACKAGE DEPENDENCY1 DEPENDENCY2 > PACKAGE.rosinstall # example rosinstall_generator desktop --rosdistro noetic --deps --tar > noetic-desktop.rosinstall vcstool Command-line tools for maintaining a workspace of projects from multiple version-control systems. vcstool provides commands to manage several local SCM repositories (supports git, mercurial, subversion, bazaar) based on a single workspace definition file (.repos or .rosinstall). vcs help # example vcs import --input noetic-desktop.rosinstall ./src "},"Learn/PersonalWebsite/Note.html":{"url":"Learn/PersonalWebsite/Note.html","title":"Learn Personal Website","keywords":"","body":"Note How to Create a Static Website with Jekyll Andrej Karpathy blog - 2014 Why you (yes, you) should blog - 2017 Hexo Theme Nexmoe pages.github hhw-google-blogger Static Site Generator A static site generator builds a website using plain HTML files. When a user visits a website created by a static site generator, it is loaded no differently than if you had created a website with plain HTML. By contrast, a dynamic site running on a server side language, such as PHP, must be built every time a user visits the site. 静态站点生成器使用纯 HTML 文件构建网站。当用户访问由静态网站生成器创建的网站时，其加载方式与您使用纯 HTML 创建网站时没有什么不同。相比之下，在服务器端语言（例如 PHP）上运行的动态站点必须在用户每次访问该站点时构建。 You can treat a static site generator as a very simple sort of CMS (content management system). Instead of having to include your entire header and footer on every page, for example, you can create a header.html and footer.html and load them into each page. Instead of having to write in HTML, you can write in Markdown, which is much faster and more efficient. 您可以将静态站点生成器视为一种非常简单的 CMS（内容管理系统）。例如，您不必在每个页面上包含整个页眉和页脚，您可以创建 header.html 和 footer.html 并将它们加载到每个页面中。您不必使用 HTML 编写，而是可以使用 Markdown 编写，这样更快、更高效。 Here are some of the main advantages of static site generators over dynamic sites:以下是静态站点生成器相对于动态站点的一些主要优点： Speed: your website will perform much faster, as the server does not need to parse any content. It only needs to read plain HTML. 速度: 您的网站将执行得更快，因为服务器不需要解析任何内容。它只需要读取纯 HTML。 Security: your website will be much less vulnerable to attacks, since there is nothing that can be exploited server side. 安全性: 您的网站将更不容易受到攻击，因为服务器端没有任何东西可以被利用。 Simplicity: there are no databases or programming languages to deal with. A simple knowledge of HTML and CSS is enough. 简单性: 无需处理数据库或编程语言。了解 HTML 和 CSS 的简单知识就足够了。 Flexibility: you know exactly how your site works, as you made it from scratch. 灵活性: 您确切地知道您的网站是如何运作的，因为您是从头开始创建的。 Of course, dynamic sites have their advantages as well. The addition of an admin panel makes for ease of updating, especially for those who are not tech-savvy. Generally, a static site generator would not be the best idea for making a CMS for a client. Static site generators also don't have the possibility of updating with real time content. It's important to understand how both work to know what would work best for your particular project. 当然，动态网站也有其优点。添加管理面板可以轻松更新，特别是对于那些不精通技术的人来说。一般来说，静态站点生成器并不是为客户制作 CMS 的最佳主意。静态站点生成器也无法更新实时内容。了解两者的工作原理非常重要，这样才能知道哪种方法最适合您的特定项目。 "},"Learn/LearnMocCUDA/MocCUDA.html":{"url":"Learn/LearnMocCUDA/MocCUDA.html","title":"Learn MocCUDA","keywords":"","body":""},"Learn/LearnPolygeist/Note.html":{"url":"Learn/LearnPolygeist/Note.html","title":"Learn Polygeist","keywords":"","body":"Polygeist: Raising C to Polyhedral MLIR Published in: 2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT) 发表于：2021 年第 30 届国际并行架构与编译技术会议（PACT） Abstract: 摘要： We present Polygeist, a new compilation flow that connects the MLIR compiler infrastructure to cutting edge polyhedral optimization tools. It consists of a C and C++ frontend capable of converting a broad range of existing codes into MLIR suitable for polyhedral transformation and a bi-directional conversion between MLIR and OpenScop exchange format. The Polygeist/MLIR intermediate representation featuring high-level (affine) loop constructs and n-D arrays embedded into a single static assignment (SSA) substrate enables an unprecedented combination of SSA-based and polyhedral optimizations. We illustrate this by proposing and implementing two extra transformations: statement splitting and reduction parallelization. Our evaluation demonstrates that Polygeist outperforms on average both an LLVM IR-level optimizer (Polly) and a source-to-source state-of-the-art polyhedral compiler (Pluto) when exercised on the Polybench/C benchmark suite in sequential (2.53x vs 1.41x, 2.34x) and parallel mode (9.47x vs 3.26x, 7.54x) thanks to the new representation and transformations. 我们介绍 Polygeist，这是一种新的编译流程，它将 MLIR 编译器基础设施与前沿的多面体优化工具连接起来。它包括一个 C 和 C++前端，能够将广泛现有的代码转换为适用于多面体变换的 MLIR，并实现 MLIR 与 OpenScop 交换格式的双向转换。具有高级（仿射）循环构造和嵌入到单个静态赋值（SSA）基底的 Polygeist/MLIR 中间表示，实现了基于 SSA 和多面体优化的前所未有的组合。我们通过提出并实现两个额外的转换来展示这一点：语句拆分和减少并行化。我们的评估表明，在 Polybench/C 基准测试套件上，Polygeist 在顺序（2.53 倍 vs 1.41 倍，2.34 倍）和并行模式（9.47 倍 vs 3.26 倍，7.54 倍）上平均优于 LLVM IR 级别的优化器（Polly）和源到源的前沿多面体编译器（Pluto），这得益于新的表示和转换。 William S. Moses Polygeist: Affine C in MLIR [MLIR Open Design Meeting 02/11/2021] https://www.youtube.com/@billymoses7764 getting_started/Use_Polygeist Retargeting and Respecializing GPU Workloads for Performance Portability 重新定位和针对性能可移植性重新专业化的 GPU 工作负载 In order to come close to peak performance, accelerators like GPUs require significant architecture-specific tuning that understand the availability of shared memory, parallelism, tensor cores, etc. Unfortunately, the pursuit of higher performance and lower costs have led to a significant diversification of architecture designs, even from the same vendor. This creates the need for performance portability across different GPUs, especially important for programs in a particular programming model with a certain architecture in mind. Even when the program can be seamlessly executed on a different architecture, it may suffer a performance penalty due to it not being sized appropriately to the available hardware resources such as fast memory and registers, let alone not using newer advanced features of the architecture. We propose a new approach to improving performance of (legacy) CUDA programs for modern machines by automatically adjusting the amount of work each parallel thread does, and the amount of memory and register resources it requires. By operating within the MLIR compiler infrastructure, we are able to also target AMD GPUs by performing automatic translation from CUDA and simultaneously adjust the program granularity to fit the size of target GPUs. Combined with autotuning assisted by the platform-specific compiler, our approach demonstrates 27% geomean speedup on the Rodinia benchmark suite over baseline CUDA implementation as well as performance parity between similar NVIDIA and AMD GPUs executing the same CUDA program. 为了接近峰值性能，像 GPU 这样的加速器需要针对特定架构进行显著的调整，这些调整理解共享内存、并行性、张量核心等的可用性。不幸的是，追求更高的性能和更低的成本导致了架构设计的显著多样化，即使是来自同一供应商的产品也是如此。这产生了在不同 GPU 之间实现性能可移植性的需求，这对于特定编程模型和特定架构的程序尤为重要。即使程序可以在不同的架构上无缝执行，它也可能因为未适当调整以适应可用的硬件资源（如快速内存和寄存器）而遭受性能损失，更不用说没有使用架构的新先进特性。我们提出了一种新方法，通过自动调整每个并行线程执行的工作量以及它所需的内存和寄存器资源，来提高（遗留）CUDA 程序在现代机器上的性能。 通过在 MLIR 编译器基础设施中操作，我们能够通过自动从 CUDA 进行翻译来针对 AMD GPU 进行目标定位，同时调整程序粒度以适应目标 GPU 的大小。结合平台特定编译器辅助的自动调整，我们的方法在 Rodinia 基准测试套件上相对于基线 CUDA 实现实现了 27%的几何平均速度提升，以及执行相同 CUDA 程序时类似 NVIDIA 和 AMD GPU 之间的性能对等。 Frontend Performance Differences 8% performance boost on Floyd-Warshall occurs if Polygeist generates a single MLIR module for both benchmarking and timing code by default MLIR doesn't properly generate LLVM datalayout, preventing vectorization for MLIR-generated code (patched in our lowering) Different choice of allocation function can make a 30% impact on some tests (adi) LLVM strength-reduction is fragile and sometimes misses reversed loop induction variable (remaining gap in adi) 如果 Polygeist 默认为基准测试和计时代码生成单个 MLIR 模块，则 Floyd-Warshall 的性能将提升 8% MLIR 无法正确生成 LLVM 数据布局，从而阻止了 MLIR 生成的代码的矢量化（在我们的降低版本中进行了修补） 不同的分配函数选择可能会对某些测试 （adi） 产生 30% 的影响 LLVM 强度降低很脆弱，有时会错过反向环感应变量（ADI 中的剩余间隙） Poligeist MLIR Compiler Frontend Polygeist 的核心功能 Polygeist 的主要目标是 bridging the gap between C/C++ and MLIR。它具有以下核心功能: C/C++前端:能够解析和分析广泛的 C 和 C++代码。 MLIR 生成:将 C/C++代码转换为适合多面体变换的 MLIR 表示。 多面体优化:利用 MLIR 的多面体优化能力进行高级循环优化。 并行优化:支持自动并行化和并行构造的优化。 GPU 后端支持:包括 CUDA 和 ROCm 后端,实现 GPU 加速。 这些功能使 Polygeist 成为连接传统 C/C++代码和现代 MLIR 编译架构的关键工具。 Polygeist 的工作原理 Polygeist 的工作流程可以简要概括为以下几个步骤: 解析 C/C++代码:使用 Clang 的前端能力解析输入的 C/C++代码。 AST 分析:对抽象语法树(AST)进行深入分析,提取程序的结构和语义信息。 MLIR 生成:基于 AST 分析结果,生成对应的 MLIR 表示。 多面体建模:将 MLIR 表示转换为多面体模型,为后续优化铺平道路。 优化应用:应用多面体优化、并行优化等高级优化技术。 代码生成:将优化后的 MLIR 转换回 LLVM IR 或直接生成目标代码。 通过这一系列步骤,Polygeist 能够充分利用 MLIR 的强大功能,同时保持对原始 C/C++代码的兼容性。 Polygeist 的优势与应用 Polygeist 为 C/C++程序带来了诸多优势: 高级优化:通过多面体模型,可以进行更复杂和有效的循环优化。 并行化:自动检测和利用并行机会,提高程序性能。 可移植性:通过 MLIR 表示,可以更容易地将程序移植到不同的硬件平台。 GPU 加速:内置的 CUDA 和 ROCm 后端支持,简化 GPU 编程。 与 LLVM 生态系统集成:作为 LLVM 项目的一部分,可以无缝集成到现有的 LLVM 工具链中。 这些优势使 Polygeist 在高性能计算、科学计算、机器学习等领域具有广泛的应用前景。 实际应用案例 以下是 Polygeist 在实际项目中的应用案例: 科学计算优化:在一个大规模数值模拟项目中,使用 Polygeist 对核心计算 kernel 进行优化,通过多面体变换和自动并行化,性能提升了 30%。 机器学习框架:某开源机器学习框架使用 Polygeist 优化其 C++后端,实现了更高效的张量运算,在某些模型上推理速度提升了 20%。 图形渲染引擎:一个游戏引擎项目利用 Polygeist 的 GPU 后端支持,简化了 CUDA 代码的生成过程,大大提高了开发效率。 这些案例展示了 Polygeist 在提升程序性能和简化开发流程方面的巨大潜力。 cgeist input.c -S -emit-mlir | mlir-opt --canonicalize --cse > output.mlir 2022 LLVMHPC William S. Moses, Polygeist: C++ Frontend for MLIR text The MLIR Framework MLIR is a recent compiler infrastructure designed for reuse and extensibility Rather than providing a predefined set of instructions and types, MLIR operates on collections of dialects that contain sets of interoperable user-defined operations, attributes and types Anyone can define their own optimizable dialect/operation, with a large set of existing dialects (structured control flow, affine, GPU, quantum, fully homomorphic encryption, circuits, LLVM, and more!) MLIR 是一种为重用和可扩展性而设计的最新编译器基础架构 MLIR 不是提供一组预定义的指令和类型,而是对包含一组可互操作的用户定义操作、属性和类型的方言集合进行操作 任何人都可以使用大量现有方言(结构化控制流、仿射、GPU、量子、完全同态加密、电路、LLVM 等!)定义自己可优化的方言/操作。 The Polyhedral Model Represent programs as a collection of computations and constraints on a multi-dimensional grid (polyhedron) Makes it easy to analyze and specify program transformations best exploit the available hardware Loop restructuring for spatial/temporal locality, automatic parallelization, etc. One of the best frameworks for optimizing compute-intensive programs like machine learning kernels or scientific simulations as well as for programming accelerators. Preserve the parallel structure Maintain GPU parallelism in a form understandable to the compiler Enables optimization between caller and kernel Enable parallelism-specific optimization 将程序表示为多维网格(多面体)上的计算和约束集合 便于分析和指定最佳利用可用硬件的程序转换 用于空间/时间局部性、自动并行化等的循环重构 优化机器学习内核或科学模拟等计算密集型程序以及编程加速器的最佳框架之一。 Synchronization via Memory Synchronization (sync_threads) ensures all threads within a block finish executing codeA before executing codeB The desired synchronization behavior can be reproduced by defining sync_threads to have the union of the memory semantics of the code before and after the sync. This prevents code motion of instructions which require the synchronization for correctness, but permits other code motion (e.g. index computation). 同步 （sync_threads） 确保块中的所有线程在执行 CodeB 之前完成对 CodeA 的执行 可以通过定义 sync_threads 来重现所需的同步行为，以便在同步之前和之后具有代码的内存语义的并集。 这可以防止需要同步以确保正确性的指令的代码移动，但允许其他代码移动（例如索引计算）。 High-level synchronization representation enables new optimizations, like sync elimination. A synchronize instruction is not needed if the set of read/writes before the sync don’t conflict with the read/writes after the sync. 高级同步表示支持新的优化，例如同步消除。 如果同步前的读/写集与同步后的读/写集不冲突，则不需要 synchronize 指令。 __global__ void bpnn_layerforward(...) { __shared__ float node[HEIGHT]; __shared__ float weights[HEIGHT][WIDTH]; if ( tx == 0 ) node[ty] = input[index_in] ; // Unnecessary Barrier #1 // None of the read/writes below the sync // (weights, hidden) // intersect with the read/writes above the sync // (node, input) __syncthreads(); // Unnecessary Store #1 weights[ty][tx] = hidden[index]; __syncthreads(); // Unnecessary Load #1 weights[ty][tx] = weights[ty][tx] * node[ty]; // … } GPU Transpilation A unified representation of parallelism enables programs in one parallel architecture (e.g. CUDA) to be compiled to another (e.g. CPU/OpenMP) Most CPU backends do not have an equivalent block synchronization Efficiently lower a top-level synchronization by distributing the parallel for loop around the sync, and interchanging control flow parallel_for %i = 0 to N { codeA(%i); sync_threads; codeB(%i); } ; => parallel_for %i = 0 to N { codeA(%i); } parallel_for %i = 0 to N { codeB(%i); } GPU Synchronization Lowering: Control Flow Synchronization within control flow (for, if, while, etc) can be lowered by splitting around the control flop and interchanging the parallelism. parallel_for %i = 0 to N { for %j = … { codeB1(%i, %j); sync_threads; codeB2(%i, %j); } } ; Interchange => for %j = … { parallel_for %i = 0 to N { codeB1(%i, %j); sync_threads; codeB2(%i, %j); } } ; Split => for %j = … { parallel_for %i = 0 to N { codeB1(%i, %j); } parallel_for %i = 0 to N { codeB2(%i, %j); } } GPU Transpilation Performance CUDA programs transcompiled by Polygeist not only match the performance of handwritten OpenMP programs, but achieve a speedup! 58% geomean speedup on Rodinia 2.7x geomean speedup on PyTorch versus built-in CPU backend (also using our MocCUDA compatibility layer) "},"Learn/LearnPolyhedralModel/Note.html":{"url":"Learn/LearnPolyhedralModel/Note.html","title":"Learn Polyhedral Compilation","keywords":"","body":"Polyhedral Model cs6120-Advanced Compilers text text The polyhedral model (also called the polytope method) is a mathematical framework for programs that perform large numbers of operations -- too large to be explicitly enumerated -- thereby requiring a compact representation. Nested loop programs are the typical, but not the only example, and the most common use of the model is for loop nest optimization in program optimization. The polyhedral method treats each loop iteration within nested loops as lattice points inside mathematical objects called polyhedra, performs affine transformations or more general non-affine transformations such as tiling on the polytopes, and then converts the transformed polytopes into equivalent, but optimized (depending on targeted optimization goal), loop nests through polyhedra scanning. 多面体模型（也称为多面体方法）是一个数学框架，用于执行大量运算的程序 -- 太大而无法显式列举 -- 因此需要紧凑的表示。嵌套循环程序是典型的但不是唯一的例子，该模型最常见的用途是程序优化中的循环嵌套优化。多面体方法将嵌套循环中的每个循环迭代视为称为多面体的数学对象内的格子点，执行仿射变换或更通用的非仿射变换，例如在多面体上平铺，然后通过多面体扫描将转换后的多面体转换为等效但优化（取决于目标优化目标）的循环嵌套。 Polyhedral model in programming Frameworks supporting the polyhedral model 多面体模型及其在循环编译优化中的应用 多面体模型是一个用于表示和优化程序的数学框架，尤其适用于涉及循环的程序。它是编译器技术中执行优化的强大工具，尤其在高性能计算中具有重要作用。下面我们将详细解析该概念及其在循环优化中的应用。 多面体模型概述 多面体模型是一个用于描述循环及其迭代的数学抽象，采用多维几何体（即多面体）来表示循环的迭代空间。通过对这些多面体的优化，可以提升程序的执行效率。 在多面体模型中，循环通过以下关键要素进行表示： 循环边界：定义循环变量可能取的范围。 依赖关系：描述循环迭代之间的关系，特别是数据依赖（如读后写或写后读），这些关系必须在转换过程中保持一致。 迭代空间：表示所有可能的循环迭代的多维空间，通常以多面体形式可视化，每个点代表一个循环的迭代。 多面体模型的工作原理 循环表示：在多面体模型中，循环通过一组数学不等式表示，这些不等式定义了循环变量的边界。例如，嵌套循环可以用一组不等式来描述，这些不等式构成一个多面体，其中每个点对应循环的一个迭代。 依赖分析：在循环程序中，不同迭代之间可能会存在数据依赖。多面体模型可以帮助正式分析这些依赖，判断哪些迭代可以并行执行，哪些需要按照特定顺序执行以保证程序正确性。 转换操作：多面体模型允许对循环进行各种转换，例如： 循环融合：将两个循环合并为一个循环，以提高数据局部性。 循环分裂：将一个循环拆分为两个，以提高并行性或数据局部性。 循环块化（Tiling）：将循环迭代空间分成更小的块（tile），以提高缓存使用效率和局部性。 循环交换：改变嵌套循环的执行顺序，以改善内存访问模式或并行性。 多面体模型在循环编译优化中的应用 并行化：多面体模型可以帮助识别并行执行的机会。通过分析迭代空间和数据依赖关系，模型能够判断哪些循环迭代可以安全地并行执行，这对于多核处理器的利用至关重要。 缓存优化：多面体模型通过转换循环来优化内存访问模式，提升数据局部性。例如，通过应用循环块化或分块技术，模型确保多个循环迭代访问的数据可以适配到缓存中，从而减少内存延迟。 矢量化：多面体模型可以帮助判断哪些循环部分可以进行矢量化，从而更好地利用现代处理器的 SIMD（单指令多数据）指令集。 乱序执行：通过分析循环依赖关系，多面体模型可以帮助编译器确定哪些迭代可以乱序执行，从而提高指令级并行性，并在现代处理器上获得更好的性能。 自动并行化：该模型在自动并行化编译器中扮演重要角色，通过分析循环结构和依赖关系，它可以决定哪些循环或循环部分可以安全并行化。 针对特定架构的优化：多面体模型可以生成针对特定硬件的优化代码，如 GPU，通过应用适合该架构内存层次结构和处理能力的转换来提升性能。 多面体模型的优点 数学严谨性：多面体模型提供了一种精确且正式的方式来描述循环，使编译器可以更容易地推理循环优化。 表达能力强：它能够在一个统一的框架中表达多种循环优化，如并行化、局部性优化和矢量化。 自动化：它能够自动优化代码，减少手动调整的需求。 多面体模型的挑战 复杂性：多面体模型数学上较为复杂，在编译器中实现起来可能具有一定难度，尤其是在处理具有复杂依赖关系的程序时。 可扩展性：对于大规模应用程序，管理多面体表示的开销可能会非常大，尤其是在内存和计算上。 示例 考虑一个简单的嵌套循环： for (i = 0; i 在多面体模型中，i和j的迭代空间被表示为一个二维多面体。依赖分析表明，内循环的每次迭代依赖于前几次迭代的数据。基于此，模型可以应用循环融合或块化等优化技术，从而提高缓存使用效率或并行性。 结论 多面体模型是一个用于优化循环的复杂工具。通过将循环表示为多面体并分析其依赖关系，模型能够实现包括并行化、循环转换（如块化、融合等）和内存优化在内的多种优化。尽管它的实现较为复杂，但它已成为高性能编译技术的基础，尤其在科学计算和高性能应用中具有重要意义。 The Polyhedral Model is a mathematical framework used to represent and optimize programs, particularly those involving loops. It is a powerful tool for performing optimization in compiler technology, especially in high-performance computing. Let's break down the concept and its application to loop optimization. Polyhedral Model Overview The Polyhedral Model is a mathematical abstraction used to describe loops and their iterations in terms of multi-dimensional shapes, or polyhedra. These polyhedra represent the iteration spaces of loops, and their optimization helps improve the performance of programs. In the polyhedral model, loops are represented using the following key components: Loop Bounds: The range of indices that a loop variable can take. Dependences: The relationships between iterations of loops, specifically data dependencies (such as read-after-write or write-after-read), which must be respected during transformations. Iteration Spaces: The multi-dimensional space that represents all possible iterations of the loops. These are often visualized as polyhedra (multi-dimensional geometric objects) where each point corresponds to an iteration of the loop. How the Polyhedral Model Works Representation of Loops: In the polyhedral model, loops are represented in a high-level mathematical form. For example, a nested loop can be described by a set of inequalities that define the bounds of the loop's variables. This set of inequalities is a polyhedron, where each point corresponds to an iteration of the loop. Dependence Analysis: In loop programs, data can be dependent across different iterations. The polyhedral model enables the formal analysis of such dependencies, helping identify if iterations can be executed in parallel or need to be executed in a specific order to preserve correctness. Transformations: The polyhedral model allows for a variety of transformations on the loops, such as: Loop Fusion: Combining two loops into one to improve data locality. Loop Fission: Splitting a loop into two to enhance parallelism or data locality. Loop Tiling (Blocking): Dividing the loop iteration space into smaller blocks (tiles) to improve cache usage and locality. Loop Interchange: Changing the order of nested loops to improve memory access patterns or parallelism. Applications in Loop Compilation and Optimization Parallelism: The polyhedral model helps identify opportunities for parallel execution. By analyzing the iteration space and data dependencies, it can determine whether iterations can be safely executed in parallel, which is crucial for leveraging multi-core processors. Cache Optimization: The polyhedral model can help optimize memory access patterns by transforming loops to improve data locality. For example, by applying loop blocking or tiling, the model can ensure that data accessed by multiple iterations of the loop fit within the cache, reducing memory latency. Vectorization: The polyhedral model aids in determining which parts of the loop can be vectorized, allowing for better utilization of SIMD (Single Instruction, Multiple Data) instructions on modern processors. Out-of-Order Execution: By analyzing loop dependencies, the polyhedral model helps compilers determine which iterations can be executed out of order, improving instruction-level parallelism and performance on modern processors. Automatic Parallelization: The model plays a crucial role in compilers that automatically parallelize code. By analyzing the loop structure and dependencies, it can decide which loops or parts of loops can be parallelized safely. Optimization for Specific Architectures: The polyhedral model can be used to generate optimized code for specific hardware, such as GPUs, by applying transformations that are well-suited for the architecture's memory hierarchy and processing capabilities. Advantages of the Polyhedral Model Mathematical Rigor: The polyhedral model provides a formal and precise way to describe loops, making it easier for compilers to reason about loop optimizations. Expressive Power: It can express a wide range of loop optimizations, such as parallelism, locality, and vectorization, in a unified framework. Automation: It enables automatic optimization of code, reducing the need for manual tuning. Challenges of the Polyhedral Model Complexity: The polyhedral model is mathematically complex and can be difficult to implement in compilers, especially for very large programs with intricate dependencies. Scalability: For large-scale applications, the overhead of managing polyhedral representations can be significant, especially in terms of memory and computation. Example Consider a simple nested loop: for (i = 0; i The polyhedral model represents the iteration space of i and j as a 2D polyhedron. Dependence analysis would show that each iteration of the inner loop depends on data from the previous iterations. Based on this, the polyhedral model can apply optimizations like loop fusion or tiling to improve cache usage or parallelism. Conclusion The Polyhedral Model is a sophisticated tool used in compilers for optimizing loops. By representing loops as polyhedra and analyzing their dependencies, it allows for a range of optimizations, including parallelization, loop transformations (tiling, fusion, etc.), and memory optimizations. Although it can be complex to implement, it has become a foundational technique for high-performance compilation, especially in scientific computing and high-performance applications. "},"Learn/LearnROCm/ROCm.html":{"url":"Learn/LearnROCm/ROCm.html","title":"Learn ROCm","keywords":"","body":"Optimized GPU Software Stack 优化的 GPU 软件堆栈 AMD ROCm™ is an open software stack including drivers, development tools, and APIs that enable GPU programming from low-level kernel to end-user applications. ROCm is optimized for Generative AI and HPC applications, and is easy to migrate existing code into. AMD ROCm™ 是一个开放软件堆栈，包括驱动程序、开发工具和 API，支持从低级内核到最终用户应用程序的 GPU 编程。 ROCm 针对生成式 AI 和 HPC 应用程序进行了优化，并且易于将现有代码迁移到其中。 ROCm 3 是用于图形处理单元(GPU) 编程的Advanced Micro Devices (AMD) 软件堆栈。 ROCm 跨越多个领域：图形处理单元上的通用计算(GPGPU)、高性能计算(HPC)、异构计算。它提供了多种编程模型： HIP （基于 GPU 内核的编程）、 OpenMP （基于指令的编程）和OpenCL 。 wiki/ROCm AMD ROCm documentation "},"Learn/LearnJinJia2/Jinjia2.html":{"url":"Learn/LearnJinJia2/Jinjia2.html","title":"Learn JinJia2","keywords":"","body":"Jinjia2 from jinja2 import Template # 定义模板 template = Template(open(\"test.jinja2\").read()) # 渲染模板 output = template.render(target=\"RISCV\") # 打印生成的 HTML print(output) "},"Misc/Misc.html":{"url":"Misc/Misc.html","title":"Misc","keywords":"","body":""},"Misc/00_Links.html":{"url":"Misc/00_Links.html","title":"Useful Links","keywords":"","body":"Useful Links Collection 导航 s CS 自学指南 网站导航 linux.do 网站导航 Blogs linux.cn liaoxuefeng Andrej Karpathy blog - 2014 Why you (yes, you) should blog - 2017 Hacker News Daily Tools pdf-to-txt table to markdown Tutorials GDB 调试多线程程序的总结 Linux 101 Courses CS615 -- System Administration Articles What Every Programmer Should Know About Memory - Traditional Chinese Misc gnu home zh The best free stock photos, royalty free images & videos shared by creators. ToRead: zhihu - Performance and Compatibility in the HongMeng Production Microkernel zhihu - Programming Books to read Awesome Lists: Awesome-Selfhosted Radio to Txt blog: 几款免费的语音转文字工具推荐 飞书妙记 buzz: Transcribe and translate audio offline on your personal computer. Powered by OpenAI's Whisper. Online Convert onlineconvertfree caj2pdf caj2pdf gentleltd github: caj2pdf text tools delete all enters doc zealdocs video download yt pkgs pkgs: Packages for Linux and Unix repology: the packaging hub "},"Misc/01_GitHubReps.html":{"url":"Misc/01_GitHubReps.html","title":"Github Reps","keywords":"","body":"docling https://ds4sd.github.io/docling/ 这是一个由 IBM 开源的 Python 工具，专门用于将各类文档转化为适合生成式 AI 使用的工具。它能够将 PDF、DOCX、PPTX、图片、HTML、Markdown 等多种流行文档格式，导出为 Markdown 和 JSON 格式，支持多种 OCR 引擎（PDF）、统一的文档对象（DoclingDocument），轻松集成检索增强生成（RAG）和问答应用，适用于需要将文档作为生成式 AI 模型输入的场景。 best-of-ml-python 该项目提供了一个高质量的机器学习 Python 库列表，包含超过 900 个开源项目，并按照项目质量评分进行排名，每周更新一次。所有开源项目被分成了 30 多个分类，包括机器学习框架、数据可视化、自然语言处理、OCR、模型序部署等，便于不同应用领域的开发者快速找到所需的机器学习工具和资源。 bananas: cross-platform screen sharing bananas官网：https://getbananas.net 云朵备份发布地址：https://github.com/likeflyme/cloudbak 云朵备份官网：https://www.cloudbak.org/ 使用教程：https://www.cloudbak.org/use/create-session.html 魔镜：项目地址https://github.com/idootop/MagicMirror/blob/main/docs/cn/readme.md 作者提供的安装包，提取密码: 4ro2：https://del-wang.lanzout.com/b01qdt5nba 一键马赛克：https://github.com/Ritr/publicTools Traymond 增强版：https://github.com/tabris17/traymond 壹印：https://github.com/ggchivalrous/yiyin 仿生阅读：https://github.com/yitong2333/Bionic-Reading/blob/main/README-CN.md "},"Misc/GitBook.html":{"url":"Misc/GitBook.html","title":"Gitbook","keywords":"","body":"GitBook Product documentation (your users will love) Forget building your own custom docs platform. With GitBook you get beautiful documentation for your users, and a branch-based Git workflow for your team. gitbook.com gitbook-documentation zh gitbook-cli Extensions include-codeblock Error: file not found: /home/runner/work/Notes/Notes/Misc/test.py Install # install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash # install node version 10.24.1 nvm install 10.24.1 # install gitbook-cli (with change npm source) npm config set registry https://registry.npmmirror.com npm install gitbook-cli -g # install gitbook gitbook -V # 下载添加的插件 & build gitbook install gitbook build # generate static files under `_book` directory # start server: localhost:4000 gitbook serve 格式 格式主要注重简单和易读性 GitBook 约定了下面这些文件的作用： README：书本的介绍 SUMMARY：章节结构, 用来生成书本内容的预览表。 LANGS：多语言书籍 GLOSSARY：术语描述的清单 至少需要一个 README 和 SUMMARY 文件来构建一本书。 "},"Misc/Typst.html":{"url":"Misc/Typst.html","title":"Typst","keywords":"","body":"Typst Links Typst polylux: package to create presentation slides. Reference #show rule #show \"ArtosFlow\": name => box[ #box(image( \"logo.svg\", height: 0.7em, )) #name ] This report is embedded in the ArtosFlow project. ArtosFlow is a project of the Artos Institute. With show rules, you can redefine how Typst displays certain elements. You specify which elements Typst should show differently and how they should look. Show rules can be applied to instances of text, many functions, and even the whole document. 使用 show rules，您可以重新定义 Typst 显示某些元素的方式。您可以指定 Typst 应该以不同的方式显示哪些元素以及它们应该如何显示。显示规则可以应用于文本实例、许多函数，甚至整个文档。 There is a lot of new syntax in this example: We write the show keyword, followed by a string of text we want to show differently and a colon. Then, we write a function that takes the content that shall be shown as an argument. Here, we called that argument name. We can now use the name variable in the function's body to print the ArtosFlow name. Our show rule adds the logo image in front of the name and puts the result into a box to prevent linebreaks from occurring between logo and name. The image is also put inside of a box, so that it does not appear in its own paragraph. 这个例子中有很多新的语法：我们编写 show 关键字，后跟我们想要以不同方式显示的一串文本和一个冒号。然后，我们编写一个函数，该函数将应显示为参数的内容。在这里，我们将该参数称为 name。我们现在可以使用函数体中的 name 变量来打印 ArtosFlow 名称。我们的 show 规则将 logo 图像添加到名称前面，并将结果放入一个框中，以防止 logo 和 name 之间出现换行。图像也被放在一个框内，这样它就不会出现在自己的段落中。 "},"Misc/Markdown.html":{"url":"Misc/Markdown.html","title":"Markdown","keywords":"","body":"Markdown vscode extenssion Markdown All in One Key Command Ctrl/Cmd + B Toggle bold Ctrl/Cmd + I Toggle italic Alt+S (on Windows) Toggle strikethrough1 Ctrl + Shift + ] Toggle heading (uplevel) Ctrl + Shift + [ Toggle heading (downlevel) Ctrl/Cmd + M Toggle math environment Alt + C Check/Uncheck task list item Ctrl/Cmd + Shift + V Toggle preview Ctrl/Cmd + K V Toggle preview to side Tips Complete list of github markdown emoji markup developer-icons devicon "},"Misc/VScode.html":{"url":"Misc/VScode.html","title":"VScode","keywords":"","body":"VScode Shortcuts Ctrl+p 快速打开文件 Ctrl+shift+p 打开命令面板 Ctrl+w 关闭当前文件 Ctrl+k+w 关闭所有文件 Ctrl+f 快速搜索 Ctrl+g 跳转到指定行 Ctrl+Alt+f Find in Explorer Ctrl+R Run Recent Command Others 文件恢复 在 vscode 界面，按住 Ctrl+shift+p 打开命令面板，找到本地历史记录 VScode server 确认 server commit_id # ~/.vscode-server/bin # 下载对应的server程序 # （注意把:${commit_id}替换成对应的Commit ID） https://update.code.visualstudio.com/commit:${commit_id}/server-linux-x64/stable # vscode-server-linux-x64.tar.gz # 放到 ~/.vscode-server/bin/${commit_id}/ 文件夹下 # 解压 tar -zxvf vscode-server-linux-x64.tar.gz --strip=1 https://update.code.visualstudio.com/commit:e7e037083ff4455cf320e344325dacb480062c3c/server-linux-x64/stable Debugging Launch Configuration Debugging Launch Configuration "},"Misc/Bochs.html":{"url":"Misc/Bochs.html","title":"Bochs","keywords":"","body":"Bochs 欢迎来到Bochs IA-32仿真器项目 Bochs是一个用C++编写的高度可移植的开源IA-32(x86)PC仿真器，可以在大多数流行的平台上运行。它包括对英特尔x86 CPU、普通I/O设备和自定义BIOS的仿真。Bochs可以编译模拟许多不同的x86 CPU，从早期的386到最新的x86-64英特尔和AMD处理器，甚至可能还没有进入市场。 Bochs能够在仿真中运行大多数操作系统，包括Linux、DOS或Microsoft Windows。Bochs最初由Kevin Lawton编写，目前由该项目维护。 Bochs可以被编译并以多种模式使用，其中一些模式仍在开发中。bochs的 \"典型 \"用途是提供完整的x86 PC仿真，包括x86处理器、硬件设备和内存。这允许你在工作站的模拟器中运行操作系统和软件，就像你在一台机器中拥有一台机器一样。例如，假设你的工作站是一个Unix/X11工作站，但你想运行Win'95应用程序。Bochs将允许你在Unix/X11工作站上运行Win'95和相关软件，在你的工作站上显示一个窗口，模拟PC上的显示器。 Bochs是一个仿真器（emulator）--不是虚拟化软件（virtualization software）。它可以在许多架构上移植：X86、ARM、MIPS等。这意味着它必须能够模拟每一条CPU指令。 这使Bochs区别于虚拟化解决方案，例如VirtualBox、VMWare等。那些项目提供了很好的用户体验和快速的性能，其代价是硬件限制、一些非确定性和一些必要的黑客攻击来使程序工作。 Bochs的仿真提供了一个可控的、精确的执行环境，但代价是速度/性能。这在某些情况下可能是有利的，例如： 在开发操作系统或引导程序时 当处理非常老的、关键任务的软件时 当对系统级代码进行逆向工程时 欲了解更多信息，请参见用户指南中的介绍部分 "},"Misc/RegularExp.html":{"url":"Misc/RegularExp.html","title":"Regular Exp","keywords":"","body":"Regular Expressions wikipedia runoob [^a-z] // 除了小写字母以外的所有字符 [^\\\\\\/\\^] // 除了(\\)(/)(^)之外的所有字符 [^\\\"\\'] // 除了双引号(\")和单引号(')之外的所有字符 a.*b // a后面可以有任意字符，直到遇到b "},"Misc/FuzzTesting/FuzzTesting.html":{"url":"Misc/FuzzTesting/FuzzTesting.html","title":"FuzzTesting","keywords":"","body":"Fuzz Testing wiki: Fuzz testing In programming and software development, fuzzing or fuzz testing is an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions such as crashes) \"Crash (computing)\"), failing built-in code assertions) \"Assertion (software development)\"), or potential memory leaks. Typically, fuzzers are used to test programs that take structured inputs. This structure is specified, such as in a file format or protocol and distinguishes valid from invalid input. An effective fuzzer generates semi-valid inputs that are \"valid enough\" in that they are not directly rejected by the parser, but do create unexpected behaviors deeper in the program and are \"invalid enough\" to expose corner cases that have not been properly dealt with. 在编程和软件开发中，模糊测试或模糊测试是一种自动化软件测试技术，涉及提供无效、意外或随机的数据作为计算机程序的输入。然后，将监视程序是否存在异常，例如崩溃、内置代码断言失败或潜在的内存泄漏。通常，模糊测试程序用于测试采用结构化输入的程序。此结构是指定的，例如以文件格式或协议指定，并区分有效输入和无效输入。有效的模糊测试器会生成“足够有效”的半有效输入，因为它们不会被解析器直接拒绝，但确实会在程序的更深处产生意想不到的行为，并且“足够无效”以暴露未正确处理的极端情况。 afl AFL: American Fuzzy Lop afl aflplusplus afl.rs rust-fuzz: AFL for Rust American fuzzy lop is a security-oriented fuzzer that employs a novel type of compile-time instrumentation and genetic algorithms to automatically discover clean, interesting test cases that trigger new internal states in the targeted binary. This substantially improves the functional coverage for the fuzzed code. The compact synthesized corpora produced by the tool are also useful for seeding other, more labor- or resource-intensive testing regimes down the road. American fuzzy lop 是一个以安全为导向的 模糊器，采用新型编译时检测和遗传算法来自动发现干净、有趣的测试用例，从而触发目标二进制文件中的新内部状态。这大大提高了模糊代码的功能覆盖率。该工具生成的紧凑的合成语料库也可用于为将来的其他劳动或资源密集型测试制度奠定基础。 Compared to other instrumented fuzzers, afl-fuzz is designed to be practical: it has modest performance overhead, uses a variety of highly effective fuzzing strategies and effort minimization tricks, requires essentially no configuration, and seamlessly handles complex, real-world use cases - say, common image parsing or file compression libraries. 与其他仪器化模糊器相比,afl-fuzz被设计为实用:它具有适度的性能开销,使用各种高效的模糊策略和努力最小化技巧,基本上不需要配置,并且可以无缝处理复杂的现实世界用例 - 例如,常见的图像解析或文件压缩库。 AFL.RS Fuzz testing is a software testing technique used to find security and stability issues by providing pseudo-random data as input to the software. AFLplusplus is a popular, effective, and modern fuzz testing tool based on AFL. This library, afl.rs, allows one to run AFLplusplus on code written in the Rust programming language. 模糊测试是一种软件测试技术，用于通过提供伪随机数据作为软件的输入来发现安全性和稳定性问题。 AFLplusplus 是一种基于 AFL 的流行、有效、现代的模糊测试工具。这个库 afl.rs 允许人们在用 Rust 编程语言编写的代码上运行 AFLplusplus。 "},"Misc/PowerShell.html":{"url":"Misc/PowerShell.html","title":"PowerShell","keywords":"","body":"PowerShell 常用命令 Proxy settings # C:\\Users\\ludiser\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 function set_proxy0 { $env:HTTP_PROXY=\"http://127.0.0.1:7890\" } function set_proxy1 { $env:HTTPS_PROXY=\"https://127.0.0.1:7890\" } function unset_proxy0 { $env:HTTP_PROXY=\"\" } function unset_proxy1 { $env:HTTPS_PROXY=\"\" } 删除 # 删除指定文件 Remove-Item * -Include *.json -Recurse # 删除文件而保留文件夹 # 「This example deletes all of the files that have names that include a dot (.) 」 Remove-Item * -Include *.* -Exclude *.md -Recurse # 删除包含指定字符的文件夹 # 一定要注意加上通配符「*bin*」，否则只会删除bin这样的文件夹 Remove-Item * -Recurse -Include *bin* 获取版本信息 $PSVersionTable.PSVersion 获取主机信息 Get-Host 下载文件 Invoke-WebRequest / iwr 命令 get-command 进程 get-process 指令重命名 Set-Alias xxx0 xxx1 清屏cls 查找程序路径 Get-Command -Name xxname "},"Misc/NPM.html":{"url":"Misc/NPM.html","title":"NPM","keywords":"","body":"Node Package Manager (NPM) # change npm registry source npm config set registry https://registry.npmmirror.com # check in verbose mode npm install --verbose # clean cache and reinstall npm cache clean --force npm install # set proxy for npm npm config set proxy http://proxy.example.com:8080 npm config set https-proxy http://proxy.example.com:8080 # update npm npm install -g npm Node Version Manager (NVM) nvm PNPM (Fast, disk space efficient package manager) https://pnpm.io/installation "},"Misc/Fonts.html":{"url":"Misc/Fonts.html","title":"Fonts","keywords":"","body":"serach font on google, download the font file and install. gui managers: sudo apt install font-manager fc-cache -fv fc-list | grep \"IBM Plex Serif\" "},"Misc/Drawing.html":{"url":"Misc/Drawing.html","title":"Drawing","keywords":"","body":"Drawing 神经网络画图 gwding/draw_convnet (github.com) https://cbovar.github.io/ConvNetDraw/ Others canva: Visual Suite for Everyone can draw simple icons from words, e.g. \"NUDT SysY Compiler\" "},"Linux/Linux.html":{"url":"Linux/Linux.html","title":"Linux","keywords":"","body":""},"Linux/Display/Display.html":{"url":"Linux/Display/Display.html","title":"Display","keywords":"","body":"Great question! Let's go over each component and how they fit together. 1. GNOME GNOME (GNU Network Object Model Environment) is a desktop environment. A desktop environment provides the graphical interface that users interact with, including the desktop, windows, icons, file managers, and settings applications. GNOME is one of the most popular desktop environments for Linux and is used in Debian, Ubuntu, Fedora, and other distributions. Key components provided by GNOME include: Window management: Managing open applications and windows. Applications: Built-in utilities like the file manager, terminal, settings (i.e., gnome-control-center), and more. User interface: Consistent design and behavior for how things look and feel in the system. GNOME uses a display protocol (like X11 or Wayland) to render graphics to the screen, which brings us to the next components. 2. X11 (X Window System) X11, often called the X Window System or just X, is a display server protocol that provides the fundamental layer for graphical display on Unix-like operating systems. It was created in the 1980s and has been the standard display protocol for Linux desktops for a long time. X11’s primary responsibilities include: Managing windows (open applications) and their placement. Handling input events (mouse, keyboard). Communication between the hardware (graphics card) and applications. GNOME traditionally ran on top of X11, which would handle all the graphical processing and window management on the display. However, X11 has some limitations, particularly with modern graphics hardware, security, and handling multiple screens, which led to the development of a more modern protocol: Wayland. 3. Wayland Wayland is a newer display server protocol designed to replace X11. It was created to address some of X11’s limitations and to provide a simpler, more secure, and efficient system for rendering graphics in modern desktop environments. Wayland’s advantages over X11 include: Improved security: Each application (client) has its own display server instance, reducing security risks. Better performance: It’s designed to take advantage of modern graphics capabilities, resulting in smoother graphics and lower latency. Simplicity: Wayland’s architecture is much simpler than X11, reducing the number of potential points of failure. In Wayland, the compositor (in this case, GNOME’s compositor called Mutter) manages windows, input events, and rendering directly, whereas in X11, this was handled by a separate X server. 4. Relationship Between GNOME, X11, and Wayland GNOME is the desktop environment that users interact with. X11 or Wayland is the display protocol that GNOME uses to render graphics to the screen. GNOME can run on top of either X11 or Wayland, depending on the configuration. In a Wayland session, GNOME directly communicates with the Wayland compositor (Mutter), which manages the display. In an X11 session, GNOME interacts with the X server, which acts as a middle layer between the hardware and the applications. Each display protocol has its own strengths and weaknesses. Many Linux distributions, including Debian, have been moving towards using Wayland as the default due to its performance and security benefits. However, some applications and configurations still rely on X11, so X11 remains an option and fallback in many systems. How This Relates to Your Issue Your error message shows that gnome-control-center (the GNOME Settings app) encountered problems because it was running under Wayland but tried to interact with components in an X11 manner, resulting in a crash. This kind of error can happen when applications expect one protocol but the system defaults to another. Switching to an X11 session (or forcing the GNOME Settings app to use X11) is a workaround to avoid these incompatibilities if certain applications or configurations are unstable under Wayland. "},"Linux/LinuxFamily/LinuxFamily.html":{"url":"Linux/LinuxFamily/LinuxFamily.html","title":"LinuxFamily","keywords":"","body":""},"Linux/UserManagement.html":{"url":"Linux/UserManagement.html","title":"UserManagement","keywords":"","body":"groups # to check groups of a user usermod -G # to add a user to a group usermod -G -d # to change the home directory of a user usermod -s # to change the shell of a user # give user sudo access sudo vim /etc/sudoers ALL=(ALL) ALL # add the line above to the end of the file # add user to sudo|wheel group sudo usermod -aG sudo su Switch shell to another user. More information: https://manned.org/su. - Switch to superuser (requires the root password): su - Switch to a given user (requires the user's password): su username - Switch to a given user and simulate a full login shell: su - username - Execute a command as another user: su - username -c \"command\" adduser User addition utility. More information: https://manned.org/adduser. - Create a new user with a default home directory and prompt the user to set a password: adduser username - Create a new user without a home directory: adduser --no-create-home username - Create a new user with a home directory at the specified path: adduser --home path/to/home username - Create a new user with the specified shell set as the login shell: adduser --shell path/to/shell username - Create a new user belonging to the specified group: adduser --ingroup group username users Display a list of logged in users. See also: useradd, userdel, usermod. More information: https://www.gnu.org/software/coreutils/users. - Print logged in usernames: users - Print logged in usernames according to a given file: users /var/log/wmtp usermod Modify a user account. See also: users, useradd, userdel. More information: https://manned.org/usermod. - Change a username: sudo usermod -l|--login new_username username - Change a user ID: sudo usermod -u|--uid id username - Change a user shell: sudo usermod -s|--shell path/to/shell username - Add a user to supplementary groups (mind the lack of whitespace): sudo usermod -a|--append -G|--groups group1,group2,... username - Change a user home directory: sudo usermod -m|--move-home -d|--home path/to/new_home username gpasswd Administer /etc/group and /etc/gshadow. More information: https://manned.org/gpasswd. - Define group administrators: sudo gpasswd -A user1,user2 group - Set the list of group members: sudo gpasswd -M user1,user2 group - Create a password for the named group: gpasswd group - Add a user to the named group: gpasswd -a user group - Remove a user from the named group: gpasswd -d user group (base) ➜ ~ gpasswd Usage: gpasswd [option] GROUP Options: -a, --add USER add USER to GROUP -d, --delete USER remove USER from GROUP -h, --help display this help message and exit -Q, --root CHROOT_DIR directory to chroot into -r, --remove-password remove the GROUP's password -R, --restrict restrict access to GROUP to its members -M, --members USER,... set the list of members of GROUP -A, --administrators ADMIN,... set the list of administrators for GROUP Except for the -A and -M options, the options cannot be combined. delete user In Debian 12, the deluser and userdel commands are used to delete users. However, if you can't find these commands, it might be due to a missing package or a typo. Here's how you can delete a user in Debian 12: 1. Using deluser (Recommended) The deluser command is part of the passwd package and is the preferred method for removing users in Debian. Steps: Install the passwd package (if not already installed): sudo apt update sudo apt install passwd Delete the user: sudo deluser username Replace username with the actual username you want to delete. Delete the user's home directory and mail spool (optional): sudo deluser --remove-home username 2. Using userdel The userdel command is part of the shadow-utils package and is a lower-level tool for removing users. Steps: Install the shadow-utils package (if not already installed): sudo apt update sudo apt install shadow-utils Delete the user: sudo userdel username Delete the user's home directory and mail spool (optional): sudo rm -rf /home/username sudo rm -rf /var/mail/username Troubleshooting Command not found: If you get a \"command not found\" error, ensure that the passwd or shadow-utils package is installed. Typo: Double-check the spelling of the commands (deluser and userdel). Summary Use deluser for a more user-friendly experience. Use userdel for a more manual approach. Always ensure the necessary packages are installed. Let me know if you encounter any issues! "},"Linux/InputMethod.html":{"url":"Linux/InputMethod.html","title":"InputMethod","keywords":"","body":"Fcitx Setup_Fcitx_5 由于很多不同地方的过渡阶段，没有适合每个地方的完美解决方案。请根据您的环境选择您自己的解决方案。基本上，您要做的是为桌面会话设置以下环境变量。 XMODIFIERS=@im=fcitx GTK_IM_MODULE=fcitx QT_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx Useful commands: fcitx-diagnose # print diagnostic information fcitx-configtool # open the configuration tool # fcitx-remote CSDN: Debian Linux Input Method debian系统对中文输入法的支持少之又少，很多人会选择使用搜狗，但是对于大多数来说，会有各种各样的问题，所以这里将会介绍使用系统自带的fcitx输入法。 首先软件源更新（选）： 1.在终端中输入： sudo gedit /etc/apt/sources.list 2.在打开的文本中删除全部内容，粘贴上以下文本： deb http://mirrors.163.com/debian/ jessie-updates main non-free contrib deb http://mirrors.163.com/debian/ jessie-backports main non-free contrib deb-src http://mirrors.163.com/debian/ jessie main non-free contrib deb-src http://mirrors.163.com/debian/ jessie-updates main non-free contrib deb-src http://mirrors.163.com/debian/ jessie-backports main non-free contrib deb http://mirrors.163.com/debian-security/ jessie/updates main non-free contrib deb-src http://mirrors.163.com/debian-security/ jessie/updates main non-free contrib deb http://ftp.cn.debian.org/debian wheezy main contrib non-free （此处包含163以及debian官方软件源） 3.点击保存，并关闭，回到终端，进行软件源同步 输入指令：sudo apt-get update apt-get install fcitx-ui-classic && apt-get install fcitx-ui-light 5.点击菜单，找到应用: 输入法，并打开。 6.在用户设置中 点击 fctix选项 ，并点击确定。 7.根据输入法配置中的提示，打开终端，输入指令（根据自身要求选择）： sudo apt-get install fcitx-sunpinyin fcitx-googlepinyin fcitx-pinyin （这里有三种拼音输入法：fcitx-sunpinyin ，fcitx-googlepinyin 和 fcitx-pinyin ，不需要的可以删掉） sudo apt-get fcitx-table-wubi fcitx-table-wbpy (两种五笔输入法：fcitx-table-wubi 和fcitx-table-wbpy) sudo apt-get fcitx-table-cangjie (繁体中文输入，只有一种) 8.安装通用的输入法码表: fcitx-table* 套件（必装！） sudo apt-get fcitx-table* 9.应用程序支持（必装！） sudo apt-get install fcitx-frontend-gtk2 fcitx-frontend-gtk3 fcitx-frontend-qt4 (fcitx-frontend-gtk2 和 fcitx-frontend-gtk3 必装，同时 fcitx-frontend-qt4 也建议一起装上) 10.最后重启，根据自己的快捷键启动输入法（默认 Ctrl+空格 ） 在右下脚会有小键盘，右键 --配置 可以选择各种输入选项 ———————————————— 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 原文链接：https://blog.csdn.net/ieeso/article/details/105274943 "},"Linux/PrintingDrivers.html":{"url":"Linux/PrintingDrivers.html","title":"PrintingDrivers","keywords":"","body":"Printing Drivers Debian12 # hplip-gui: HP Linux Printing and Imaging - GUI utilities (Qt-based) sudo apt install hplip-gui # deps on: hplip: HP Linux Printing and Imaging cups: Common Unix Printing System - PPD/driver support, web interface. HPLIP is composed of: System services to handle communications with the printers HP CUPS backend driver (hp:) with bi-directional communication with HP printers (provides printer status feedback to CUPS and enhanced HPIJS functionality such as 4-side full-bleed printing support) HP CUPS backend driver for sending faxes (hpfax:) hpcups CUPS Raster driver to turn rasterized input from the CUPS filter chain into the printer's native format (PCL, LIDIL, ...). (hpcups is shipped in a separate package) HPIJS Ghostscript IJS driver to rasterize output from PostScript(tm) files or from any other input format supported by Ghostscript, and also for PostScript(tm) to fax conversion support (HPIJS is shipped in a separate package) Command line utilities to perform printer maintenance, such as ink-level monitoring or pen cleaning and calibration GUI and command line utility to download data from the photo card interfaces in MFP devices GUI and command line utilities to interface with the fax functions A GUI toolbox to access all these functions in a friendly way HPAIO SANE backend (hpaio) for flatbed and Automatic Document Feeder (ADF) scanning using MFP devices "},"PythonLibs/PythonLibs.html":{"url":"PythonLibs/PythonLibs.html","title":"PythonLibs","keywords":"","body":""},"PythonLibs/ArgParse.html":{"url":"PythonLibs/ArgParse.html","title":"ArgParse","keywords":"","body":"ArgParse in Python unexpected parser.add_argument(\"--train\", type=bool) 在使用 argparse 时，parser.add_argument(\"--train\", type=bool) 的用法可能会导致一些意外行为，因为 type=bool 并不会将输入的字符串自动转换为布尔值。相反，它会尝试将输入的字符串作为 Python 的 bool() 函数的参数，这通常会导致输入的值被解释为 True 或 False，但结果可能不符合预期。 问题分析 bool(\"True\") 返回 True，但 bool(\"False\") 也返回 True，因为非空字符串在 Python 中被视为 True。 因此，直接使用 type=bool 无法正确解析布尔值参数。 正确的用法 为了正确解析布尔值参数，可以使用 action='store_true' 或 action='store_false'，或者自定义类型转换函数。 方法 1：使用 action='store_true' 或 action='store_false' 这是最常用的方法，适用于布尔值参数的默认行为。 import argparse parser = argparse.ArgumentParser() parser.add_argument(\"--train\", action='store_true', help=\"是否启用训练模式\") args = parser.parse_args() print(f\"训练模式: {args.train}\") 使用示例 python script.py --train 输出： 训练模式: True 如果不传递 --train 参数： python script.py 输出： 训练模式: False 方法 2：自定义类型转换函数 如果你希望用户明确传递 True 或 False，可以使用自定义类型转换函数。 import argparse def str_to_bool(value): if value.lower() in ['true', '1', 't', 'y', 'yes']: return True elif value.lower() in ['false', '0', 'f', 'n', 'no']: return False else: raise argparse.ArgumentTypeError(\"布尔值必须是 'True' 或 'False'\") parser = argparse.ArgumentParser() parser.add_argument(\"--train\", type=str_to_bool, help=\"是否启用训练模式\") args = parser.parse_args() print(f\"训练模式: {args.train}\") 使用示例 python script.py --train True 输出： 训练模式: True python script.py --train False 输出： 训练模式: False 如果传递了无效的值： python script.py --train maybe 输出： usage: script.py [-h] [--train TRAIN] script.py: error: argument --train: 布尔值必须是 'True' 或 'False' 总结 如果希望参数默认是布尔值，推荐使用 action='store_true' 或 action='store_false'。 如果需要用户明确传递 True 或 False，可以使用自定义类型转换函数。 直接使用 type=bool 可能会导致意外行为，不推荐使用。 "},"Network/Network.html":{"url":"Network/Network.html","title":"Network","keywords":"","body":""},"Network/Tailscale.html":{"url":"Network/Tailscale.html","title":"Tailscale","keywords":"","body":""},"Network/Clash.html":{"url":"Network/Clash.html","title":"Cxxxh","keywords":"","body":"text text Clash rules To ensure that Clash does not redirect traffic to your Tailscale server's hostname and allows Tailscale to handle it, you can write a DOMAIN-SUFFIX rule in your Clash configuration file (config.yaml or any custom configuration file you are using). Here's how you can do it: Steps to Update Clash Configuration: Open your Clash configuration file: Locate your config.yaml file or the configuration file you are using for Clash. It is typically found in ~/.config/clash/ or the directory specified during setup. Add a rule for your Tailscale hostname: Under the rules: section, add a rule that bypasses traffic for the specific hostname. rules: - DOMAIN-SUFFIX,my_tailscale_server_hostname,DIRECT Replace my_tailscale_server_hostname with your actual Tailscale hostname. Ensure that Clash processes the rule properly: Make sure this rule is placed before any generic rules that might otherwise catch and redirect this traffic. Restart Clash: After saving the configuration, restart your Clash service to apply the changes: systemctl restart clash Explanation: DOMAIN-SUFFIX: Matches the hostname (or its suffix) you specify. my_tailscale_server_hostname: Your Tailscale server hostname (e.g., myserver.example.com). DIRECT: Tells Clash to allow traffic to flow directly without interference. Additional Considerations: If Tailscale uses an IP address rather than a hostname, you can use a rule like this: rules: - IP-CIDR,100.64.0.0/10,DIRECT This ensures that all traffic to Tailscale's private network range (100.64.0.0/10) is not redirected. If your Tailscale hostname resolves dynamically or has multiple domains, you might need additional DOMAIN-SUFFIX or DOMAIN rules to cover all possibilities. With this setup, Clash will bypass traffic intended for my_tailscale_server_hostname:9090, allowing Tailscale to handle it seamlessly. Clash DNS The error occurs because Clash cannot resolve the domain xx-rocky to an IP address. This can happen if: The hostname xx-rocky is not resolvable via DNS. It relies on Tailscale's private DNS resolver, which Clash may not be configured to use. Here’s how to resolve the issue: Option 1: Add the Tailscale DNS to Clash Ensure Clash uses the Tailscale DNS resolver. Tailscale often sets up a private DNS resolver to resolve hostnames like xx-rocky. Find the Tailscale DNS server: Run: tailscale ip -4 This will give you the IPv4 address of your Tailscale server. Additionally, check the DNS settings from: tailscale status Add the Tailscale DNS server to Clash’s DNS configuration: Modify the dns: section of your config.yaml file: dns: enable: true listen: 0.0.0.0:53 nameserver: - 100.100.100.100 # Replace with Tailscale DNS server if different - 8.8.8.8 # Backup: Google Public DNS - 1.1.1.1 # Backup: Cloudflare DNS fallback: - 8.8.8.8 - 1.1.1.1 Save the configuration and restart Clash: systemctl restart clash Option 2: Use IP Address in the Rule If the hostname xx-rocky resolves to a known static IP (or an IP in Tailscale's range), you can write a rule that bypasses based on the IP address instead: Find the IP address of xx-rocky: Run: tailscale ping xx-rocky tailscale ip -4 This will show the Tailscale-assigned IP for the hostname. Add an IP-CIDR rule to bypass traffic for that IP or subnet: rules: - IP-CIDR,100.64.0.0/10,DIRECT Restart Clash: systemctl restart clash Option 3: Add a Static Host Entry in Clash If the hostname is not resolvable via DNS, you can manually define it in Clash’s configuration: Add a static mapping in the hosts: section of config.yaml: dns: enable: true listen: 0.0.0.0:53 hosts: \"xx-rocky\": 100.64.0.1 # Replace with the Tailscale IP of xx-rocky Save the configuration and restart Clash: systemctl restart clash Option 4: Bypass Clash for Local Traffic If you prefer to bypass Clash entirely for all Tailscale traffic: Add Tailscale's subnet to Clash's bypass list in config.yaml: rules: - IP-CIDR,100.64.0.0/10,DIRECT Restart Clash: systemctl restart clash By following these steps, you should resolve the issue and ensure that traffic to xx-rocky is properly routed through Tailscale. "},"Git/git.html":{"url":"Git/git.html","title":"git","keywords":"","body":"Git git remote set-url --add origin # You have divergent branches and need to specify how to reconcile them. # You can do so by running one of the following commands sometime before # your next pull: git config pull.rebase false # merge git config pull.rebase true # rebase git config pull.ff only # fast-forward only "},"Git/githttps.html":{"url":"Git/githttps.html","title":"githttps","keywords":"","body":"in ubuntu, i use https git repository, each time i pull or push need to type in account and passward, how to avoid this? (cant use ssh) To avoid typing your username and password each time you push or pull from a Git repository over HTTPS on Ubuntu, you can use Git's credential helper to cache your credentials. Here’s how you can set it up: 1. Use the Credential Cache Helper The credential cache helper keeps your credentials in memory for a short period (default is 15 minutes). Run the following command to enable it: git config --global credential.helper cache If you want to change the cache timeout, you can set it in seconds. For example, to cache credentials for 1 hour (3600 seconds): git config --global credential.helper 'cache --timeout=3600' 2. Use the Credential Store Helper The credential store helper saves your credentials in a plain text file on disk, which is more persistent but less secure than the cache method. To enable the credential store helper: git config --global credential.helper store When you use git pull or git push for the first time after configuring this, Git will prompt you for your username and password, and then store them in a file at ~/.git-credentials. 3. Use the libsecret Credential Helper The libsecret credential helper integrates with GNOME Keyring to securely store your credentials. First, install the required package: sudo apt-get install libsecret-1-0 libsecret-1-dev Then, you need to compile the libsecret credential helper. This is a one-time setup: cd /usr/share/doc/git/contrib/credential/libsecret sudo make Finally, configure Git to use the libsecret helper: git config --global credential.helper /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret 4. Store Credentials for a Single Repository If you want to store credentials for just one repository and not globally, navigate to your repository and run: git config credential.helper store Steps to Verify After configuring the credential helper, try pulling or pushing to your repository: git pull You should be prompted for your credentials the first time, and then they will be cached or stored based on your chosen method. Security Note Storing credentials in plain text (using the store helper) can be insecure, as anyone with access to your file system can read them. Use the cache helper or libsecret for better security. "},"Git/git-branch.html":{"url":"Git/git-branch.html","title":"git-branch","keywords":"","body":"To set a default upstream branch for git push and git pull, you need to configure the branch to track a specific remote branch. Here’s how to do it: Set Default Upstream Branch Using git push Command The easiest way to set an upstream branch is to use the --set-upstream option with git push: git push --set-upstream origin This command does two things: Pushes your branch to the remote repository. Configures the branch to track the corresponding branch on the remote. Using git branch Command You can manually set the upstream branch for the current branch: git branch --set-upstream-to=origin/ Example: git branch --set-upstream-to=origin/main main While Creating a New Branch If you create a new branch and want it to track a remote branch from the start: git checkout -b --track origin/ Example: git checkout -b feature/new-feature --track origin/feature/new-feature Verify Upstream Branch To confirm the upstream branch configuration: git branch -vv The output will display the upstream branch associated with each local branch. Remove or Change Upstream Branch To Remove an Upstream Branch If you no longer want a branch to track a remote branch: git branch --unset-upstream To Change the Upstream Branch Use the --set-upstream-to option to point to a different remote branch: git branch --set-upstream-to=origin/new-branch How it Works for git push and git pull Once the upstream branch is set, git push and git pull will automatically use the configured upstream branch. Example workflow: Set upstream: git push --set-upstream origin main Push and pull without specifying branches: git push git pull Let me know if you need further assistance! "},"Git/Github.html":{"url":"Git/Github.html","title":"GitHub","keywords":"","body":"Github Displaying Contributors Image on README files with no Pain! contributors-img: contrib.rocks contributors-img all-contributors Best-README-Template "},"Papers/Papers.html":{"url":"Papers/Papers.html","title":"Papers","keywords":"","body":""},"Papers/AttentionIsTuringComplete.html":{"url":"Papers/AttentionIsTuringComplete.html","title":"Attention Is Turing Complete","keywords":"","body":"Attention Is Turing Complete Not yet complete. Attention Is Turing Complete TLDR: 本文证明了 Attention 机制是图灵完备的，本文构造了一个有1层Encoder和3层Decoder的Transformer，用其模拟了图灵机的计算过程（即计算格局的变化）。 Abstract Alternatives to recurrent neural networks, in particular, architectures based on self-attention, are gaining momentum for processing input sequences. In spite of their relevance, the computational properties of such networks have not yet been fully explored. We study the computational power of the Transformer, one of the most paradigmatic architectures exemplifying self-attention. We show that the Transformer with hard-attention is Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. Our study also reveals some minimal sets of elements needed to obtain this completeness result. 循环神经网络的替代方案，特别是基于自注意力的架构，在处理输入序列方面正在获得动力。尽管它们具有相关性，但此类网络的计算特性尚未得到充分探索。我们研究Transformer的计算能力，它是体现自我注意力的最具范式的架构之一。我们证明，具有硬注意力的Transformer 完全基于其计算和访问数据内部密集表示的能力，是图灵完备的。我们的研究还揭示了获得这种完整性结果所需的一些最小元素集。 Introduction 序列到序列神经网络 我们对序列到序列 (seq-to-seq) 神经网络架构感兴趣,我们接下来将正式化。对于某些 d > 0,seq-to-seq 网络 N 接收向量 x∈ Q 的序列 X = (x, . . . . , x) 作为输入,并生成向量 y∈ Q 的序列 Y = (y, . . . . , y) 作为输出。大多数这种类型的架构都需要一个种子向量 s 和一些停止标准来确定输出的长度。后者通常基于特定输出向量的生成,称为序列结束标记。相反,在我们的形式化中,我们允许网络产生一个固定数量的 r ≥ 0 的输出向量。因此,为方便起见,我们将一般的 seq-to-seq 网络视为函数 N,使得值 N (X, s, r) 对应于 Y = (y, y, . . . . , y) 形式的输出序列。通过此定义,我们可以将 seq-to-seq 网络解释为字符串的语言识别器,如下所示。 定义 1 seq-to-seq 语言识别器是一个元组 A = (Σ, f, N, s, F),其中 Σ 是有限字母表,f : Σ → Q 是嵌入函数,N 是 seq-to-seq 网络,s ∈ Q 是种子向量,F ⊆ Q是一组最终向量。我们说 A 接受字符串 w ∈ Σ,如果存在一个整数 r ∈ N,使得 N (f (w), s, r) = (y, . . . , y) 和 y∈ F。 A 接受的语言(用 L(A) 表示)是 A 接受的所有字符串的集合。 我们对识别器施加了两个额外的限制。 嵌入函数 f : Σ → Q应该由图灵机以 Σ 大小的多项式时间计算。这涵盖了计算符号的 input 嵌入的两种最典型方法:one-hot 编码和由固定前馈网络计算的嵌入。 此外,集合 F 也应该在多项式时间内可识别;给定一个向量 f ,隶属度 F ∈ F 应该由在多项式时间内相对于 f 的大小(以位为单位)工作的图灵机决定。这涵盖了使用固定序列结束向量检查相等性的常用方法。 我们施加这些限制是为了防止通过在 input embedding 或 stopping 条件中编码任意计算来作弊的可能性,同时足够宽容地构建有意义的 embeddings 和 stopping criterions。 图灵机计算 让我们回顾一下,(确定性的)图灵机是 M = (Q, Σ, δ, q, F ) 形式的元组,其中: seq-to-seq 神经网络架构的图灵完备性 seq-to-seq 神经网络架构的 N 类定义了类 L,该类由使用 N 中的网络的语言识别器接受的所有语言组成。从这些概念中,N 类的图灵完备性的形式化自然而然地随之而来。 定义 2 如果 L 包含所有可判定语言(即图灵机可识别的所有语言),则 seq-to-seq 神经网络架构的 N 类为图灵完备。 The Transformer architecture 在本节中,我们提出了 Transformer 架构的形式化(Vaswani et al., 2017),从函数和参数的一些具体选择中抽象出来。我们的形式化并不是为了产生 Transformer 的有效实现,而是提供一个简单的设置,通过该设置可以以正式的方式建立其数学属性。 Transformer 在很大程度上基于接下来介绍的注意力机制。考虑评分函数 score : Q× Q→ Q 和归一化函数 ρ : Q→ Q,对于 d、n > 0。假设 q ∈ Q,并且 K = (k, . . . , k) 和 V = (v, . . . , v) 是 Q 中元素的元组。由 Att(q, K, V) 表示的 q-attention over (K, V) 是 Q ∈向量 a,定义如下。 (s1,…,sn)=ρ(score⁡(q,k1),score⁡(q,k2),…,score⁡(q,kn)) \\left(s_{1}, \\ldots, s_{n}\\right) =\\rho\\left(\\operatorname{score}\\left(\\boldsymbol{q}, \\boldsymbol{k}_{1}\\right), \\operatorname{score}\\left(\\boldsymbol{q}, \\boldsymbol{k}_{2}\\right), \\ldots, \\operatorname{score}\\left(\\boldsymbol{q}, \\boldsymbol{k}_{n}\\right)\\right) (s1​,…,sn​)=ρ(score(q,k1​),score(q,k2​),…,score(q,kn​)) a=s1v1+s2v2+⋯+snvn \\boldsymbol{a} =s_{1} \\boldsymbol{v}_{1}+s_{2} \\boldsymbol{v}_{2}+\\cdots+s_{n} \\boldsymbol{v}_{n} a=s1​v1​+s2​v2​+⋯+sn​vn​ 通常,q 称为查询,K 称为键,V 称为值。我们对评分函数没有任何限制,但我们确实对归一化函数施加了一些限制,以确保它在持仓上产生概率分布。我们要求归一化函数满足以下条件:对于每个 x = (x, . . . , x) ∈ Q,有一个函数 f从 Q 到 Q,使得 ρ(x) 的第 i 个分量 ρ(x) 相等 j=1 f(x) 的 f(x) 。我们注意到,例如,可以通过简单地选择 f(x) 作为指数函数 e 来以这种方式定义 softmax 函数,但我们允许其他可能性,接下来我们将解释。 在证明可能性结果时,我们需要选择特定的评分和归一化函数。评分函数的通常选择是由具有输入 (q, k) 的前馈网络定义的非线性函数,有时称为加性注意力 (Bahdanau et al., 2014)。另一种可能性是使用点积 〈q, k〉,称为乘法注意力 (Vaswani et al., 2017)。 我们实际上使用了两者的组合:乘法注意力加上定义为 σ(g(·)) 形式的函数组成的前馈网络,其中 g 是仿射变换,σ 是方程 (1) 中定义的分段线性 S 形激活。对于归一化函数,softmax 是一个标准选项。尽管如此,在我们的证明中,我们使用 hardmax 函数,如果 x是 x 中的最大值,则通过设置 f(x) = 1 来获得,否则设置 f(x) = 0。因此,对于最大值出现 r 次的向量 x,如果 x是 x 的最大值,则 hardmax(x) = ,否则为 hardmax(x) = 0。每当 hardmax 用作归一化函数时,我们都称其为 hard attention。 让我们观察一下,hardmax 的选择对于我们的证明以当前形式工作至关重要,因为它允许模拟 “访问” 向量序列中特定位置的过程。Hard attention 以前专门用于处理图像(Xu et al., 2015;Elsayed et al., 2019),但是,据我们所知,它尚未在自我注意架构的上下文中用于处理序列。有关我们对正结果函数的选择的进一步讨论,请参见第 5 节。按照惯例,对于函数 F : Q→ Q 和序列 X = (x, x, . . . , x),其中 x∈ Q,我们写 F (X) 来表示序列 (F (x), . . . , F (x))。 Transformer 编码器和解码器 Transformer 的单层编码器是一个参数函数 Enc(θ),其中 θ 是参数,它接收 Q 中向量的序列 X = (x, . . . , x) 作为输入,返回一个序列 Enc(X; θ) = (z, . . . . , z) Q中与 X 长度相同的向量。一般来说,我们认为 θ 中的参数是参数化函数 Q(·)、K(·)、V (·) 和 O(·),它们都从 Q到 Q。然后,单层编码器定义如下 a= Att(Q(x), K(X), V (X)) + x (4) z= O(a) + a (5) 请注意,在等式 4 中,我们将函数 Q 和 V 分别应用于 X 中的每个条目。在实践中,Q(·)、K(·)、V (·) 通常是指定为维度 (d × d) 矩阵的线性变换,而 O(·) 是前馈网络。+ xand + asummands 通常称为残差连接(He et al., 2016;他等人)。当用作参数的特定函数不重要时,我们只需编写 Z = Enc(X)。 Transformer 编码器被简单地定义为单层编码器(具有独立参数)的重复应用,加上两个最终变换函数 K(·) 和 V (·) 应用于最终层输出序列中的每个向量。因此,L 层 Transformer 编码器由以下递归定义(1 ≤ ' ≤ L−1 且 X= X): X= Enc(X; θ), K = K(X), V = V (X).(6) V = V (X) 我们写 (K, V ) = TEnc(X) 来表示 (K, V ) 是 L 层变压器编码器在输入序列 X 上的结果。 解码器 单层解码器类似于单层编码器,但需要额外注意一对外部键值向量 (K, V )。单层解码器的输入是序列 Y = (y, . . . , y) 加上外部对 (K, V ),输出是与 Y 长度相同的序列 Z = (z, . . . . , z) 。在定义解码器层时,我们用 Y 表示序列 (y, . . . , y),为 1 ≤ i ≤ k。该层的输出 Z = (z, . . . . , z) 也被参数化,这次由四个函数 Q(·)、K(·)、V (·) 和 O(·) 从 Q to Q 组成,并且对于每个 1 ≤ i ≤ k 定义如下: p= Att(Q(y), K(Y), V (Y)) + y (7) a= Att(p, K, V ) + p (8) z= O(a) + a (9) 请注意,对 (K(Y), V (Y)) 的第一个(自我)关注仅在索引 i 之前考虑 Y 的子序列,并用于生成查询 pto 关注外部对 (K, V)。我们将 Y 和 (K, V ) 上的单解码器层的输出表示为 Dec((K, V ), Y ; θ)。 Transformer 解码器是单层解码器的重复应用,加上一个转换函数 F : Q→ Q应用于解码序列的最终向量。因此,解码器的输出是 Q ∈单个向量 z。正式地,L 层 Transformer 解码器定义为 Y = Dec((K, V ), Y ; θ), z = F (y) (1 ≤ ' ≤ L − 1 和 Y = Y ).(10) 我们使用 z = TDec((K, V ), Y ) 来表示 z 是这个 L 层变压器解码器在输入 Y 和 (K, V) 上的输出。 Transformer 的一个重要限制是 Transformer 解码器的输出总是对应于某些有限字母Γ中的字母编码。从形式上讲,需要存在一个有限字母Γ和一个嵌入函数 g : Γ → Q,使得 Transformer 解码器的最终变换函数 F 将 Q 中的任何向量映射到 Γ 中字母嵌入的有限集 g(Γ) 中的向量。 完整的 Transformer Transformer 网络接收一个输入序列 X、一个种子向量 y 和一个值 r ∈ N。它的输出是一个序列 Y = (y, . . . , y),定义为 y= TDec(TEnc(X), (y, y, . . . , y)), 对于 0 ≤ t ≤ r − 1。(11) yt+1=TDec⁡(TEnc⁡(X),(y0,y1,…,yt)),for 0≤t≤r−1. \\boldsymbol{y}_{t+1}=\\operatorname{TDec}\\left(\\operatorname{TEnc}(\\boldsymbol{X}),\\left(\\boldsymbol{y}_{0}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{t}\\right)\\right), \\quad for ~ 0 \\leq t \\leq r-1. yt+1​=TDec(TEnc(X),(y0​,y1​,…,yt​)),for 0≤t≤r−1. 我们将变压器的输出序列表示为 Y = (y, y, . . . . , y) = Trans(X, y, r)。 3.1 比例下的不变性 如上所述,Transformer 网络在捕获语言的能力方面相当弱。这是因为 Transformer 是顺序不变的,即它们无法访问 input 中元素的相对顺序。更正式地说,两个相互排列的 Importing 序列会产生完全相同的输出。这是注意力函数的以下属性的结果:如果 K = (k, . . . . , k), V = (v, . . . , v) 且 π : {1, . . . . , n} → {1, . . . , n} 是排列,则对于每个查询 q,Att(q, K, V ) = Att(q, π(K), π(V ))。 ai=Att⁡(Q(xi),K(X),V(X))+xi \\boldsymbol{a}_{i} =\\operatorname{Att}\\left(Q\\left(\\boldsymbol{x}_{i}\\right), K(\\boldsymbol{X}), V(\\boldsymbol{X})\\right)+\\boldsymbol{x}_{i} ai​=Att(Q(xi​),K(X),V(X))+xi​ zi=O(ai)+ai \\boldsymbol{z}_{i} =O\\left(\\boldsymbol{a}_{i}\\right)+\\boldsymbol{a}_{i} zi​=O(ai​)+ai​ 4. Transformer 的位置编码的图灵完备性 定理 6 具有位置编码的 Transformer 网络类是图灵完备的。此外,图灵完备性即使在受限设置中也成立,其中 n ∈ N 的位置嵌入 pos（n） 中唯一的非恒定值是 n、1/n 和 1/n,并且 Transformer 网络具有单个编码器层和三个解码器层。 实际上,这个结果的证明显示了更有力的东西:不仅 Transformers 可以识别图灵机接受的所有语言,即所谓的可识别或可判定语言;它们可以识别所有递归可枚举或半可判定的语言,这些语言 L 存在枚举 L 中所有字符串的 TM。 我们现在提供定理 6 的完整证明。为了可读性,一些中间引理的证明被归入附录。 设 M = （Q, Σ, δ, q, F ） 是一个图灵机,其磁带向右是无限的,并假设特殊符号 # ∈ Σ 用于标记磁带中的空白位置。 我们对 M 在处理 Importing 字符串时的工作原理做出以下假设: M 从状态 q 开始,指向磁带的第一个读取空白符号 #的单元格。输入将立即写入第一个单元格的右侧。 Q 具有特殊状态 qused 来读取完整的输入。 最初（步骤 0）,M 过渡到状态 q,并将其头部向右移动。 当处于状态时,qit 向右移动,直到读取符号 #。 不接受状态（F 中的状态）没有转换。 很容易证明,每个通用的图灵机都等同于满足上述假设的图灵机。我们证明了可以构建一个 transformer 网络 Transthat 能够在每个可能的输入字符串上模拟 M;或者,更正式地说,L（M ） = L（Trans）。 构造以某种方式参与其中,并使用了几个辅助定义和中间结果。为了便于阅读,我们将构造和证明分为三个部分。我们首先给出我们使用的策略的高级视图。然后,我们详细介绍了实现我们的策略所需的编码器和解码器的架构,最后我们正式证明我们架构的每个部分都可以实际实现。 在 Transwe 的编码器部分接收字符串 w = ss· · ·s.我们首先使用嵌入函数将每个 sas 表示为 one-hot 向量,并为每个索引添加位置编码。编码器产生输出 （K, V）,其中 K= （k, . . . , k） 和 V = （v, . . . , v） 是键和值的序列,使得 v包含沙子的信息 k包含第 i 个位置编码的信息。我们稍后会证明,这允许我们关注每个特定位置,并将每个输入符号从编码器复制到解码器（参见引理 7）。 在 Transwe 的解码器部分模拟 M 在 w = ss· · · ·s.为此,我们定义以下序列（对于 i ≥ 0）: q:计算步骤 i 中 M 的状态 s:步骤 i 中 M 头部读取的符号 v:步骤 i 中 M 写入的符号 m:步骤 i 中 M 头部在过渡中移动的方向 v = [ q1, s1, x1, q2, s2, x2, x3, x4, x5, s3, x6, s4, x7 x8, x9, x10, x11 ], "},"Programming/Programming.html":{"url":"Programming/Programming.html","title":"Programming","keywords":"","body":""},"ConfigExamples/ConfigExamples.html":{"url":"ConfigExamples/ConfigExamples.html","title":"ConfigExamples","keywords":"","body":"Configuration Examples Configuration Examples .clang-format .gitignore .clang-format --- Language: Cpp AccessModifierOffset: -2 AlignAfterOpenBracket: Align AlignArrayOfStructures: None AlignConsecutiveMacros: None AlignConsecutiveAssignments: None AlignConsecutiveBitFields: None AlignConsecutiveDeclarations: None AlignEscapedNewlines: Left AlignOperands: Align AlignTrailingComments: true ... .gitignore **/.** **/__pycache__ **/data **/__pycache__ **/*.model *.zip *.pyc *.pyo "},"ReadingNotes/ReadingNotes.html":{"url":"ReadingNotes/ReadingNotes.html","title":"ReadingNotes","keywords":"","body":"Reading Notes 论青年的修养-张闻天-1938 论青年修养-张申府-1946 研究与学风-张申府-1948 青年在选择职业时的考虑-马克思 语录 计算之魂 References jyywiki.cn/Letter.md 《清华名师谈学风》-清华大学校史馆 "},"ReadingNotes/青年在选择职业时的考虑-马克思-1835.html":{"url":"ReadingNotes/青年在选择职业时的考虑-马克思-1835.html","title":"青年在选择职业时的考虑-马克思","keywords":"","body":"青年在选择职业时的考虑 卡·马克思的中学考试德语作文 自然本身给动物规定了它应该遵守的活动范围，动物也就安分地在这个范围内活动，而不试图越出这个范围，甚至不考虑有其它范围存在。神也给人指定了共同的目标──使人类和他自己趋于高尚，但是，神要人自己去寻找可以达到这个目标的手段；神让人在社会上选择一个最适合于他、最能使他和社会变得高尚的地位。 这种选择是人比其它创造物远为优越的地方，但同时也是可能毁灭人的一生、破坏他的一切计划并使他陷于不幸的行为。因此，认真地权衡这种选择，无疑是开始走上生活道路而又不愿在最重要的事情上听天由命的青年的首要责任。 每个人眼前都有一个目标，这个目标至少在他本人看来是伟大的，而且如果最深刻的信念，即内心深处的声音，认为这个目标是伟大的，那它实际上也是伟大的，因为神决不会使世人完全没有引导者；神轻声地但坚定地作启示。 但是，这声音很容易被淹没；我们认为是热情的东西可能倏忽而生，同样可能倏忽而逝。也许，我们的幻想蓦然迸发，我们的感情激动起来，我们的眼前浮想联翩，我们狂热地追求我们以为是神本身给我们指出的目标；但是，我们梦寐以求的东西很快就使我们厌恶，于是，我们便感到自己的整个存在遭到了毁灭。 因此，我们应当认真考虑：我们对所选择的职业是不是真的怀有热情？发自我们内心的声音是不是同意选择这种职业？我们的热情是不是一种迷误？我们认为是神的召唤的东西是不是一种自我欺骗？不过，如果不对热情的来源本身加以探究，我们又怎么能认清这一切呢？ 伟大的东西是闪光的，闪光会激发虚荣心，虚荣心容易使人产生热情或者一种我们觉得是热情的东西；但是，被名利迷住了心窍的人，理性是无法加以约束的，于是他一头栽进那不可抗拒的欲念召唤他去的地方；他的职业已经不再是由他自己选择，而是由偶然机会和假象去决定了。 我们的使命决不是求得一个最足以炫耀的职业，因为它不是那种可能由我们长期从事，但始终不会使我们感到厌倦、始终不会使我们劲头低落、始终不会使我们的热情冷却的职业，相反，我们很快就会觉得，我们的愿望没有得到满足，我们的理想没有实现，我们就将怨天尤人。 但是，不仅虚荣心能够引起对某种职业的突然的热情，而且我们也许会用自己的幻想把这种职业美化，把它美化成生活所能提供的至高无上的东西。我们没有仔细分析它，没有衡量它的全部分量，即它加在我们肩上的重大责任；我们只是从远处观察它，而从远处观察是靠不住的。 在这里，我们自己的理性不能给我们充当顾问，因为当它被感情欺骗，受幻想蒙蔽时，它既不依靠经验，也不依靠更深入的观察。然而，我们的目光应该投向谁呢？当我们丧失理性的时候，谁来支持我们呢？ 是我们的父母，他们走过了漫长的生活道路，饱尝了人世辛酸。──我们的心这样提醒我们。 如果我们经过冷静的考察，认清了所选择的职业的全部分量，了解它的困难以后，仍然对它充满热情，仍然爱它，觉得自己适合于它，那时我们就可以选择它，那时我们既不会受热情的欺骗，也不会仓促从事。 但是，我们并不总是能够选择我们自认为适合的职业；我们在社会上的关系，还在我们有能力决定它们以前就已经在某种程度上开始确立了。 我们的体质常常威胁我们，可是任何人也不敢藐视它的权利。 诚然，我们能够超越体质的限制，但这么一来，我们也就垮得更快；在这种情况下，我们就是冒险把大厦建筑在残破的废墟上，我们的一生也就变成一场精神原则和肉体原则之间的不幸的斗争。但是，一个不能克服自身相互斗争的因素的人，又怎能抗御生活的猛烈冲击，怎能安静地从事活动呢？然而只有从安静中才能产生出伟大壮丽的事业，安静是唯一能生长出成熟果实的土壤。 尽管我们由于体质不适合我们的职业，不能持久地工作，而且很少能够愉快地工作，但是，为了克尽职守而牺牲自己幸福的思想激励着我们不顾体弱去努力工作。如果我们选择了力不胜任的职业，那么我们决不能把它做好，我们很快就会自愧无能，就会感到自己是无用的人，是不能完成自己使命的社会成员。由此产生的最自然的结果就是自卑。还有比这更痛苦的感情吗？还有比这更难于靠外界的各种赐予来补偿的感情吗？自卑是一条毒蛇，它无尽无休地搅扰、啃啮我们的胸膛，吮吸我们心中滋润生命的血液，注入厌世和绝望的毒液。 如果我们错误地估计了自己的能力，以为能够胜任经过较为仔细的考虑而选定的职业，那么这种错误将使我们受到惩罚。即使不受到外界的指责，我们也会感到比外界指责更为可怕的痛苦。 如果我们把这一切都考虑过了，如果我们的生活条件容许我们选择任何一种职业，那么我们就可以选择一种使我们获得最高尊严的职业，一种建立在我们深信其正确的思想上的职业，一种能给我们提供最广阔的场所来为人类工作，并使我们自己不断接近共同目标即臻于完美境界的职业，而对于这个共同目标来说，任何职业都只不过是一种手段。 尊严是最能使人高尚、使他的活动和他的一切努力具有更加崇高品质的东西，是使他无可非议、受到众人钦佩并高出于众人之上的东西。 但是，能给人以尊严的只有这样的职业，在从事这种职业时我们不是作为奴隶般的工具，而是在自己的领域内独立地进行创造；这种职业不需要有不体面的行动（哪怕只是表面上不体面的行动），甚至最优秀的人物也会怀着崇高的自豪感去从事它。最合乎这些要求的职业，并不总是最高的职业，但往往是最可取的职业。 但是，正如有失尊严的职业会贬低我们一样，那种建立在我们后来认为是错误的思想上的职业也一定会成为我们的沉重负担。 这里，我们除了自我欺骗，别无解救办法，而让人自我欺骗的解救办法是多么令人失望啊！ 那些主要不是干预生活本身，而是从事抽象真理的研究的职业，对于还没有确立坚定的原则和牢固的、不可动摇的信念的青年是最危险的，当然，如果这些职业在我们心里深深地扎下了根，如果我们能够为它们的主导思想而牺牲生命、竭尽全力，这些职业看来还是最高尚的。 这些职业能够使具有合适才干的人幸福，但是也会使那些不经考虑、凭一时冲动而贸然从事的人毁灭。 相反，重视作为我们职业的基础的思想，会使我们在社会上占有较高的地位，提高我们自己的尊严，使我们的行为不可动摇。 一个选择了自己所珍视的职业的人，一想到他可能不称职时就会战战兢兢──这种人单是因为他在社会上所处的地位是高尚的，他也就会使自己的行为保持高尚。 在选择职业时，我们应该遵守的主要指针是人类的幸福和我们自身的完美。不应认为，这两种利益会彼此敌对、互相冲突，一种利益必定消灭另一种利益；相反，人的本性是这样的：人只有为同时代人的完美、为他们的幸福而工作，自己才能达到完美。 如果一个人只为自己劳动，他也许能够成为著名的学者、伟大的哲人、卓越的诗人，然而他永远不能成为完美的、真正伟大的人物。 历史把那些为共同目标工作因而自己变得高尚的人称为最伟大的人物；经验赞美那些为大多数人带来幸福的人是最幸福的人；宗教本身也教诲我们，人人敬仰的典范，就曾为人类而牺牲自己──有谁敢否定这类教诲呢？ 如果我们选择了最能为人类而工作的职业，那么，重担就不能把我们压倒，因为这是为大家作出的牺牲；那时我们所享受的就不是可怜的、有限的、自私的乐趣，我们的幸福将属于千百万人，我们的事业将悄然无声地存在下去，但是它会永远发挥作用，而面对我们的骨灰，高尚的人们将洒下热泪。 卡·马克思写于1835年8月 "},"ReadingNotes/研究与学风-张申府-1948.html":{"url":"ReadingNotes/研究与学风-张申府-1948.html","title":"研究与学风-张申府-1948","keywords":"","body":"研究与学风-张申府 在学问上，今日一定要加强研究，必非深造自得不可，必须养成大器，再不可浅尝辄止，皮毛肤浅。 有人说，我不要做学者专家，我为何要研究？但你只要是一个人，为社会做一点事，你总要有一些东西，总要有一个专长。因此，我们在学校里的时候就要深厚；联带着来的就要脚踏实地，实是求是，就要认真，就要实实在在；再一点就要精核，要一丝不苟，绝不等于漠忽笼统。 现在所谓研究在英文就是Research，法文，意大利文，西班牙文，以至俄文都也差不多。这也可翻译成考索，普通都以为研究很高深很难，其实，也很平常，肯作就不难。先看字面：从Research，这个字看，不过是找了又找。你肯去找，找了又找，这是不难的，只要你肯去找就行。各种学问全是找来的，各种真理全是找来的。研究古书，研究新学，全要肯找，肯找才能得。所谓 “求则得之” 这不是空话。辟如有一算学难题，别人解答不出，我肯去找，就有找出来的希望。十九世纪初，法国有一青年名叫葛录亚（Galois），未成年就死了。他对算学有兴趣，肯去找，二三年间便成为世界上第一流的算学家了。现在世界最伟大的学者是爱因斯坦，作成相对论，他的成功，也是找来的。找的另一个名字可叫调查。我国的考古学家去找，不是找出北京人来吗？研究不过如此，这并不难，不要怕，任何人都能做。再换个说法，中国人普通不说研究，他说我捉摸捉摸，因此，研究也就可以说是捉摸。捉摸是要动手动脚的，所以研究不但要用脑，要也用手，用脚。中国字同音的常同义，而且一个字常有反面的意思。有两个和捉摸音相近义相似的，就是琢磨，这就是说研究要一点一点来。如研究政治，研究经济，你都要一点一点去捉摸或琢磨，换句话说，就是要钻。前边说专家，专家就是肯钻。肯去钻，肯去捉摸，肯去琢磨，这就是研究。刚才我说中国字常有同音同义，而且一个字常有相反的意思，钻和专也就如此。另如忍字。忍是忍耐，这不坏。但由忍耐变成残忍这就坏了。忍于己是忍耐，忍于人就变成残忍了。又如创和撞也如此，这告诉人要创造非撞不可。这都是从字面上得到的意思。关于这些，不再说了。我主要的意思就是说研究不难，只要肯找了又找，只要一步一步作去，就能成功。但研究不是study，那是从书本上求知识；而是Research，就是要发现真理！ 附带要讲讲研究室的任务。所谓研究室英文作 Seminary，它的字源与种子，精同义，所以它的本义就是利于产生新品种，新性质的地方，也就是可以产生新东西的地方。我们的研究室就要本着这个意旨去做。有新学问就有新行动，有新行动就能创造新事物。研究新道理不仅是知的，而且是行的，是用的。自然科学有社会功用，社会科学，经济学，政治学……更有社会功用，我们应本此精神使整个学术，整个学术界都服务于人民，都服务人类！ 我十二年不教书，一个原因也许是因为有人认我一教书就会谈到政治。其实，这是一种误会，在学我自要言学。近年我深深感觉中国需要一种新学风。尤其是大学或学院更要注意研究。我所谓新的学风就是深厚，笃实，精核的学风，这与研究有密切关系。 中国从八十年前起办新教育，其中有抄英国的，有抄日本的，以至德国的，美国的，但抄的全是皮毛。今日除非不办教育，要办教育就应根据自己的实况，自己的需要，独立地来办。在学问上，今日一定要加强研究，必非深造自得不可，必须养成大器，再不可浅尝辄止，皮毛肤浅。 有人说，我不要做学者专家，我为何要研究？但你只要是一个人，为社会做一点事，你总要有一些东西，总要有一个专长。 因此，我们在学校里的时候就要深厚；联带着来的就要脚踏实地，实是求是，就要认真，就要实实在在；再一点就要精核，要一丝不苟，绝不等于漠忽笼统。 我认为学校应该如此，应该造成这样的学风，应该培植这样的学生，这就需要大家共同努力。 （原载1948年4月20日《天琴》创刊号。选自河北人民出版社2005年版《张申府文集》第二卷，有删节。） "},"ReadingNotes/论青年修养-张申府-1946.html":{"url":"ReadingNotes/论青年修养-张申府-1946.html","title":"论青年修养-张申府-1946","keywords":"","body":"论青年修养-张申府-1946 心不静，写不出系统深刻的东西来。无可如何，这是随便谈谈罢。但这样子，也许比起装腔作势，板起面孔来说话，读者可以感着更亲切一点儿。 关于青年修养，我现在有三五点意思涌上心头。现在就以次分别写在下边。 第一，我也与许多古人一样，总觉着一个青年，为学必先立志。就令一个人，不必专门读书，有一个大志向，也是非常之要紧。一个人但令多少有点儿知识，多少有点儿自觉，那就要有一个志向，这样子才可以免得麻麻胡胡过一辈子。 至于立志作什么，那却有点儿难说，但至低限度，最一般地来说，你要立志作一个好人。这话也许大空，那就反过来说也可：你总要立志不为恶。凡你平常骂人的事，你总要下决心一件也不作。这一点在今日实在最最要紧。许多人两面作风，许多人口是心非，许多人口称民主而行反民主，许多人天天骂当局，而他的行为没有一点不与当局一样，除了地位不同以外。诸如此类，都因他没有坚定志向的缘故。 说得更具体一点，一个人总要作一个合乎时代的人，因此应该对于自己的时代不可不有一点切实的认识。但这地方很容易犯一个毛病，那就是随波逐流。一个人知识行动合乎时代是必要的，但随波逐流可就大要不得。怎样免掉这种毛病，那就要注意自觉，作得了自己的主宰，有自得之处，尽量防备虚荣，并且对时代有深刻的认识。 人怎样才能有志，尤其怎样才能有大志，这原因颇不简单。从外来原因说，这一种要靠父兄师长的告语教导。这不是人人所可得。一种靠同学朋友的切磋鼓励。这也可遇不可期。另一种就是靠阅名人传记，读大家著作；甚至看名家小说、戏剧、电影，也都会有好处。这是人人都可作得到的。但是你这样子作时，你必懂得体察，懂得与好人看齐。换言之，要作一个好人，适当的自觉总是必要的。 再进一步说，在今日这个时代，要作一个好人，拿旧话说，“民胞物与”总是一个必要的出发点。说得通俗一点，你要有一种治病救人的意趣。你不要把自己孤立，你要使世界因有你而不同，但你都不可总觉着你与一般人不同。 第二，关于为学读书，我特别愿意告诉你一个“专”字。本来，凡事，“专”都是最首要的成功诀。读书为学，也不外是。一个人要容易有成，那就最好只干一样事。古人讲学，常说博与约。但博如没有中心，必至泛滥无归，事倍功半，费力而不讨好。一个人读书，与其对一切知道一点，确不如对一点知道一切。等到你有了中心，有了主宰，有了专长之后，再对一切都知道一点，那就正可以作你原来一点的必要补助。所谓由博返约，能约，也就不妨博。 前已说过，要读大家的名著。这也是为学读书上的一个必要的要诀。以专而言，与其泛览群籍，不如精读一书。但这一书必须是大家名著，不刊的经典，意味深长，使你研寻不尽者。这种书，不拘那个文明国家，自古以来都是有的。一个人读书，最好读到深造自得。大家的名典当然都是深造自得的书。不深造而有自得处，必不会开辟新纪元、创发新时代。大家名著必有不同气味，正与名乐一般。你如与它化了，你自也可以不同。 我以前尝为青年读者写过一篇以“切实，深入，专”为题的东西。深入与专，当然有联带关系。此外，最要紧也相关的，那就要说到切实。一个人作人，最怕作到飘飘然。一个人在有些地方能够飘飘若仙，未尝没有好处。但如全不着实，全不实在，全不入里，尽是肤面表毛，油腔滑调，花言巧语，那就只能说他在作人上已经失败。读书为学也如此。不拘怎样抽象的学问，最后也不能不切实际。有的人讲学尚“空灵”，其实正是为的“如实”而不执着。假使一个人一生为学，而却全与实际不相干，那就是时力精神白费了。前说读书要读大家名著，假使这种书是现代的或讲现代的，那就更好。当然我并不是说古书就不切实际。 再补充一句。一个人读书要专，要读名著，这都说过了。但一个人有丰富的人生常识也有其必要，特别是关于你的时代，你的世界，你的国家，你的社会，以及你的身体精神的常识。一个人如果没有关于生理、心理、卫生的常识，必会常在苦恼中。 第三，前边已经提到自觉，我现在要更进一步，再加上反省。一个人不识不知的生活是一种无意思的生活；一个人不长进的生活也可说是与死差不多的生活。人怎样才能不断长进？条件之一就是时时反省。怎样反省？就是你要时时自己检讨：这件事我为什么作得很成功？那件事我为什么失败了？昨天我身体那样好，今天我为什么病了？以前我这样作很顺利，现在为什么行不通了？一个有理性的人，不但要事事自觉，事事要作得有理由，而且也要成功知道理由，失败也知道理由。反省了以后，更不惮于败，这便是进步所由成。 我近来很感到反省的必要。现在许多人作事，如有错过，总是加在别人身上，或加在不能自表的客观环境身上，绝不肯回头看看自己。这样的坚决信心，这样的勇往气概并不是没有是处，但是事情弄得不好，前途也会弄得不堪设想。因此，除了相反相成，不要过分以外，我总愿教人回头看看，也愿教人有时也作一作退一步想。也许有人要感着打了他们的高兴。其实我不过愿他在一往直前上同时也要脚步放得稳一些，不要失足，失掉不必要的牺牲而已。 中国过去有许多在人的修养上特别注意的事，也许因为有了流弊，现在遂因噎废食，再不复提的。这其中一个就是一个敬字。我近年大大感到敬字的要紧。敬不必对人，尤其要紧的还在对事。我所谓敬，差不多就是小心慎重的意思，但更加了一番庄严郑重。我近年每逢什么弄坏了，即自谓不敬不敬，以自警惕，以自改正。这是与反省相联的。我相信，假使人常能如此，一定也可减少些过失。我深愿今日青年都能早点养成这个习惯。所谓修养，也不过就是养成些好习惯，尤其是沉着慎重不轻浮的习惯。 有大志，读名典，时自反省，对事专而敬。青年的应有修养，当然还不止。这三五点却是我近来时在感到之点。勉强抽空写出来，但愿大家不吝，试试看！ （四月廿六夜） 张申府（1893－1986），名崧年，河北献县人。哲学家。1920年初参与中国共产党的建党活动，是周恩来的入党介绍人。1931年至1936年任清华大学哲学系教授。 （作于1946年4月26日。原载1946年5月4日《唯民周刊》第1卷第5期。） "},"ReadingNotes/论青年的修养-张闻天-1938.html":{"url":"ReadingNotes/论青年的修养-张闻天-1938.html","title":"论青年的修养-张闻天-1938","keywords":"","body":"论青年的修养-张闻天（1938年4月28日） 同志们!你们都是从全国各方面来的青年，所以我今天就同你们讨论讨论关于青年的问题。 从全国民族抗战开始到现在这个时期内，青年的确占着一个很重要的地位。我们无论从哪一方面来看，不论是军事方面也好，政治方面也好，文化教育方面也好，都可以看到广大青年群众的活跃。他们总是站在抗战的最前线，为中华民族的解放而牺牲奋斗。同时，由于抗战规模扩大和展开，青年的责任也更加重要了。怎样使我们的青年更能在抗战中尽他们的责任，是我们大家所要商讨的问题。我今天所要讲的青年的修养问题，即是想在这方面向大家贡献一点意见。 我认为中国现代的青年，有很多共同的优点，同时也有一些共同的弱点。怎样发挥这些优点，克服那些弱点，即是青年的修养问题的内容。我现在就根据我们的经验与考察来具体讲几点，希望大家来讨论。 一、要有坚定的高尚的理想 青年有一个很大的优点，这就是他们有高尚的理想，不论这个理想是抗日救国也好，共产主义也好。他们对于现社会是不满足的，他们希望着一个理想的社会的出现。青年的这种理想是最可宝贵的东西。正是这种理想不断地鼓舞着我们的青年向前进步。正是这种理想使他们不愿在现社会中醉生梦死，而愿意为理想社会的实现牺牲奋斗。正是这种理想，使我们的青年，在过去，在现在，创造了许多惊天动地与可歌可泣的伟大事业。眼前的例子，也可证明这一点。比如你们，陕北公学的同学，究竟为了什么不远千里而来陕北这个比较荒凉的地区呢?陕北公学的物质条件是很困难的。住的是窑洞，吃的是小米饭。教育设备都很简陋。然而你们为什么不怕一切困难而来呢?如果你们不是为了寻找高尚的理想，我想你们是不会来的。所以高尚的理想对于青年是最可宝贵的东西。我们对于青年的理想，不但不应该鄙视，而且应该极大地爱护。旧社会里的人，常常骂青年为“理想太高”。这种骂法，我们认为是根本错误的。这只是证明那些骂青年的人的无理想之可鄙而已。孙中山先生三民主义的理想，不是也给人家骂为“大炮”吗?马克思、恩格斯、列宁、斯大林的共产主义理想，不是给人家骂为“空谈”吗?然而我们知道马克思、恩格斯、列宁、斯大林、孙中山的伟大，也正是在于他们有高尚的理想。所以青年有高尚的理想，正是青年的优点。 但是如果我们把青年人的某些理想研究一下，那我们即可发现有些青年人的理想不是理想而是空想。空想可能常常是美丽的东西，然而究竟是空想而不是理想。那么，理想与空想的区别究竟在哪里呢? 首先，我们应该说，我们的理想是建筑在现社会的物质基础之上的东西。 空中楼阁，究竟是幻想而不是理想，因为它是脱离现社会的物质基础而建立在空中的。一切伟大的理想，都从现社会的具体分析得来。闭户造车究竟只能是幻想而不是理想，因为它是个人头脑中间随便想出来的，而不合于具体的与实际的物质条件。共产主义的理想，是人类有史以来最崇高的最伟大的理想，然而这个理想，只能从资本主义社会的物质基础上产生出来。脱离对于资本主义社会的科学分析的“大同世界”的思想，究竟是幻想而不是理想。 其次，我们的理想应该适合于人类社会发展的必然趋势。 我们说民族独立、民权自由，民生幸福的三民主义可以是我们今天的理想，因为中国社会今天正是向着这个方向发展着的。我们说共产主义社会是我们将来的理想，因为人类社会终究是要向着这个方向发展前去的。如果今天有人要恢复原始共产社会或恢复奴隶社会。或仿效君主专制或法西斯独裁的统治。那这种思想就不是真正的理想，因为这种思想是违反于人类现代社会发展的趋势的。这种思想，我们不叫它理想，而叫它做反动思想。因为这种思想不是要使社会走向前进，而是要它转向后退，就是所谓“开倒车”。一切不合于人类社会发展趋势的思想，只能是一种空想。 第三，理想与空想不同，就是理想是能够实现的，空想则是永远不得实现的。 而理想的实现，需要有在一定的物质条件下所产生的社会力量。没有这种力量，理想也就不能实现。战胜日本帝国主义、争取民族独立的理想，就要依靠在全国一切不愿意当亡国奴的各阶级、各党派、各团体的民族统一战线的力量之上。离开实现自己理想的社会力量而谈理想。那也是一种空想而不是理想。共产主义的最高理想，离开工人阶级的社会力量，就成为乌托邦，成为空想。 第四，理想之为理想，除上述条件外，还需有实现理想的具体办法。 比如，实现民族解放的理想，就要有建立、扩大与巩固抗日民族统一战线的办法。实现共产主义，就要有一定的纲领与步骤，一定的策略与战略。马克思列宁主义的策略与战略是每一共产主义者所必须学习的科学，因为离开了它们，理想虽是很好，然而仍然成为空谈而不能实现，使理想成为空想。 这就是理想与空想的基本区别。我们的青年如果赞成我所说的这些区别，那末请你们就拿这几个标准，来检查一下自己的理想：究竟你们过去所抱的理想，是理想还是空想?如果是空想，那就应该决然抛弃。如果是理想，但又不完全，那就应该修正与充实，或重新加以检讨与整理。我们应该不客气地说，在我们青年的理想中，常常不免有些空想的成分，因此，他们有时对于自己的理想把握不定，而发生一些朝三暮四的摇摆现象。要免去这种现象，就必须把自己的理想建筑在结实坚固的科学的基础之上。这是我所要说的第一个问题。 二、要为实现自己的理想奋斗到底 青年人在一旦觉醒，找到了自己的理想之后，都能够不顾一切为自己的理想奋斗。这种为实现自己的理想而牺牲奋斗的精神，是我们青年的一个很大优点。我们在社会中看到不少这样的人，他们也不满意于现状，他们也希望有个较好的社会出现，他们甚至同情共产主义，但是他们自己不愿意为改造社会而奋斗。他们舍不得自己的妻子儿女，他们舍不得自己的家乡故土，他们舍不得自己的生命财产，他们不得不“马马虎虎”地生活，不得不“做一天和尚撞一天钟”。但青年就不是这样。他们不了解到自己的理想则已，一旦他们了解到了自己的理想，他们就会抛弃一切，奔赴自己的理想，去为自己的理想而奋斗。这种不顾个人利害而牺牲奋斗的精神，是值得钦佩与歌颂的。 但是，实现理想，实在不是一件容易的事。理想虽是建筑在现社会的物质基础之上，但理想是超过现社会的东两。理想好比泥土中生长出来的花。它虽生长在泥土中，但它又不是泥土。所以理想看来常常是美丽的，而现社会则看来是丑恶的。要把这个丑恶的现社会变为美丽的理想社会，那决不是一天两天以至一年两年的事，而需要几十年以至上百年的奋斗与工作。不但这样，在奋斗与工作的过程中还必然要碰到无数的困难与波折，有时甚至看来似乎是不能克服以至绝望的困难。所以不论在任何困难之下，坚持自已的理想，坚持为自己理想的实现而奋斗，是绝对必要的。没有这种坚持性，任何的理想也都不能实现。 一切伟大的革命家之所以伟大，不但因为他们有着伟大的理想，而且还因为他们始终能够为了自已的理想奋斗到底。孙中山先生“致力国民革命凡四十年”。虽是在他奋斗的过程中碰到了无数的困难，然而他始终坚持他的理想，为他的理想奋斗到最后。世界无产阶级的导师马克思、恩格斯、列宁、斯大林，他们的伟大，就是他们那种为自己的理想而坚持奋斗到底的精神。他们在丑恶的旧社会中看到美丽，他们在黑暗中看到光明，他们排除万难、克服一切困难而前进。 中国共产党人曾经完成了二万五千里长征。这件事震动了全世界。为什么二万五千里长征能够有这样伟大的影响呢?原因就在于中国共产党在这次长征中充分地表现出了它为了自己的理想而牺牲奋斗与坚持到底的精神。没有这种精神，就是一千里的长征也是不可能的。在这次长征中，我们的确曾经碰到了无数的困难。我们曾经碰到了几乎不能渡过的天险金沙江与大渡河，我们曾经碰到了人类几乎没有到过的雪山与草地，我们处在敌军的四面包围之中。困难几乎是不能克服的，然而我们那时只有一个思想，就是无论如何要克服这些困难，要为自己的理想奋斗到底。最后，我们还是完成了我们当时的任务，到达了当时的目的地。 就以今天的抗战为例吧，中国的民族抗战是一个持久的战争，困难也是很多的，波折也是很多的。如果我们碰到一些困难，遭受一些挫折，就悲观失望，就准备同敌人妥协，那中国就有亡国的危险。显然地，如果我们不能坚持抗战到底，则最后战胜日寇、实现民族独立的理想是不可能的。 正是在这个问题上，在为了自己理想的实现而奋斗到底的这个问题上，我们有些青年常常表示一些弱点。他们一开始往往以无限的热情与兴奋去奔赴自己的理想，但是一旦他们碰到困难，碰到波折，他们往往不能坚持到底，以至半途而废。过去常常有人以“五分钟热度”讥笑青年学生。这种讥笑在现在说来已经不对了，因为中国青年学生经过无数的斗争，现在已经有了极大的进步。然而在某些部分的青年中缺乏斗争的坚持性，则依然是不可否认的事实。 为什么在我们部分青年中发生这种现象呢?我以为有以下几个原因。 第一，我们的青年往往对于革命的持久性估计不足。 要把人类的高尚理想实现出来的革命，像我在前面说过的，不是一天两天以至一年两年的事，而是几十年以至上百年的事。这是一个持久的斗争。不但实现共产主义理想是如此，即最后战胜日寇也不是一年两年的事，而是一个持久战。同志们大多数是从西安步行到延安的。你们对于走陕北的山地，大概已经有了相当的经验。革命就好像走陕北的山地一样，翻过了一座山又一座山。你们在路上不是常常想，翻过了前面一个山头一定再没有山了吧，一定可以到延安了吧。然而事实常常是相反的，山接连着山，而延安总是还在前面。这样你们在路上就要走十天，才得到达延安。这对于你们，就是一个很好的入学考试。然而革命究竟比翻山要持久得多，困难得多。如果我们青年参加到革命中来，没有持久斗争的准备，结果是一定要失望与半途而废的。正像你们到延安来，如果你们没有这样一定要到延安的决心，你们一定会半途而返的。 第二，我们的青年往往对于革命的困难性估计不足。 要把人类高尚的理想实现出来的革命，不克服无数的困难是不可能的。革命决不像上海的大马路那样平坦好走，决不像吃饭睡觉那样容易简便。这简直同爬着没有人迹到过的崎岖的高山一样，山上没有路，没有人家，到处是荆棘与浓密的森林，到处有毒蛇猛兽的威胁，有些地方简直是不能越过的绝壁。但是我们必须前进，必须克服一切困难前进。我们有时会被荆棘树枝所刺伤，有时会被毒蛇猛兽所咬伤，我们前面的伙伴有时会用尽了一切气力面倒毙，但是我们必须前进。如果我们的青年参加到革命中来，不预先看到这种困雅，没有克服一切困难的准备，那结果必然是“知难而退”。 第三，旧社会的思想习惯以及一切物质上的诱惑，也往往是使青年半途而废的一种极大的力量。 我们常常看到一有志的青年，受不起这种力量的压迫而退却了，而放弃了自己的理想。旧社会对于青年的理想，总是取着敌视的态度。他们用各种思想上的毒素，讥笑青年，陷害青年，使青年“老大”，“颓丧”，“消沉”。他们用地位、金钱、美女腐化青年，笼络青年，消磨青年的“朝气”。你们看到过无数这类的例子吧。当青年学生在学校读书的时候，往往“志向远大”，想“给国家民族谋点幸福”，“为人类解放做番事业”，然而一出学校到旧社会中混上几年，往往把过去的一切完全抛弃了。旧社会吞没了他们，旧社会融化了他们。所以，如果我们的青年不能同这种旧势力奋斗，坚持自己的理想，不为这些力量所动摇，那结果也必然是半途而废。 第四，我们的有些青年本身常常有一种“动摇性”，缺乏足够的忍耐与坚定。 我们常常称这种特点为“小资产阶级性”。他们往往今天参加革命工作，就希望革命在明天胜利。如果明天革命不能胜利，那他们就失望而消极了。他们在革命形势高涨的时候，常常趋向狂热与盲动，而在革命形势低落的时候，则又转到消沉与绝望的深渊。他们胜利时常常为胜利冲昏头脑，失败时则又因失败而垂头丧气。他们不会把今天一点一滴的切实的工作，同他们远大的理想联系起来；他们不能穷年累月地去为自己的远大的理想而进行今天看来好像是没有结果的工作；他们常常讨厌这种“琐碎”与“麻烦”的工作；他们不能清楚了解，一切革命的工作，只要能够坚持地干下去，一定可以得到一定的成绩。巨大的建筑工程没有一砖一瓦的砌筑，是永远不能成功的。伟大的理想不经过许多胜利与失败，是永远不会实现的。 这些是常常使我们的青年不能坚持自己的理想、为自己的理想奋斗到底的原因。这些弱点的克服，是每一个中国青年的严重任务。这当然不是一件容易的事，然而这件事是必须做与可能做到的。 当然，今天我们离开我们的理想还很远，然而我们总是在一天一天接近着自己的理想。社会主义的理想，过去还只是在书本上，而现在已经在占全世界六分之一的土地的苏联，变成了完全的现实。所以只要我们能够坚持自己的理想，为自己的理想奋斗到底，我们的理想的实现是可能而且是必然的。 三、要学习实现理想的办法 有了理想，有了实现理想的决心，是不是就已经够了呢?还是不够的。我们青年还要有实现理想的办法。 青年有一种很大的长处，就是有热烈的革命情绪，纯洁坦白的胸怀。正是因为他们有热烈的革命情绪，所以他们能够勇往直前，不顾一切困难与个人利益，向着自己的理想前进。正是因为他们有纯洁坦白的胸怀，很少受到旧社会的各种陈腐的思想、习惯与传统的影响，所以他们比较容易接受革命的真理。青年同那些麻木不仁的人是不相同的，同那顽固守旧的死硬派也是不相同的。青年是有生气的，活跃的，充满着愉快与光明的一代。他们是旧社会中生长起来的美丽的花，他们也是美丽的新社会的创造者。 在中国革命史上，我们可以举出无数例子，来证明中国青年如何常常站在民族解放的前卫地位，以他们的头颅与热血创造了无数可歌可泣的英勇的光荣的史诗。在今天伟大的民族抗战中，我们也可以到处看到青年怎样为民族抗战的最后胜利而进行着勇敢无比的斗争。如果他们没有热烈的革命情绪与纯洁坦白的胸怀，则一切这些事业的创造是不可能的。我们共产党人对于青年的这一优良的特质，尤其有深切的感觉。当我们在最黑暗最困难的时候，当在我们的周围到处是恐怖与造谣污蔑的时候，我们总是从许多青年朋友那里得到他们的同情与拥护。在今天，他们对于我们所表示的爱戴与信任，也是我们不能以言语来表示感激的。谁都可以看到，在我们延安，没有官可做，没有薪水可拿，没有很好的生活可过，还有不少人在我们的周围造我们的谣言，破坏我们，然而你们仍然不远千里而来，其原因何在呢?除了你们的那种热烈的革命情绪与纯洁坦白的胸怀，还能有其他的说明吗? 青年的这种优良的品质，是我们所应极力爱护与发扬的。 但是光是热烈的革命情绪，纯洁坦白的胸怀，还是不能实现自己的理想，完成伟大的革命事业的。有些青年的弱点，常常表现在不会使革命的热情去服从于革命的理智，不会把纯洁坦白变为对于真理的深刻的追求。响亮的革命口号，耀眼的革命词句，可以迷惑我们的青年，使他们发狂，使他们以此为满足。他们往往抓住问题的一方面，把它发挥，把它夸大，认为这是唯一的“真理”，排除一切其他的方面，否认一切其他的意见。他们的革命热情容易变为盲目的冲动，他们的纯洁坦白容易走向片面性、狭隘性与幼稚病。他们往往不能更冷静地去考虑问题，想出具体的办法来实现自己当前的任务，来实现自己的理想。这种弱点，当然是我们的青年所应该设法克服的。 我们应该使我们的青年清楚地了解到：在理想确定之后，有了为理想而奋斗到底的决心之后，学习实现理想的具体办法就有着决定的意义。在这一方面，我想向同志们贡献几点意见。 第一，就是要了解具体情况。 我们无论做什么事，首先的问题就是了解情况。正像一个在前线上指挥军队的将军，他为了要实现他打胜仗的理想，就必须要讲求作战的方法。这里，首先就是要弄清敌情，然后来决定作战的计划。我们的工作方法也是如此。比如，你们将来从学校毕业之后，你们到一个地方去工作，你们的责任决不是到那里去照你们自己的头脑中所想的来乱干一顿。而是要首先了解当时当地的具体情况，比如那里的抗战形势，那里的政治与经济，各阶级、各阶层、各党派、各民众团体以及党、政、军、民间的相互关系等。没有这种了解，我们就无法决定正确的工作方针与工作计划来实现自己的理想。 当然了解具体情况不是一件容易的事。但是，一切具体情况都是在矛盾中发展着的，都是依照普遍的辩证法的规律变动着的，所以只要我们能够把握住这些方法，那比较正确地去了解具体情况也不是不可能的。问题只是要我们的青年在接触具体情况的时候，能够更多地“想一想”，更多地研究一下，考察一下，决不要以自己的一些片面的主观的了解为满足。世界上的一切都是非常复杂的，不是马马虎虎、冒冒失失就能够了解的。如果我们没有这种了解具体情况的要求与忍耐心，那我可以告诉大家，你们是什么事也做不好的。 当然，这并不是说我们一定要彻底了解具体情况之后才能开始工作。这种彻底了解，在一个短的时期内是不可能的。但我们要求在开始工作之前，有一种初步的与起码的了解，不然，到一个地方不管一切乱干一气，那是异常危险的。 第二，在我们对于具体情况有了初步的与起码的了解之后，我们就要根据对具体情况的了解来决定我们的方针，我们的任务，斗争的形式，工作的方法方式等。 比如你们现在到上海工作，你们在解到了上海的具体情况之后，你们就必须规定你们今天在上海需要做什么，能够做什么，而且如何做法。显然地，在上海日寇军队占领的地区，我们今天的具体任务、斗争形式、工作方法方式等，决不能同其他区域相同。你们要老虑，究竟今天在上海是同在其他某些战区一样地去发展游击战争，进行公开的反日的活动，或者是这里需要执行另一种任务，即积聚自己力量准备待时而动的任务，采取另一种斗争形式，即秘密的反日斗争的活动呢?是组织公开的抗敌后援会，还是一方面组织秘密的抗日团体而同时去利用公开的合法的组织形式呢?只有正确地解决这些问题，我们才能使我们的力量巩固与发展，达到最后配合全国力量战胜日寇的理想。如果我们的任务规定得不正确，工作方法方式等都规定得不对，那我们必然会遭受严重的失败，而达不到我们的目的地。 在目前全中国人民的前面的最中心的任务是坚持抗战，最后战胜日寇。但是实现这个中心任务的方法是在各个具体的地方都不能完全相同的。我们青年要善于根据不同地区的具体情况，来决定实现这个中心任务的具体办法。由于中国政治经济发展的不平衡，由于中国的地大物博，所以实现目前中心任务的方法，都不能有千篇一律的公式。公式主义常常是我们实现总任务中的最大的障碍物。 就是各地方的具体情况，也不是固定不变的。它同样地变动着。所以你们就在一个地方的工作中仍然要时时刻刻注意新的情况的变化，而及时地改变自己的任务及工作方法方式等。至死不变的东西是没有的。这里，要求我们以极大的灵活性、机动性与创造性，来实现自己的理想。固执自己的死公式或老办法，结果必然会遭受严重的失败，而达不到目的地。 第三，在任务等规定之后，就应该立刻开始自己的实际工作。 我们是革命家，行动家，而不是空谈家。“议而不决、决而不行”的恶习惯，我们青年是决不应该学习的。我们必须把我们自己所决定的方针与计划来见诸实行。只有革命的行动才能改造世界。革命的伟人马克思曾经说过，我们的任务不但在认识世界，而主要的在改造世界。要改造世界就要有革命的实际工作，而且也只有在实际工作中，我们才更能进一步地认识世界，更正确地来决定自己的任务与工作方法方式等。也只有实际工作，能够考验我们过去的决定是否正确，充实与发展我们的决定，改正错误等。 所以我们青年在实际工作中也仍然不是盲目地乱干一顿，而要时时刻刻注意自己在实际工作中所碰到的一切问题，总结自己在实际工作中的经验。如果在实际工作中证明过去自己的决定是不正确的，是做不通的，那自己应该毫无怜惜地抛弃过去自己的决定，而根据自己对于具体情况的新的了解来重新决定自己的任务。如果有部分的错误，即应改正部分的错误。如果有不够的地方，即应以新的经验使之充实，使之发展。这里所需要的是虚心的服从真理的态度，而不是“自作聪明”去蛮干。不顾事实的蛮干，是会碰到钉子以至毁灭自己的。 所以我们的青年，应该时时刻刻在实际工作中实行工作的检查，发展自我批评，总结工作中的经验与教训，使我们的工作能够更好地得到进步与成绩。一切先进的革命的理论，我们也只能当作行动的指南而不能当作教条。一切先进的革命理论，也要在实践中充实自己与发展自己的。 我想，这三点是我们青年人在工作中所应该注意的。这三点的中心所在，即是要使我们的青年不但要有理想，有实现理想的决心，而且要学习实现理想的具体办法。 当然，青年人终究是青年人，他们一般是富于革命热情而缺乏实际经验，因此他们有时要“感情用事”，有点“冲动”，有点“幼稚病”。我们指出青年的这些弱点，并不想以此来责备青年，而只是要求青年们在自己的工作中注意到自己的这些弱点，克服这些弱点，使自己逐渐地在实际工作中成为一个比较“能干的”与“老练的”革命青年。 四、要同群众在一起去实现自己的理想 我们在前面已经讲过，要实现革命的理想，一定要依靠于一定的社会的力量，这社会的力量就是千千万万的群众。历史上的伟人，固然在创造人类历史中起了很大的作用，然而如果没有群众的拥护与群众的行动，任何推动历史前进的理想都是不可能实现的。所以我们青年要实现自己的理想，就必须要能够率领群众去为自己的理想奋斗。 青年在革命行动中肯负责，肯出头，肯打先锋，这是很可宝贵的品质。在中国民族解放的历史中，青年常常起着先锋的作用，就是由于青年这种优良品质的结果。但是就在这个方面，部分青年仍然表现出他们的弱点。他们肯负责，肯出头，肯打先锋的品质，往往发展到好出风头，目空一切，自高自大，包办，不耐烦，脱离群众的偏向。这种例子，我是见得很多的，不知道你们见过没有?我想一定是见过的。 在陕北公学学习的青年，一般地是觉悟程度较高的一部分中国人，他们的确有责任去领导另一部分比较落后而又占最大多数的中国人，发动他们，教育他们，训练他们，以提高他们的政治水平与组织力量。没有先锋的领导，广大的比较落后的群众，是不会自己走到先锋的地位的，这是无可否认的真理。 然而怎样可以使我们陕公的青年能够负担起这个先锋的领导的责任呢?是不是我们青年装出一个领导者的架子，自命不凡，目空一切，就能够实现这个领导作用呢?是不是只要我们的青年把我们的理想在群众前面一解释一宣布，群众就会跟了我们走，就实现了我们的领导作用呢?这些都是不能成功的。任何领导者，如果自命不凡，目空一切，装出领导者的架子，那结果他不但不能领导群众，而且会脱离群众。至于把很美丽的理想讲给群众听;群众自然可能来听一下，然而要他们为了这个理想来牺牲奋斗，那单靠这种宣传工作是不能达至到目的的。 领导群众，是一件非常不容易的事。青年人决不要把这个问题看得太简单了。这里，第一个问题，就是要求我们的青年到群众中去。我们的青年对于“群众”两个字的观念常常是比较抽象的，有时把“群众”理想化，有时看不起“群众”，而不能活生生地去了解“群众”。所以下决心到群众中去，实是开始领导群众时的必要的一步。 我们青年到了群众中之后，就要学习如何去接近群众，去同他们生活在一起，去了解他们，以至取得他们的信任。这里，首先就是要时时刻刻去为群众服务，处处能够为他们谋利益，为他们的利益牺牲自己的一切。其次，要我们处处谦逊和气，刻苦耐劳，宽宏大量，急公好义，诲人不倦，做人家的模范与教师。第三，要能够团结与培养群众中的积极分子，发挥积极分子的作用。依靠积极分子推动全体。第四，要以民主的精神与民主的工作方法来吸收与教育群众积极参加工作，不要个人包办一切。第五，要善于孤立最顽固的少数坏人，依靠群众的公愤去打击他们与驱逐他们。只要这样。群众就会把我们当作他们“自己的人”，就会信任我们，接受我们的领导。 无疑的，当我们真正深入到群众中去时，我们必然会看到群众中有许多落后意识，落后的思想与习惯，他们的狭隘的宗法的与行会的观点。这一切我们都会碰到的。然而我们是不是因此就失望而脱离他们呢?决不是的。我们的责任，正是要提高他们的政治水平，克服他们的落后性。但是当我们认真在群众中工作时，我们更会碰到群众中的光明的一面，他们的丰富的革命性与他们的伟大的革命力量。我们在他们的深处，可以看到在旧社会的上层所没有的那种德性。他们的力量，他们的德性，可以给我们以最大的信心，使我们相信他们是我们的最伟大理想的负担者，而且他们最后一定能胜利。这就使我们在他们中间感觉到一种崇高的安慰与喜悦，这就使我们在任何困难的情形之下，总是不会脱离他们，而且总是要依靠他们的力量来克服一切困难。事实已经证明，正在证明，将要证明，一切我们当前的困难，只要我们真能发动千百万群众起来，并且依靠他们的力量，就是一定能够克服的。一切前进的革命者，是决不应该惧怕群众的，相反的，他们必须依靠群众。 领导群众的第二个问题，就是要善于使群众根据自身的经验来了解我们的领导的正确。比如抗日民族统一战线的方针，开始时不是大多数中国人都能了解与拥护的。当时我们用什么口号使群众根据自身的经验来了解我们的方针的正确呢?这就是“停止内战，一致抗日”的口号。每一个有良心的中国人，对于中国内战的痛苦都是切身感受到的，他们都能够懂得日寇的无限制的侵掠同内战是不能分离的。因此他们也许还不了解抗日民族统一战线的方针，然而对于“停止内战，一致抗日”的口号是热烈拥护的。结果，内战停止了，中国开始统一了，抗日民族统一战线得到了初步的成功。不久卢沟桥事变发生了。当时我们用什么口号使群众根据自身的经验来了解我们的方针的正确呢?这就是“为保卫平津、保卫华北、保卫全中国而战”。每一个有良心的中国人，对于日寇的这种无限制的侵掠已经到了不能再行忍耐的地步，他们都要求抵抗。全国抗战开始了，于是抗日民族统一战线有了进一步的成功。在抗战发动之后，我们用什么口号使群众根据切身的经验来了解我们的方针的正确呢?这就是“坚持抗战、争取抗战最后胜利”的口号。每一个有良心的中国人，对于这个口号是能够懂得的，因为谁都可以看到，如果中国今天半途投降妥协，那中国就会亡国，中国人就会变为亡国奴，而亡国奴是不好当的。于是抗日民族统一战线有了更进一步的成功。 这不过是举一个例子罢了。不但在提出口号上是如此，即在斗争形式的采用上也是如此。我们要使自己不脱离群众，必须采取那种能够吸引群众参加的斗争形式，他们认为今天可以而且需要的斗争形式，以锻炼他们，提高他们的觉悟程度与组织力量。在斗争形式上是如此，在其他问题上也莫不如此。 不估计群众今天觉悟的程度，而提出很高很“左”的口号，是决然不能动员群众参加运动的。这里需要采取各种过渡的办法来动员他们，再在运动中提高他们走向我们的理想。这里，一切急性病是有害的;这里，领导者的努力工作的精神，要同极大的忍耐心配合起来。群众的觉悟的速度，不但依靠于我们的工作，而且依靠于无数客观的政治经济的要素。革命是决不能凭主观的愿望来制造的。在没有革命形势的时候，特别在反动的黑暗的时期，群众觉悟的速度非常缓慢。那时我们如果没有等待时机的忍耐心，而想用强追命令的方法去对待群众，那结果是必然会失败的。所谓等待时机，当然不是不要努力工作，而是把我们的工作放在准备必要的力量上，以便将来革命形势到来时，群众成千成万卷入革命浪潮中时，我们能够站在浪潮的前面负担起领导者的责任。 在群众工作的命令主义，结果没有不遭受失败的。群众在某种压力之下，可能勉强地服从，然而一切不根据群众自觉的服从，违反于他们意志的服从，最后必然会引起他们反抗并推翻那些压迫者或命令主义者。只有群众自觉的自愿的服从，才能发挥群众的积极性与创造力量到最高限度，来实现我们远大的理想。所以我们领导群众的中心问题，也就是如何使群众心悦诚服地来接受我们的领导，并且为了我们的理想的实现牺牲一切。这就要求我们在领导工作中善于使群众能够根据自身的经验来了解我们领导的正确。这里，应该反对那些光会喊喊革命口号、唱唱高调的空谈主义者，与企图以强迫命令的方法来“驱使群众为己用”的老爷们! 但是，当革命形势迅速发展、群众觉醒的速度一日千里的时候，我们要谨防自己落后于群众，变为群众的尾巴。在这种情况之下，我们仍然应该站在群众的前面，组织群众的积极性向着一定的目标前进。我们要善于迅速地改变过时的口号，提出新的更高的动员口号，采取大刀阔斧的办法来动员、组织与武装群众，推动群众斗争走向最高的形式。比如在目前被敌人占领区域的广大乡村与战区的情况，就是如此。群众抗日的斗争往往一开始就采取武装斗争的形式。在这里，几天内群众所达到的觉醒程度，可以比上过去的几十年。旧的口号、旧的斗争方式与工作方法等，决不能满足群众今天的要求。群众的民族革命的高潮，会冲开一切这些陈腐的东西，找到他们的新的领导者，走向他们的目的地。 领导群众的第三个问题，就是要向群众学习。一切革命的经验，都是从群众的实际斗争中创造出来的。离开群众的革命实践，我们就不能有革命的理论。而群众的革命实践是长生的，是无穷的，是永远继续前进着的。它发展与充实我们的理论，它改正我们的错误，它使我们一天一天接近着绝对的真理。所以我们必须在群众斗争中去学习，在这个人生的大海中去学习。我们的学习是没有止境的。群众的革命实践不断创造出极可宝贵的斗争形式、组织形式、工作方法与方式以及各种极可宝贵的经验。这些群众创造出来的东西，常常是一个领导者，即使是天才的领导者，也不能预料到的。有时一个领导者常常苦于找不到一个适当的斗争形式或组织形式以进一步开展群众运动。但是群众的革命实践常常创造了这种适合的形式。群众对于一个问题所发表的意见，他们对于一个问题的看法与感觉，常常是一个领导者，即使是天才的领导者，也想不到的与感觉不到的。而正是他们的意见，他们的看法与感觉，可以补足一个领导者对于某个问题的理解之不足与缺陷，使某个问题得到圆满的正确的解决。 所以，群众不但需要我们去教育，而且他们也教育我们。我们的青年切不要在群众前面摆资格，自高自大，瞧不起群众，而应该很虚心地去跟他们学习，很细心地去倾听他们的意见，与他们的脉搏一起跳动。 总之，我们在领导群众中的基本原则，是无论如何要领导群众前进，而同时无论如何不要脱离群众。脱离群众的先锋主义与落后于群众的尾巴主义，都是我们所不应该赞成的。 同志们，要把这样广大的、千千万万的、觉悟程度不同的群众动员到为我们的理想而牺牲奋斗的一条战线上，真不是简单的事。要达到这个目的，你们无论如何不要脱离群众，而要成为他们的领袖。 同志们！这就是我今天想同同志们讨论的关于青年的修养的四个问题。 同志们！我前面已经说过，革命是一件最伟人的事业，也是最困难的事业。我们在革命中犯些错误，甚至是非常严重的错误，是免不了的。至于青年朋友们，由于生活经验与斗争经验的缺乏，犯些错误更不是什么不得了的问题。我们决不能因为怕犯错误，就不干革命。相反的，我们就要在错误中学习。 我们青年人切不要以自己已经有的一点知识与一点经验，甚至一些生吞活剥的革命的公式与口号为满足。这对于青年们是最大的危险，因为这种满足就阻碍了你们的进步，结果必然会使你们成为无用的“空头革命家”。青年们必须以列宁的“学习学习再学习”的口号，当作自己的座右铭。你们今天在陕北公学学习，明天就要到抗战的前线与后方的实际工作中去学习。青年应该有最热烈的学习愿望，很高的学习精神与谦逊的学习态度。只有这种不断地学习，才能丰富你们的知识与经验，才能使你们成为一个能干的革命者，才能使你们更能负担起在自己肩膀上所负担的责任。 我重复地说，工作中的错误是谁也免不了的。我们就要在错误中学习。任何错误，不论为它曾经付了如何高昂的代价，只要我们从错误中得了宝贵的经验，那这种代价也是不算可惜的。所以，我们用不着怕犯错误。但犯了错误，我们应该知道很快地去改正错误，切不要坚持错误。任何错误，如果坚持下去，那是非常危险的。青年人应该打破“爱好面子”、不肯或者害怕承认错误的恶劣倾向。青年人应该“爱好真理”。一切错误的非真理的东西，都应该决然抛弃，丝毫也不应该留恋。只有这样，才能使你们不断地进步。 所以，同志们，你们更努力地去学习革命理论吧，更大胆地去工作奋斗吧，在学习的过程中，在工作奋斗的过程中，去锻炼你们自己成为钢铁一样的战士吧！要不怕困难，不怕挫折与失败，要不怕天不怕地，要再接再厉地不屈不挠地向着你们光明的伟大的理想前进! "},"ReadingNotes/计算之魂-吴军/":{"url":"ReadingNotes/计算之魂-吴军/","title":"计算之魂","keywords":"","body":" 引子 计算的本质-从机械到电子 第一章 毫厘千里之差-大 O 概念 第二章 逆向思考-从递推到递归 "},"ReadingNotes/语录-毛泽东.html":{"url":"ReadingNotes/语录-毛泽东.html","title":"Mao","keywords":"","body":"丢掉幻想，准备斗争 “说来也是笑话，我读过小学、中学，也当过兵，却不曾看见过世界地图，因此就不知道世界有多大。湖南省图书馆的墙壁上，挂有一张世界大地图，我每天经过那里，总是站着看一看。 过去我认为湘潭县大，湖南省更大，中国自古就称为天下，当然大得了不得。但从这个地图上看来，中国只占世界的一小部分，湖南省更小，湘潭县在地图上没有看见，韶山当然更没有影子了。世界原来有这么大！ 世界既大，人就一定特别多。这样多的人怎样过生活，难道不值得我们注意吗？从韶山冲的情形来看，那里的人大都过着痛苦的生活，不是挨饿，就是挨冻。有无钱治病看着病死的；还有家庭里、乡邻间，为着大大小小的纠纷，吵嘴、打架，闹得鸡犬不宁，甚至弄得投塘、吊颈的；至于没有书读，做一世睁眼瞎子的就更多了。在韶山冲里，我就没有看见几个生活过得快活的人。韶山冲的情形是这样，全湘潭县、全湖南省、全中国、全世界的情形，恐怕也差不多！ 我真怀疑，人生在世间，难道都注定要过痛苦的生活吗？决不！为什么会有这种现象呢？ 这是制度不好，政治不好，是因为世界上存在人剥削人、人压迫人的制度，所以使世界大多数的人都陷入痛苦的深潭。这种不合理的现象，是不应该永远存在的，是应该彻底推翻、彻底改造的！总有一天，世界会起变化，一切痛苦的人，都会变成快活的人！幸福的人！ 世界的变化，不会自己发生，必须通过革命，通过人的努力。我因此想到，我们青年的责任真是重大，我们应该做的事情真多，要走的道路真长。 从这时候起，我就决心要为全中国痛苦的人、全世界痛苦的人贡献自己全部的力量。” 我是个学生出身的人，那时，我觉得世界上干净的人只有知识分子，工人农民总是比较脏的。知识分子的衣服，别人的我可以穿，以为是干净的；工人农民的衣服，我就不愿意穿，以为是脏的。革命了，同工人农民和革命军的战士在一起了，我逐渐熟悉他们，他们也逐渐熟悉了我。这时，只是在这时，我才根本地改变了资产阶级学校所教给我的那种资产阶级的和小资产阶级的感情。这时，拿未曾改造的知识分子和工人农民比较，就觉得知识分子不干净了，最干净的还是工人农民，尽管他们手是黑的，脚上有牛屎，还是比资产阶级和小资产阶级知识分子都干净。 “一个人；如若不被敌人反对，那就不好了，那一定是同敌人同流合污了。如若被敌人反对，那就好了，那就证明我们同敌人划清界限了。如若敌人起劲地反对我们，把我们说得一塌糊涂，一无是处，那就更好了，那就证明我们不但同敌人划清了界限，而且证明我们的工作是卓有成效的了。” 与天奋斗，其乐无穷； 与地奋斗，其乐无穷； 与人奋斗，其乐无穷。 “让内外反动派在我们的面前发抖吧！让他们去说我们这也不行那也不行吧！中国人民不屈不挠的努力，必将稳步地达到自己的目的！” “仗我们是不怕打的，帝国主义要想‘和平演变’我们这一代人也难；可下一代、再下一代就不好讲了。中国人讲‘君子之泽，五世而斩’，英国人说‘爵位不传三代’；到我们的第三代、第四代人身上，情形又会是个什么样子啊？我不想哪一天，在中国的大地上再出现人剥削人的现象，再出现资本家、企业主、雇工、妓女和吸食鸦片烟；如果那样，许多烈士的血就白流了……” "}}
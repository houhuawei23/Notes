# 2020-09: PettingZoo: Gym for Multi-Agent Reinforcement Learning (arXiv)

-> PettingZoo: A Standard API for Multi-Agent Reinforcement Learning

This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.

本文介绍了 PettingZoo 库及其配套的代理环境循环（“AEC”）游戏模型。PettingZoo 是一个包含多样化多智能体环境集合的库，拥有统一且优雅的 Python API。开发 PettingZoo 旨在加速多智能体强化学习（“MARL”）领域的研究，通过使工作更具互换性、易访问性和可重复性，类似于 OpenAI 的 Gym 库对单智能体强化学习所做出的贡献。PettingZoo 的 API 虽然继承了 Gym 的许多特性，但在 MARL API 中独树一帜，因为它基于新颖的 AEC 游戏模型。我们通过针对流行 MARL 环境中主要问题的案例研究，部分论证了流行的游戏模型作为 MARL 常用游戏的概念模型存在不足，因此可能引发难以察觉的混淆性错误，而 AEC 游戏模型则有效解决了这些问题。

- [arXiv: PettingZoo](https://arxiv.org/abs/2009.14471)
- [openreview version: ICLR 2021](https://openreview.net/pdf?id=WoLQsYU8aZ)
- [arXiv: Agent Environment Cycle Games](https://arxiv.org/abs/2009.13051)
  - Partially Observable Stochastic Games (POSGs) are the most general and common model of games used in Multi-Agent Reinforcement Learning (MARL). We argue that the POSG model is conceptually ill suited to software MARL environments, and offer case studies from the literature where this mismatch has led to severely unexpected behavior. In response to this, we introduce the Agent Environment Cycle Games (AEC Games) model, which is more representative of software implementation. We then prove it's as an equivalent model to POSGs. The AEC games model is also uniquely useful in that it can elegantly represent both all forms of MARL environments, whereas for example POSGs cannot elegantly represent strictly turn based games like chess.
  - 部分可观测随机博弈（POSGs）是多智能体强化学习（MARL）中最通用且常见的博弈模型。我们认为，POSG 模型在概念上并不适合软件 MARL 环境，并通过文献中的案例研究展示了这种不匹配如何导致严重出乎意料的行为。针对这一问题，我们引入了代理环境循环博弈（AEC Games）模型，该模型更能代表软件实现。随后，我们证明了 AEC Games 模型与 POSGs 模型是等价的。AEC Games 模型的独特之处在于，它能够优雅地表示所有形式的 MARL 环境，而 POSGs 模型则无法优雅地表示像国际象棋这样严格回合制的游戏。
- [Gymnasium](https://github.com/Farama-Foundation/Gymnasium)
  - An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)
  - [gymnasium doc](https://gymnasium.farama.org/)
- [PettingZoo](https://github.com/Farama-Foundation/PettingZoo/)
  - An API standard for multi-agent reinforcement learning environments, with popular reference environments and related utilities
  - [pettingzoo doc](https://pettingzoo.farama.org/)

## Introduction

Multi-Agent Reinforcement Learning (MARL) has been behind many of the most publicized achievements of modern machine learning — AlphaGo Zero (Silver et al., 2017), OpenAI Five (OpenAI, 2018), AlphaStar (Vinyals et al., 2019). These achievements motivated a boom in MARL research, with Google Scholar indexing 9,480 new papers discussing multi-agent reinforcement learning in 2020 alone. Despite this boom, conducting research in MARL remains a significant engineering challenge. A large part of this is because, unlike single agent reinforcement learning which has OpenAI’s Gym, no de facto standard API exists in MARL for how agents interface with environments. This makes the reuse of existing learning code for new purposes require substantial effort, consuming researchers’ time and preventing more thorough comparisons in research. This lack of a standardized API has also prevented the proliferation of learning libraries in MARL. While a massive number of Gym-based single-agent reinforcement learning libraries or code bases exist (as a rough measure 669 pip-installable packages depend on it at the time of writing GitHub (2021)), only 5 MARL libraries with large user bases exist (Lanctot et al., 2019; Weng et al., 2020; Liang et al., 2018; Samvelyan et al., 2019; Nota, 2020). The proliferation of these Gym based learning libraries has proved essential to the adoption of applied RL in fields like robotics or finance and without them the growth of applied MARL is a significantly greater challenge. Motivated by this, this paper introduces the PettingZoo library and API, which was created with the goal of making research in MARL more accessible and serving as a multi-agent version of Gym.

多智能体强化学习（MARL）是现代机器学习中许多备受瞩目成就背后的技术——如 AlphaGo Zero（Silver 等人，2017 年）、OpenAI Five（OpenAI，2018 年）、AlphaStar（Vinyals 等人，2019 年）。这些成就推动了 MARL 研究的蓬勃发展，仅在 2020 年，Google Scholar 就索引了 9,480 篇讨论多智能体强化学习的新论文。尽管研究热潮高涨，但在 MARL 领域开展研究仍面临重大工程挑战。这很大程度上是因为，与拥有 OpenAI Gym 的单智能体强化学习不同，MARL 中缺乏一个关于智能体如何与环境交互的事实标准 API。这使得将现有学习代码重用于新目的需要大量努力，消耗研究人员的时间，并阻碍了研究中更全面的比较。这种标准化 API 的缺失也阻碍了 MARL 学习库的普及。 尽管存在大量基于 Gym 的单智能体强化学习库或代码库（粗略估计，截至撰写本文时，GitHub 上有 669 个可 pip 安装的包依赖于它），但仅有 5 个拥有大量用户的多智能体强化学习（MARL）库（Lanctot 等人，2019；Weng 等人，2020；Liang 等人，2018；Samvelyan 等人，2019；Nota，2020）。这些基于 Gym 的学习库的普及对于推动强化学习在机器人学或金融等领域的应用至关重要，若没有它们，应用多智能体强化学习的增长将面临更大的挑战。受此启发，本文介绍了 PettingZoo 库及其 API，其创建目的是使多智能体强化学习研究更加易于进行，并作为 Gym 的多智能体版本服务于研究社区。

Prior to PettingZoo, the numerous single-use MARL APIs almost exclusively inherited their design from the two most prominent mathematical models of games in the MARL literature—Partially Observable Stochastic Games (“POSGs”) and Extensive Form Games (“EFGs”). During our development, we discovered that these common models of games are not conceptually clear for multi-agent games implemented in code and cannot form the basis of APIs that cleanly handle all types of multi-agent environments.

在 PettingZoo 之前，众多一次性使用的多智能体强化学习（MARL）API 几乎无一例外地继承了 MARL 文献中两种最著名的游戏数学模型——部分可观测随机博弈（“POSGs”）和扩展形式博弈（“EFGs”）的设计。在我们的开发过程中，我们发现这些常见的游戏模型对于代码实现的多智能体游戏在概念上并不清晰，无法作为基础构建出能够干净利落地处理所有类型多智能体环境的 API。

To solve this, we introduce a new formal model of games, Agent Environment Cycle (“AEC”) games that serves as the basis of the PettingZoo API. We argue that this model is a better conceptual fit for games implemented in code. and is uniquely suitable for general MARL APIs. We then prove that any AEC game can be represented by the standard POSG model, and that any POSG can be represented by an AEC game. To illustrate the importance of the AEC games model, this paper further covers two case studies of meaningful bugs in popular MARL implementations. In both cases, these bugs went unnoticed for a long time. Both stemmed from using confusing models of games, and would have been made impossible by using an AEC games based API.

为解决此问题，我们引入了一种新的游戏形式化模型——代理环境循环（“AEC”）游戏，该模型作为 PettingZoo API 的基础。我们认为，此模型更契合于代码实现的游戏概念，并特别适用于通用多智能体强化学习（MARL）API。随后，我们证明了任何 AEC 游戏均可由标准的 POSG 模型表示，反之亦然。为阐明 AEC 游戏模型的重要性，本文进一步探讨了流行 MARL 实现中两个具有意义的错误案例。在这两个案例中，这些错误长时间未被察觉，均源于使用了令人困惑的游戏模型，而若采用基于 AEC 游戏的 API，则可避免此类错误的发生。

## 2 Background and Related Works

## 3 PettingZoo Design Goals

## 4 Case Studies of Problems With The POSG Model in MARL

## 5 The Agent Environment Cycle Games Model

## 6 API Design

## 9 Conclusion

This paper introduces PettingZoo, a Python library of many diverse multi-agent reinforcement learning environments under one simple API, akin to a multi-agent version of OpenAI’s Gym library, and introduces the agent environment cycle game model of multi-agent games.
本文介绍了 PettingZoo，一个集成了多种多样多智能体强化学习环境的 Python 库，其提供统一的简易 API 接口，类似于 OpenAI Gym 库的多智能体版本，并引入了多智能体游戏中的智能体环境循环博弈模型。

Given the importance of multi-agent reinforcement learning, we believe that PettingZoo is capable of democratizing the field similar to what Gym previously did for single agent reinforcement learning, making it accessible to university scale research and to non-experts. As evidenced by it’s early adoption into numerous MARL libraries and courses, PettingZoo is moving in the direction of accomplishing this goal.
鉴于多智能体强化学习的重要性，我们相信 PettingZoo 有能力像 Gym 之前为单智能体强化学习所做的那样，使该领域民主化，使其能够被大学规模的研究和非专家所接触。正如其早期被众多 MARL 库和课程所采用所证明的那样，PettingZoo 正朝着实现这一目标的方向迈进。

We’re aware of one notable limitation of the PettingZoo API. Games with significantly more than 10,000 agents (or potential agents) will have meaningful performance issues because you have to step each agent at once. Efficiently updating environments like this, and inferencing with the associated policies, requires true parallel support which almost certainly should be done in a language other than Python. Because of this, we view this as a practically acceptable limitation.
我们意识到 PettingZoo API 存在一个显著的局限性。对于拥有远超过 10,000 个代理（或潜在代理）的游戏，由于需要同时推进每个代理的步骤，将会遇到显著的性能问题。高效更新此类环境，并结合相关策略进行推理，需要真正的并行支持，这几乎肯定需要在 Python 之外的语言中实现。因此，我们认为这是一个实际可接受的限制。

We see three directions for future work. The first is additions of more interesting environments under our API (possibly from the community, as has happened with Gym). The second direction we envision is a service to allow different researchers’ agents to play against each other in competitive games, leveraging the standardized API and environment set. Finally, we envision the development of procedurally generated multi-agent environments to test how well methods generalize, akin to the Gym procgen environments (Cobbe et al., 2019).
我们预见了未来工作的三个方向。首先是在我们的 API 下添加更多有趣的环境（可能来自社区，正如 Gym 所经历的那样）。我们设想的第二个方向是提供一项服务，让不同研究者的智能体能够在竞争性游戏中相互对抗，利用标准化的 API 和环境集。最后，我们预见了程序化生成的多智能体环境的开发，以测试方法的泛化能力，类似于 Gym 的 procgen 环境（Cobbe 等人，2019 年）。

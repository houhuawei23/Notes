# 2023-04 Drones: Task Assignment of UAV Swarms Based on Deep Reinforcement Learning

- åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ— äººæœºèœ‚ç¾¤ä»»åŠ¡åˆ†é…
- å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç©ºé—´æ§åˆ¶ä¸æƒ¯æ€§æŠ€æœ¯ä¸­å¿ƒæ™ºèƒ½å¯¼èˆªè¯¾é¢˜ç»„
- Space Control and Inertial Technology Research Center, Harbin Institute of Technology, Harbin 150000, China
- Bo Liu, Shulei Wang, Qinghua Li, Xinyang Zhao, Yunqing Pan and Changhong Wang
- [doi](https://doi.org/10.3390/drones7050297)
- [mdpi](https://www.mdpi.com/2504-446X/7/5/297)

ä½œè€…åœ¨æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å®Œå…¨åˆ†å¸ƒå¼æ— äººæœºèœ‚ç¾¤ä»»åŠ¡åˆ†é…ç®—æ³•â€”â€”Ex-MADDPGï¼Œå³åœ¨ MADDPG ç®—æ³•çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹ã€èœ‚ç¾¤åŒæ­¥è®­ç»ƒæœºåˆ¶ä»¥åŠæ‰©å±•å¤šé‡å†³ç­–æµç¨‹ç­‰æ”¹è¿›ï¼Œæœ‰æ•ˆå…‹æœäº† MADDPG ç®—æ³•çš„æ‰©å±•æ€§å·®ã€éš¾é€‚åº”å¤§è§„æ¨¡æ™ºèƒ½ä½“åœºæ™¯çš„ç¼ºç‚¹ã€‚æ–‡ä¸­ä¾æ® MADDPG ç®—æ³•ä»¥åŠèœ‚ç¾¤æ‰“å‡»ä»»åŠ¡åˆ†é…åœºæ™¯çš„éœ€æ±‚è®¾è®¡äº†å¥–åŠ±æ¨¡å‹ã€åŠ¨ä½œæ¨¡å‹ä»¥åŠé€šè®¯æ¨¡å‹ç”¨ä»¥å®Œæˆä»»åŠ¡åˆ†é…ã€‚å…¶æ¬¡ï¼Œç”±äºæ‰©å±•æ€§çš„è¦æ±‚ï¼Œè®¾è®¡äº†å…·æœ‰æ‰©å±•æ½œåŠ›çš„å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¼šåœ¨ä½æ•°ç›®çš„è®­ç»ƒåœºæ™¯ä¸­æ¨¡æ‹Ÿå‡ºè™šæ‹Ÿæ™ºèƒ½ä½“ï¼Œä»¥è¾¾åˆ°åœ¨å¤§è§„æ¨¡åœºæ™¯è®­ç»ƒçš„æ•ˆæœï¼Œæå‡ºèœ‚ç¾¤åŒæ­¥è®­ç»ƒæœºåˆ¶ï¼Œèƒ½å¤Ÿç›´æ¥å°†è®­ç»ƒå¥½çš„ç­–ç•¥å¤åˆ¶åˆ°æ›´å¤šæ•°ç›®çš„æ— äººæœºç¾¤ä½“ä¸Šè€Œä¸å¿…è€ƒè™‘æ— äººæœºä¹‹é—´çš„ç­–ç•¥é…åˆï¼Œä»»åŠ¡åˆ†é…ç®—æ³•ç»è¿‡æ‰©å±•ä¹‹åï¼Œå¯ç›´æ¥åº”ç”¨äºå¤§è§„æ¨¡çš„ä»»åŠ¡åˆ†é…é—®é¢˜ã€‚ä¸ºä¿è¯ä»»åŠ¡åˆ†é…ç®—æ³•æ€§èƒ½ï¼Œæå‡ºäº†æ‰©å±•å¤šé‡å†³ç­–æµç¨‹ï¼Œä¿è¯è®­ç»ƒç½‘ç»œå¯ä»¥ç›´æ¥éƒ¨ç½²åœ¨æ›´å¤šçš„æ— äººæœºä¸Šã€‚

## Abstract

UAV swarm applications are critical for the future, and their mission-planning and decision-making capabilities have a direct impact on their performance. However, creating a dynamic and scalable assignment algorithm that can be applied to various groups and tasks is a significant challenge. To address this issue, we propose the Extensible Multi-Agent Deep Deterministic Policy Gradient (Ex-MADDPG) algorithm, which builds on the MADDPG framework. The Ex-MADDPG algorithm improves the robustness and scalability of the assignment algorithm by incorporating **local communication**, **mean simulation observation**, **a synchronous parameter-training mechanism**, and **a scalable multiple-decision mechanism**. Our approach has been validated for effectiveness and scalability through both simulation experiments in the Multi-Agent Particle Environment (MPE) and a real-world experiment. Overall, our results demonstrate that the Ex-MADDPG algorithm is effective in handling various groups and tasks and can scale well as the swarm size increases. Therefore, our algorithm holds great promise for mission planning and decision-making in UAV swarm applications.

æ— äººæœºé›†ç¾¤åº”ç”¨å¯¹æœªæ¥è‡³å…³é‡è¦ï¼Œå…¶ä»»åŠ¡è§„åˆ’ä¸å†³ç­–èƒ½åŠ›ç›´æ¥å½±å“å…¶æ€§èƒ½è¡¨ç°ã€‚ç„¶è€Œï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿé€‚åº”ä¸åŒç¾¤ä½“å’Œä»»åŠ¡çš„åŠ¨æ€å¯æ‰©å±•åˆ†é…ç®—æ³•æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäº MADDPG æ¡†æ¶çš„å¯æ‰©å±•å¤šæ™ºèƒ½ä½“æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆEx-MADDPGï¼‰ç®—æ³•ã€‚Ex-MADDPG ç®—æ³•é€šè¿‡èå…¥**å±€éƒ¨é€šä¿¡**ã€**å¹³å‡æ¨¡æ‹Ÿè§‚æµ‹**ã€**åŒæ­¥å‚æ•°è®­ç»ƒæœºåˆ¶**åŠ**å¯æ‰©å±•çš„å¤šå†³ç­–æœºåˆ¶**ï¼Œå¢å¼ºäº†åˆ†é…ç®—æ³•çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ Multi-Agent Particle Environmentï¼ˆMPEï¼‰ä¸­çš„ä»¿çœŸå®éªŒåŠå®é™…åœºæ™¯å®éªŒä¸­éƒ½éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“æœè¡¨æ˜ Ex-MADDPG ç®—æ³•èƒ½æœ‰æ•ˆå¤„ç†å„ç§ç¾¤ä½“å’Œä»»åŠ¡ï¼Œå¹¶éšç€é›†ç¾¤è§„æ¨¡æ‰©å¤§å±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚å› æ­¤ï¼Œè¯¥ç®—æ³•åœ¨æ— äººæœºé›†ç¾¤åº”ç”¨çš„ä»»åŠ¡è§„åˆ’ä¸å†³ç­–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚

Keywords: UAV swarm; task assignment; deep reinforcement learning; Ex-MADDPG

- [2023-04 Drones: Task Assignment of UAV Swarms Based on Deep Reinforcement Learning](#2023-04-drones-task-assignment-of-uav-swarms-based-on-deep-reinforcement-learning)
  - [Abstract](#abstract)
  - [Introduction](#introduction)
    - [1.1. Related Works](#11-related-works)
    - [1.2. Contribution](#12-contribution)
  - [2 Deep Reinforcement Learning Background](#2-deep-reinforcement-learning-background)
  - [3 Extensible Task Assignment Algorithm of UAV Swarm](#3-extensible-task-assignment-algorithm-of-uav-swarm)
    - [3.1 Local Communication Model](#31-local-communication-model)
    - [3.2 Mean Simulation Observation Model](#32-mean-simulation-observation-model)
    - [3.3 Swarm Synchronization Training](#33-swarm-synchronization-training)
    - [3.4 Extensible Multi-Decision Mechanism](#34-extensible-multi-decision-mechanism)
  - [4 Simulation Experiments and Results](#4-simulation-experiments-and-results)
    - [4.1 Training Experiment Scenario](#41-training-experiment-scenario)
    - [4.2 Construction of Training Model](#42-construction-of-training-model)
      - [4.2.1 Action Value](#421-action-value)
      - [4.2.2 Mean Simulation Observation](#422-mean-simulation-observation)
      - [4.2.3 Centralized and Distributed Reward Function](#423-centralized-and-distributed-reward-function)
    - [4.3 Validity of the Algorithm](#43-validity-of-the-algorithm)
    - [4.4 Extended Experiments](#44-extended-experiments)
    - [4.5 Extended Performance Test](#45-extended-performance-test)
      - [4.5.1 Task Completion Rate](#451-task-completion-rate)
      - [4.5.2 Task Loss](#452-task-loss)
      - [4.5.3 Decision Time](#453-decision-time)
      - [4.5.4 Number of Communications](#454-number-of-communications)
  - [5 Experiments and Results](#5-experiments-and-results)
    - [5.1 Architecture Overview](#51-architecture-overview)
    - [5.2 Flight Test](#52-flight-test)
  - [6 Conclusions](#6-conclusions)
  - [7 References](#7-references)

## Introduction

With their advantages of high altitude, low price, and strong substitutability, unmanned aerial vehicle (UAV) swarms are becoming increasingly prevalent in daily life. UAV swarm refers to a large number of UAVs with weak autonomous capability that can effectively perform complex tasks such as multi-aircraft formation and cooperative attack through information interaction and autonomous decision-making.

æ— äººæœºé›†ç¾¤å‡­å€Ÿå…¶é«˜ç©ºä½œä¸šã€æˆæœ¬ä½å»‰åŠå¼ºæ›¿ä»£æ€§ç­‰ä¼˜åŠ¿ï¼Œåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ—¥ç›Šæ™®åŠã€‚æ— äººæœºé›†ç¾¤æŒ‡çš„æ˜¯å¤§é‡è‡ªä¸»èƒ½åŠ›è¾ƒå¼±çš„æ— äººæœºï¼Œé€šè¿‡ä¿¡æ¯äº¤äº’ä¸è‡ªä¸»å†³ç­–ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œå¤šæœºç¼–é˜Ÿã€ååŒæ”»å‡»ç­‰å¤æ‚ä»»åŠ¡ã€‚

UAV swarm target-attacking is a complex process, including autonomous path planning, target detection, and task assignment, and it is almost impossible to design one algorithm to complete the whole combat process mentioned above. Therefore, this paper simplifies the whole UAV swarm target-attacking process into two parts: target detection and target assignment. The target-detection and target-assignment abilities of the UAV swarm affect the quality of mission accomplishment and are the most important parts of the swarm target-attacking system. However, different tasks have significant differences in operational objectives, time constraints, mission requirements, and other aspects. Simultaneously, sub-task coupling, self-organizing, and the large-scale nature of swarms pose great challenges for the mission planning and decision-making of the UAV swarm.

æ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œæ¶‰åŠè‡ªä¸»è·¯å¾„è§„åˆ’ã€ç›®æ ‡æ£€æµ‹å’Œä»»åŠ¡åˆ†é…ç­‰å¤šä¸ªç¯èŠ‚ï¼Œå‡ ä¹ä¸å¯èƒ½è®¾è®¡ä¸€ç§ç®—æ³•æ¥å®Œæˆä¸Šè¿°æ•´ä¸ªä½œæˆ˜æµç¨‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡ç®€åŒ–äº†æ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»çš„å…¨è¿‡ç¨‹ï¼Œå°†å…¶åˆ†ä¸ºç›®æ ‡æ£€æµ‹å’Œç›®æ ‡åˆ†é…ä¸¤éƒ¨åˆ†ã€‚æ— äººæœºé›†ç¾¤çš„ç›®æ ‡æ£€æµ‹ä¸ç›®æ ‡åˆ†é…èƒ½åŠ›ç›´æ¥å½±å“ä»»åŠ¡å®Œæˆçš„è´¨é‡ï¼Œæ˜¯é›†ç¾¤ç›®æ ‡æ”»å‡»ç³»ç»Ÿä¸­æœ€ä¸ºå…³é”®çš„éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¸åŒä»»åŠ¡åœ¨ä½œæˆ˜ç›®æ ‡ã€æ—¶é—´é™åˆ¶ã€ä»»åŠ¡éœ€æ±‚ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŒæ—¶ï¼Œå­ä»»åŠ¡é—´çš„è€¦åˆæ€§ã€è‡ªç»„ç»‡æ€§ä»¥åŠé›†ç¾¤çš„å¤§è§„æ¨¡ç‰¹æ€§ï¼Œç»™æ— äººæœºé›†ç¾¤çš„ä»»åŠ¡è§„åˆ’ä¸å†³ç­–å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚

In recent years, the great potential of reinforcement learning (RL) within the swarm intelligence domain makes it an important approach to studying UAV swarm task assignment. However, RL task-assignment algorithms applied to UAV swarms still face a series of technical bottlenecks such as low sampling efficiency, difficult reward function design, poor stability, and poor scalability, so it is especially critical for scalable and robust task planning and decision-making algorithms to be designed for UAV swarms. Therefore, we propose a scalable task-assignment method to deal with the dynamic UAV swarm task planning and decision-making problem in this paper.

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç¾¤ä½“æ™ºèƒ½é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ä½¿å…¶æˆä¸ºç ”ç©¶æ— äººæœºç¾¤ä»»åŠ¡åˆ†é…çš„é‡è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œåº”ç”¨äºæ— äººæœºç¾¤çš„ RL ä»»åŠ¡åˆ†é…ç®—æ³•ä»é¢ä¸´é‡‡æ ·æ•ˆç‡ä½ã€å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾ã€ç¨³å®šæ€§å·®å’Œå¯æ‰©å±•æ€§å·®ç­‰ä¸€ç³»åˆ—æŠ€æœ¯ç“¶é¢ˆï¼Œå› æ­¤è®¾è®¡å¯æ‰©å±•ä¸”é²æ£’çš„æ— äººæœºç¾¤ä»»åŠ¡è§„åˆ’ä¸å†³ç­–ç®—æ³•å°¤ä¸ºå…³é”®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„ä»»åŠ¡åˆ†é…æ–¹æ³•ï¼Œä»¥åº”å¯¹åŠ¨æ€æ— äººæœºç¾¤ä»»åŠ¡è§„åˆ’ä¸å†³ç­–é—®é¢˜ã€‚

### 1.1. Related Works

The UAV swarm task planning problem can be formulated as a complex combinatorial optimization problem [1] considering time constraints, task decomposition, and dynamic reallocation, which make it an NP-hard problem. The algorithms for task assignment are generally divided into optimization algorithms, heuristic algorithms, swarm intelligence algorithms, contract network, auction algorithms, and reinforcement learning algorithms.

æ— äººæœºé›†ç¾¤ä»»åŠ¡è§„åˆ’é—®é¢˜å¯è¢«è¡¨è¿°ä¸ºä¸€ä¸ªå¤æ‚çš„ç»„åˆä¼˜åŒ–é—®é¢˜[1]ï¼Œæ¶‰åŠæ—¶é—´çº¦æŸã€ä»»åŠ¡åˆ†è§£åŠåŠ¨æ€é‡åˆ†é…ç­‰å› ç´ ï¼Œä½¿å…¶æˆä¸º NP éš¾é—®é¢˜ã€‚ä»»åŠ¡åˆ†é…ç®—æ³•é€šå¸¸åˆ†ä¸ºä¼˜åŒ–ç®—æ³•ã€å¯å‘å¼ç®—æ³•ã€ç¾¤ä½“æ™ºèƒ½ç®—æ³•ã€åˆåŒç½‘åè®®ã€æ‹å–ç®—æ³•ä»¥åŠå¼ºåŒ–å­¦ä¹ ç®—æ³•ç­‰ç±»åˆ«ã€‚

The optimization algorithm aims to obtain the optimal solution according to the objective function under constraint conditions. Common optimization methods include enumeration algorithms, dynamic programming algorithms [2], integer programming algorithms [3], etc. The enumeration algorithm is the simplest task assignment algorithm and can only be used to solve problems of small size and low complexity. The dynamic programming algorithm is a bottom-up algorithm that establishes several sub-problems from the bottom and solves the whole problem by solving the sub-problems. The integer programming algorithm is the general name of a set of algorithms for solving integer programming problems, and it includes the Hungarian algorithm [4], the branch and bound method, etc.

ä¼˜åŒ–ç®—æ³•æ—¨åœ¨æ ¹æ®çº¦æŸæ¡ä»¶ä¸‹çš„ç›®æ ‡å‡½æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¸¸è§çš„ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬æšä¸¾ç®—æ³•ã€åŠ¨æ€è§„åˆ’ç®—æ³•[2]ã€æ•´æ•°è§„åˆ’ç®—æ³•[3]ç­‰ã€‚æšä¸¾ç®—æ³•æ˜¯æœ€ç®€å•çš„ä»»åŠ¡åˆ†é…ç®—æ³•ï¼Œä»…èƒ½ç”¨äºè§£å†³è§„æ¨¡å°ã€å¤æ‚åº¦ä½çš„é—®é¢˜ã€‚åŠ¨æ€è§„åˆ’ç®—æ³•æ˜¯ä¸€ç§è‡ªåº•å‘ä¸Šçš„ç®—æ³•ï¼Œä»åº•å±‚å»ºç«‹è‹¥å¹²å­é—®é¢˜ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›å­é—®é¢˜æ¥è§£å†³æ•´ä¸ªé—®é¢˜ã€‚æ•´æ•°è§„åˆ’ç®—æ³•æ˜¯è§£å†³æ•´æ•°è§„åˆ’é—®é¢˜çš„ä¸€ç³»åˆ—ç®—æ³•çš„æ€»ç§°ï¼Œå…¶ä¸­åŒ…æ‹¬åŒˆç‰™åˆ©ç®—æ³•[4]ã€åˆ†æ”¯å®šç•Œæ³•ç­‰ã€‚

The heuristic algorithm is an algorithm based on intuition or experience that aims to find feasible solutions to complex problems in a limited time. Common heuristic algorithms include the genetic algorithm [5] (GA), tabu search, simulated annealing [6] (SA), etc. Take GA as an example. GA was proposed by John Holland of the United States in the 1970s. The algorithm simulates genetic evolution in nature to search for the optimal solution. Wu et al. [7] combined the optimization idea of SA to improve the global optimization effect and convergence speed of GA. Martin et al. [8] dynamically adjusted the parameters of the genetic algorithm according to the available computational capacity, thus realizing the trade-off between computation time and accuracy.

å¯å‘å¼ç®—æ³•æ˜¯ä¸€ç§åŸºäºç›´è§‰æˆ–ç»éªŒçš„ç®—æ³•ï¼Œæ—¨åœ¨æœ‰é™æ—¶é—´å†…ä¸ºå¤æ‚é—®é¢˜å¯»æ‰¾å¯è¡Œè§£ã€‚å¸¸è§çš„å¯å‘å¼ç®—æ³•åŒ…æ‹¬é—ä¼ ç®—æ³•[5]ï¼ˆGAï¼‰ã€ç¦å¿Œæœç´¢ã€æ¨¡æ‹Ÿé€€ç«[6]ï¼ˆSAï¼‰ç­‰ã€‚ä»¥ GA ä¸ºä¾‹ï¼Œè¯¥ç®—æ³•ç”±ç¾å›½çš„çº¦ç¿°Â·éœå…°å¾·äº 20 ä¸–çºª 70 å¹´ä»£æå‡ºï¼Œé€šè¿‡æ¨¡æ‹Ÿè‡ªç„¶ç•Œä¸­çš„é—ä¼ è¿›åŒ–æ¥æœç´¢æœ€ä¼˜è§£ã€‚Wu ç­‰äºº[7]ç»“åˆäº† SA çš„ä¼˜åŒ–æ€æƒ³ï¼Œæå‡äº† GA çš„å…¨å±€ä¼˜åŒ–æ•ˆæœå’Œæ”¶æ•›é€Ÿåº¦ã€‚Martin ç­‰äºº[8]åˆ™æ ¹æ®å¯ç”¨çš„è®¡ç®—èƒ½åŠ›åŠ¨æ€è°ƒæ•´é—ä¼ ç®—æ³•çš„å‚æ•°ï¼Œä»è€Œå®ç°äº†è®¡ç®—æ—¶é—´ä¸ç²¾åº¦ä¹‹é—´çš„æƒè¡¡ã€‚

The swarm intelligence algorithm is rooted in the concept of swarm intelligence, which is observed in nature. This algorithm addresses the task-assignment problem by exploring all feasible solutions in the problem space, including popular techniques such as Ant Colony Optimization Algorithm (ACO), Particle Swarm Optimization Algorithm (PSO), Grey Wolf (GW), etc. ACO mimics the foraging behavior of ants to determine an optimal solution [9]. Gao et al. [10] introduced a negative feedback mechanism to hasten the convergence of ACO, which has proved to be advantageous in solving large-scale task-assignment problems. Du et al. [11] devised a role-based approach for the ant colony to prioritize different search strategies, thus enhancing the efficiency of finding the global optimal solution. PSO, a random search algorithm that emulates bird feeding behavior, has been employed to tackle the task assignment problem [12,13]. Chen et al. [14] proposed a guided PSO approach that has been demonstrated to yield optimal task-assignment schemes, thereby improving the cooperative combat capability of multiple UAVs.

ç¾¤ä½“æ™ºèƒ½ç®—æ³•æºäºè‡ªç„¶ç•Œä¸­è§‚å¯Ÿåˆ°çš„ç¾¤ä½“æ™ºèƒ½æ¦‚å¿µã€‚è¯¥ç®—æ³•é€šè¿‡æ¢ç´¢é—®é¢˜ç©ºé—´å†…çš„æ‰€æœ‰å¯è¡Œè§£æ¥è§£å†³ä»»åŠ¡åˆ†é…é—®é¢˜ï¼Œå…¶ä¸­åŒ…æ‹¬èšç¾¤ä¼˜åŒ–ç®—æ³•ï¼ˆACOï¼‰ã€ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ï¼ˆPSOï¼‰ã€ç°ç‹¼ç®—æ³•ï¼ˆGWï¼‰ç­‰æµè¡ŒæŠ€æœ¯ã€‚ACO æ¨¡ä»¿èš‚èšçš„è§…é£Ÿè¡Œä¸ºæ¥ç¡®å®šæœ€ä¼˜è§£[9]ã€‚Gao ç­‰äºº[10]å¼•å…¥äº†è´Ÿåé¦ˆæœºåˆ¶ä»¥åŠ é€Ÿ ACO çš„æ”¶æ•›ï¼Œè¿™å·²è¢«è¯æ˜åœ¨è§£å†³å¤§è§„æ¨¡ä»»åŠ¡åˆ†é…é—®é¢˜ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚Du ç­‰äºº[11]è®¾è®¡äº†ä¸€ç§åŸºäºè§’è‰²çš„æ–¹æ³•ï¼Œä½¿èšç¾¤èƒ½å¤Ÿä¼˜å…ˆè€ƒè™‘ä¸åŒçš„æœç´¢ç­–ç•¥ï¼Œä»è€Œæé«˜äº†å¯»æ‰¾å…¨å±€æœ€ä¼˜è§£çš„æ•ˆç‡ã€‚PSO æ˜¯ä¸€ç§æ¨¡æ‹Ÿé¸Ÿç±»è§…é£Ÿè¡Œä¸ºçš„éšæœºæœç´¢ç®—æ³•ï¼Œå·²è¢«ç”¨äºè§£å†³ä»»åŠ¡åˆ†é…é—®é¢˜[12, 13]ã€‚Chen ç­‰äºº[14]æå‡ºäº†ä¸€ç§å¼•å¯¼å¼ PSO æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å·²è¢«è¯æ˜èƒ½å¤Ÿäº§ç”Ÿæœ€ä¼˜çš„ä»»åŠ¡åˆ†é…æ–¹æ¡ˆï¼Œä»è€Œæé«˜äº†å¤šæ— äººæœºååŒä½œæˆ˜èƒ½åŠ›ã€‚

The contract network algorithm has been proposed to solve the workload balancing problem among unmanned aerial vehicles (UAVs) through a â€œbidding winningâ€ mechanism [15]. Chen [16] presented a distributed contract network-based task assignment method to solve the communication-delay-constrained task assignment problem in multiple UAVs. The auction algorithm [17] mimics the human auction process to optimize the benefits of the UAV swarm system [18]. Liao [19] proposed a dynamic target-assignment algorithm using multi-agent distributed collaborative auctioning. Li et al. [20] introduced a result-updating mechanism where new and old tasks are reauctioned together, resulting in the most beneficial replanning results while satisfying real-time requirements. The effectiveness of this algorithm was demonstrated by the experimental results.

åˆåŒç½‘ç»œç®—æ³•è¢«æå‡ºï¼Œæ—¨åœ¨é€šè¿‡â€œç«æ ‡è·èƒœâ€æœºåˆ¶è§£å†³æ— äººæœºï¼ˆUAVï¼‰é—´çš„å·¥ä½œè´Ÿè½½å¹³è¡¡é—®é¢˜[15]ã€‚Chen[16]æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒå¼åˆåŒç½‘ç»œçš„ä»»åŠ¡åˆ†é…æ–¹æ³•ï¼Œä»¥è§£å†³å¤šæ— äººæœºç³»ç»Ÿä¸­é€šä¿¡å»¶è¿Ÿå—é™çš„ä»»åŠ¡åˆ†é…é—®é¢˜ã€‚æ‹å–ç®—æ³•[17]æ¨¡ä»¿äººç±»æ‹å–è¿‡ç¨‹ï¼Œä»¥ä¼˜åŒ–æ— äººæœºç¾¤ç³»ç»Ÿçš„æ•ˆç›Š[18]ã€‚Liao[19]æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šæ™ºèƒ½ä½“åˆ†å¸ƒå¼åä½œæ‹å–çš„åŠ¨æ€ç›®æ ‡åˆ†é…ç®—æ³•ã€‚Li ç­‰äºº[20]å¼•å…¥äº†ä¸€ç§ç»“æœæ›´æ–°æœºåˆ¶ï¼Œå…¶ä¸­æ–°æ—§ä»»åŠ¡ä¸€èµ·é‡æ–°æ‹å–ï¼Œä»è€Œåœ¨æ»¡è¶³å®æ—¶è¦æ±‚çš„åŒæ—¶ï¼Œè·å¾—æœ€æœ‰åˆ©çš„é‡æ–°è§„åˆ’ç»“æœã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚

In recent years, deep RL (DRL), which combines RL and deep learning (DL), has emerged as an important research area in UAV control and decision-making. DRL alleviates the dimension explosion problem that easily occurs in traditional RL and has made great breakthroughs in areas such as robot control [21,22,23], scheduling optimization [24,25], and multi-agent collaboration [26,27,28,29]. Ma [30] proposed a task-assignment algorithm based on Deep Q Network (DQN) to support UAV swarm operations, which significantly improves the success rate of UAV swarm combat. Huang [31] combines DQN with an evolutionary algorithm to optimize task-assignment results of traditional algorithms and can obtain assignment results dynamically.

è¿‘å¹´æ¥ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰å·²æˆä¸ºæ— äººæœºï¼ˆUAVï¼‰æ§åˆ¶ä¸å†³ç­–é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚DRL æœ‰æ•ˆç¼“è§£äº†ä¼ ç»Ÿ RL ä¸­æ˜“å‡ºç°çš„ç»´åº¦çˆ†ç‚¸é—®é¢˜ï¼Œå¹¶åœ¨æœºå™¨äººæ§åˆ¶[21, 22, 23]ã€è°ƒåº¦ä¼˜åŒ–[24, 25]åŠå¤šæ™ºèƒ½ä½“åä½œ[26, 27, 28, 29]ç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚é©¬[30]æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰çš„ä»»åŠ¡åˆ†é…ç®—æ³•ï¼Œä»¥æ”¯æŒæ— äººæœºé›†ç¾¤ä½œæˆ˜ï¼Œæ˜¾è‘—æå‡äº†æ— äººæœºé›†ç¾¤ä½œæˆ˜çš„æˆåŠŸç‡ã€‚é»„[31]åˆ™å°† DQN ä¸è¿›åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œä¼˜åŒ–äº†ä¼ ç»Ÿç®—æ³•çš„ä»»åŠ¡åˆ†é…ç»“æœï¼Œå¹¶èƒ½åŠ¨æ€è·å–åˆ†é…æ–¹æ¡ˆã€‚

In short, heuristic and swarm intelligence algorithms solve problems faster but produce suboptimal solutions that have poor scalability and flexibility. The improved distributed algorithms can only handle specific problems. Contract network and auction algorithms have high robustness and scalability, but both rely heavily on communication and computing capacities with poor flexibility. As for DRL, as the group size increases, there are problems such as spatial linkage of action states, dimensional explosion, and difficulty in determining the reward function.

ç®€è€Œè¨€ä¹‹ï¼Œå¯å‘å¼å’Œç¾¤ä½“æ™ºèƒ½ç®—æ³•è™½èƒ½æ›´å¿«åœ°è§£å†³é—®é¢˜ï¼Œä½†äº§ç”Ÿçš„è§£å†³æ–¹æ¡ˆå¾€å¾€æ¬¡ä¼˜ï¼Œä¸”å¯æ‰©å±•æ€§å’Œçµæ´»æ€§è¾ƒå·®ã€‚æ”¹è¿›åçš„åˆ†å¸ƒå¼ç®—æ³•ä»…èƒ½å¤„ç†ç‰¹å®šé—®é¢˜ã€‚åˆåŒç½‘å’Œæ‹å–ç®—æ³•è™½å…·æœ‰è¾ƒé«˜çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä½†ä¸¤è€…å‡é«˜åº¦ä¾èµ–é€šä¿¡ä¸è®¡ç®—èƒ½åŠ›ï¼Œçµæ´»æ€§æ¬ ä½³ã€‚è‡³äºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ï¼Œéšç€ç¾¤ä½“è§„æ¨¡å¢å¤§ï¼Œä¼šå‡ºç°åŠ¨ä½œçŠ¶æ€ç©ºé—´å…³è”ã€ç»´åº¦çˆ†ç‚¸åŠå¥–åŠ±å‡½æ•°éš¾ä»¥ç¡®å®šç­‰é—®é¢˜ã€‚

### 1.2. Contribution

According to the aforementioned investigations, this paper presents Ex-MADDPG to solve dynamic task assignment problems for UAV swarms. The main distinguishing advantages of our algorithm are easy training, good scalability, excellent assignment performance, and real-time decision-making. We summarize the main contributions as follows.

åŸºäºä¸Šè¿°ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº† Ex-MADDPG ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³æ— äººæœºç¾¤åŠ¨æ€ä»»åŠ¡åˆ†é…é—®é¢˜ã€‚è¯¥ç®—æ³•çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ˜“äºè®­ç»ƒã€è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€å“è¶Šçš„ä»»åŠ¡åˆ†é…æ€§èƒ½ä»¥åŠå®æ—¶å†³ç­–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ã€‚

- (1) We construct an extensible framework with local communication, a mean simulation observation model, and a synchronization parameter training mechanism to meet the scalability capability so that the strategies from small-scale system training can be directly applied to large-scale swarms.
- æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…·æœ‰æœ¬åœ°é€šä¿¡ã€å‡å€¼ä»¿çœŸè§‚æµ‹æ¨¡å‹å’ŒåŒæ­¥å‚æ•°è®­ç»ƒæœºåˆ¶çš„å¯æ‰©å±•æ¡†æ¶ï¼Œä»¥æ»¡è¶³å¯æ‰©å±•æ€§éœ€æ±‚ï¼Œä½¿å¾—ä»å°è§„æ¨¡ç³»ç»Ÿè®­ç»ƒä¸­è·å¾—çš„ç­–ç•¥èƒ½å¤Ÿç›´æ¥åº”ç”¨äºå¤§è§„æ¨¡ç¾¤ä½“ã€‚
- (2) Due to the poor assignment performance of the traditional DRL algorithm with increasing system scale, a multiple-decision mechanism is proposed to ensure the assignment performance of a large UAV swarm to perform complex and diverse tasks.
- é‰´äºä¼ ç»Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ç®—æ³•åœ¨ç³»ç»Ÿè§„æ¨¡å¢å¤§æ—¶åˆ†é…æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå†³ç­–æœºåˆ¶ï¼Œä»¥ç¡®ä¿å¤§è§„æ¨¡æ— äººæœºç¾¤åœ¨æ‰§è¡Œå¤æ‚å¤šæ ·ä»»åŠ¡æ—¶çš„åˆ†é…æ€§èƒ½ã€‚
- (3) The practicality and effectiveness of the proposed Ex-MADDPG algorithm have been verified through simulation experiments carried out on the MPE simulation platform and a real-world experiment with nine drones and three targets. The results demonstrate that the proposed algorithm outperforms traditional task-assignment algorithms in various performance indicators, including task completion rate, task loss, number of communications, and decision time.
- æ‰€æå‡ºçš„ Ex-MADDPG ç®—æ³•çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§å·²é€šè¿‡åœ¨ MPE ä»¿çœŸå¹³å°ä¸Šè¿›è¡Œçš„ä»¿çœŸå®éªŒåŠä¸€é¡¹æ¶‰åŠä¹æ¶æ— äººæœºä¸ä¸‰ä¸ªç›®æ ‡çš„çœŸå®ä¸–ç•Œå®éªŒå¾—åˆ°éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨ä»»åŠ¡å®Œæˆç‡ã€ä»»åŠ¡æŸå¤±ã€é€šä¿¡æ¬¡æ•°åŠå†³ç­–æ—¶é—´ç­‰å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿä»»åŠ¡åˆ†é…ç®—æ³•ã€‚

The paper is structured as follows. First, we describe the basic theory and background of DRL in Section 2. Then, the proposed method for the dynamic extensible task assignment problem is detailed in Section 3. Afterward, the efficiency of the proposed algorithm is verified in Section 4 and Section 5. Finally, we draw the conclusion and outline possible future directions in Section 6.

æœ¬æ–‡ç»“æ„å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œåœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬é˜è¿°äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„åŸºç¡€ç†è®ºä¸èƒŒæ™¯ã€‚æ¥ç€ï¼Œç¬¬ä¸‰éƒ¨åˆ†è¯¦ç»†ä»‹ç»äº†é’ˆå¯¹åŠ¨æ€å¯æ‰©å±•ä»»åŠ¡åˆ†é…é—®é¢˜æ‰€æå‡ºçš„æ–¹æ³•ã€‚éšåï¼Œç¬¬å››å’Œç¬¬äº”éƒ¨åˆ†éªŒè¯äº†æ‰€æç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œç¬¬å…­éƒ¨åˆ†å¾—å‡ºç»“è®ºå¹¶æ¦‚è¿°äº†æœªæ¥å¯èƒ½çš„ç ”ç©¶æ–¹å‘ã€‚

## 2 Deep Reinforcement Learning Background

RL is a machine learning method for learning â€œwhat to doâ€ to maximize utility. The agent must learn an optimal policy in the current situation through trial and error. Most RL algorithms are based on Markov Decision Processes (MDPs) for theoretical modeling, derivation, and demonstration. Figure 1 shows the interaction process between the agent and the environment in MDPs.

RL æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå­¦ä¹  â€œåšä»€ä¹ˆâ€ ä»¥æœ€å¤§é™åº¦åœ°æé«˜æ•ˆç”¨ã€‚ä»£ç†äººå¿…é¡»é€šè¿‡åå¤è¯•éªŒæ¥å­¦ä¹ å½“å‰æƒ…å†µä¸‹çš„æœ€ä½³ç­–ç•¥ã€‚å¤§å¤šæ•° RL ç®—æ³•éƒ½åŸºäºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ ï¼ˆMDPï¼‰ è¿›è¡Œç†è®ºå»ºæ¨¡ã€æ¨å¯¼å’Œæ¼”ç¤ºã€‚å›¾ 1 æ˜¾ç¤ºäº† MDP ä¸­ä»£ç†ä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº¤äº’è¿‡ç¨‹ã€‚

![Figure 1. Interaction Process between Agent and Environment.](./images/2023.04-Drones-DRL4SwarmTA/Fig1-MDP-Iteration.png)

In MDPs, the decision-making process is described as the following quads: $(S, A, P, R)$ where $S$ is the state set, $A$ is the action set, $S \times S \times A \rightarrow [0,1]$, $R$ is the expected rewards of state-action, $S \times A \rightarrow R$, and $P$ is the state transition function.

åœ¨ MDPsï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰ä¸­ï¼Œå†³ç­–è¿‡ç¨‹è¢«æè¿°ä¸ºä»¥ä¸‹å››å…ƒç»„ï¼š$(S, A, P, R)$ï¼Œå…¶ä¸­ $S$ æ˜¯çŠ¶æ€é›†åˆï¼Œ$A$ æ˜¯åŠ¨ä½œé›†åˆï¼Œ$S \times S \times A \rightarrow [0,1]$ï¼Œ$R$ æ˜¯çŠ¶æ€-åŠ¨ä½œçš„æœŸæœ›å¥–åŠ±ï¼Œ$S \times A \rightarrow R$ï¼Œè€Œ $P$ æ˜¯çŠ¶æ€è½¬ç§»å‡½æ•°ã€‚

$$
\begin{aligned}
p\left(s^{\prime} \mid s, a\right) & = \operatorname{Pr}\left\{S_{t} \mid S_{t-1}, A_{t-1}\right\} \\
& = \sum_{r \in R} P\left(s^{\prime}, r \mid s, a\right)
\end{aligned}
$$

In a UAV swarm system, a UAV observes a certain state $S_{t} \in S$ of the environment, then chooses an action $A_{t} \in A$ according to that state. In the next moment, according to the different selected action, the UAV will obtain a reward $R_{t+1} \in R$ from the state transfer function $\boldsymbol{P}$ and enters a new state $S_{t+1}$. This process can be repeated to obtain the following Markov decision sequence:

åœ¨æ— äººæœºé›†ç¾¤ç³»ç»Ÿä¸­ï¼Œæ— äººæœºè§‚å¯Ÿç¯å¢ƒçš„æŸä¸ªçŠ¶æ€ $S_{t} \in S$ï¼Œç„¶åæ ¹æ®è¯¥çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ $A_{t} \in A$ã€‚åœ¨ä¸‹ä¸€æ—¶åˆ»ï¼Œæ ¹æ®æ‰€é€‰åŠ¨ä½œçš„ä¸åŒï¼Œæ— äººæœºä¼šä»çŠ¶æ€è½¬ç§»å‡½æ•° $\boldsymbol{P}$ ä¸­è·å¾—ä¸€ä¸ªå¥–åŠ± $R_{t+1} \in R$ï¼Œå¹¶è¿›å…¥ä¸€ä¸ªæ–°çš„çŠ¶æ€ $S_{t+1}$ã€‚è¿™ä¸€è¿‡ç¨‹å¯ä»¥é‡å¤è¿›è¡Œï¼Œä»è€Œå¾—åˆ°å¦‚ä¸‹çš„é©¬å°”å¯å¤«å†³ç­–åºåˆ—ï¼š

$$
S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, S_{3}, \cdots
$$

In 1989, Watkins [[32](https://www.mdpi.com/2504-446X/7/5/297#B32-drones-07-00297)] proposed the Q-learning algorithm by combining the time series difference learning method and optimal control, which is a great milestone for RL. In 2013, Mnih [[33](https://www.mdpi.com/2504-446X/7/5/297#B33-drones-07-00297)] proposed DQN by combining RL and DL and achieved the top level of human performance in a classic Atari game. In the DRL algorithm, Lillicrap [[34](https://www.mdpi.com/2504-446X/7/5/297#B34-drones-07-00297)] proposed a new algorithm that combines the DQN and Policy Gradient (PG) algorithm named Deep Deterministic Policy Gradient (DDPG) to solve the control problem in continuous action space effectively. The framework of DDPG is shown in[Figure 2](https://www.mdpi.com/2504-446X/7/5/297#fig_body_display_drones-07-00297-f002). DDPG samples the distribution of actions by improving the policyğÎ¼to obtain the specific actionA. At this point, the reward functionğ‘…(ğ‘ ,ğ‘)R(s,a)is determined. The deterministic policy isğğœƒ:ğ‘ºâ†’ğ‘¨Î¼Î¸:Sâ†’A, and its maximum objective function is:

1989 å¹´ï¼ŒWatkins [ 32] æå‡ºäº† Q-learning ç®—æ³•ï¼Œå°†æ—¶é—´åºåˆ—å·®åˆ†å­¦ä¹ æ–¹æ³•ä¸æœ€ä¼˜æ§åˆ¶ç›¸ç»“åˆï¼Œè¿™æ˜¯ RL çš„ä¸€ä¸ªä¼Ÿå¤§é‡Œç¨‹ç¢‘ã€‚2013 å¹´ï¼ŒMnih [ 33] æå‡ºäº† DQNï¼Œå°† RL å’Œ DL ç›¸ç»“åˆï¼Œå¹¶åœ¨ç»å…¸çš„ Atari æ¸¸æˆä¸­å®ç°äº†äººç±»è¡¨æ¼”çš„é¡¶çº§æ°´å¹³ã€‚åœ¨ DRL ç®—æ³•ä¸­ï¼ŒLillicrap [ 34] æå‡ºäº†ä¸€ç§ç»“åˆäº† DQN å’Œç­–ç•¥æ¢¯åº¦ ï¼ˆPGï¼‰ ç®—æ³•çš„æ–°ç®—æ³•ï¼Œç§°ä¸ºæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ ï¼ˆDDPGï¼‰ï¼Œä»¥æœ‰æ•ˆè§£å†³è¿ç»­åŠ¨ä½œç©ºé—´ä¸­çš„æ§åˆ¶é—®é¢˜ã€‚DDPG çš„æ¡†æ¶å¦‚å›¾ 2 æ‰€ç¤ºã€‚DDPG é€šè¿‡æ”¹è¿›ç­–ç•¥ ğÎ¼ æ¥è·å–ç‰¹å®šçš„æ“ä½œ Aï¼Œä»è€Œå¯¹æ“ä½œçš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ã€‚æ­¤æ—¶ï¼Œå¥–åŠ±å‡½æ•° ğ‘…(ğ‘ ,ğ‘)R(s,a)å·²ç¡®å®šã€‚ç¡®å®šæ€§ç­–ç•¥ä¸º ğğœƒ:ğ‘ºâ†’ğ‘¨Î¼Î¸:Sâ†’Aï¼Œå…¶æœ€å¤§ç›®æ ‡å‡½æ•°ä¸ºï¼š

$$
J(\theta)=\mathbb{E}_{s \sim p^{\mu}}[R(s, a)]
$$

The corresponding gradient is:

$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim D}\left[\left.\nabla_{\theta} \boldsymbol{\mu}_{\theta}(a \mid s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)}\right]
$$

Equation (5) depends on $\nabla_{a} Q^{\mu}(s, a)$, and the action space of the DDPG algorithm must be continuous.

å…¬å¼ (5) ä¾èµ–äº $\nabla_{a} Q^{\mu}(s, a)$ï¼Œä¸” DDPG ç®—æ³•çš„åŠ¨ä½œç©ºé—´å¿…é¡»æ˜¯è¿ç»­çš„ã€‚

The critic network is updated as follows:

$$
\begin{aligned}
L(\theta) & = \mathbb{E}_{s, a, r, s^{\prime}}\left[\left(Q^{\mu}(s, a) - y\right)^{2}\right] \\
y & = r + \left.\gamma Q^{\mu^{\prime}}\left(s^{\prime}, a^{\prime}\right)\right|_{a^{\prime}=\mu_{\theta^{\prime}}^{\prime}\left(s^{\prime}\right)}
\end{aligned}
$$

where $\mu$ is the actor prediction network and $\mu^{\prime}$ is the target network. The gradient of the objective function of the actor network is:

å…¶ä¸­ï¼Œ$\mu$ æ˜¯æ¼”å‘˜é¢„æµ‹ç½‘ç»œï¼Œ$\mu^{\prime}$ æ˜¯ç›®æ ‡ç½‘ç»œã€‚æ¼”å‘˜ç½‘ç»œç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ä¸ºï¼š

$$
\nabla_{\theta} J(\boldsymbol{\mu}) = \mathbb{E}_{s, a \sim D}\left[\left.\nabla_{\theta} \boldsymbol{\mu}(a \mid s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu(s)}\right]
$$

The loss function is:

$$
L(\mu) = -Q^{\mu}(s, a)
$$

![Figure 2. DDPG Algorithm Framework.](./images/2023.04-Drones-DRL4SwarmTA/Fig2-DDPG-Algorithm-Framework.png)

...

Unlike the situation of single-agent training, the change in environment is not only related to the actions of the agent itself, but also related to the actions of other agents and the interaction between agents, which leads to the instability of multi-agent training. Based on the DDPG algorithm, Lowe [35] extended it to the case of multiple agents and proposed the MADDPG algorithm. It adopted a centralized training and distributed execution framework, as shown in Figure 3. It uses global information to guide the training of multi-agent, while in the process of execution, the agent will only make decisions based on its own observations. It greatly improves the convergence speed and training effect of the algorithm.

ä¸å•æ™ºèƒ½ä½“è®­ç»ƒçš„æƒ…å†µä¸åŒï¼Œç¯å¢ƒçš„å˜åŒ–ä¸ä»…ä¸æ™ºèƒ½ä½“æœ¬èº«çš„åŠ¨ä½œæœ‰å…³ï¼Œè¿˜ä¸å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œå’Œæ™ºèƒ½ä½“ä¹‹é—´çš„äº’åŠ¨æœ‰å…³ï¼Œè¿™å¯¼è‡´äº†å¤šæ™ºèƒ½ä½“è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚åœ¨ DDPG ç®—æ³•çš„åŸºç¡€ä¸Šï¼ŒLowe [35] å°†å…¶æ‰©å±•åˆ°å¤šä¸ªæ™ºèƒ½ä½“çš„æƒ…å†µï¼Œå¹¶æå‡ºäº† MADDPG ç®—æ³•ã€‚å®ƒé‡‡ç”¨äº†é›†ä¸­å¼è®­ç»ƒå’Œåˆ†å¸ƒå¼æ‰§è¡Œæ¡†æ¶ï¼Œå¦‚å›¾ 3 æ‰€ç¤ºã€‚å®ƒä½¿ç”¨å…¨å±€ä¿¡æ¯æ¥æŒ‡å¯¼å¤šæ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œè€Œåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“åªä¼šæ ¹æ®è‡ªå·±çš„è§‚å¯Ÿåšå‡ºå†³ç­–ã€‚å¤§å¤§æé«˜äº†ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œè®­ç»ƒæ•ˆæœã€‚

![Figure 3. Centralized Training Distributed Execution Framework.](./images/2023.04-Drones-DRL4SwarmTA/Fig3-Centralized-Training-Distributed-Execution-Framework.png)

Considering $N$ agents, each agent's policy parameters can be written in the following form:

$$
\boldsymbol{\theta} = \left\{ \theta_{1}, \theta_{2}, \theta_{3}, \cdots, \theta_{N} \right\}
$$

Its policy is:

$$
\boldsymbol{\pi} = \left\{ \pi_{1}, \pi_{2}, \pi_{3}, \ldots, \pi_{N} \right\}
$$

According to the PG algorithm, the gradient of the expected return $J\left(\theta_{i}\right) = \mathbb{E}\left[R_{i}\right]$ for agent $i$ can be obtained:

$$
\begin{aligned}
\nabla_{\theta_{i}} J\left(\theta_{i}\right) & = \mathbb{E}_{s \sim p^{u}, a_{i} \sim \pi_{i}} \left[ \mathcal{L}\left(\theta_{i}\right) \right] \\
\mathcal{L}\left(\theta_{i}\right) & = \nabla_{\theta_{i}} \log \pi_{\theta_{i}} \left( a_{i} \mid o_{i} \right) Q_{i}^{\pi} \left( x, a_{1}, \cdots, a_{N} \right)
\end{aligned}
$$

$Q_{i}^{\pi}\left(x, a_{1}, \cdots, a_{N}\right)$ is a centralized action-value function. The input consists of the state $x$, and the action $a_{1}, \cdots, a_{N}$ of all agents. The state $x$ can be simply composed of the observations of all agents. Each agent can design a reward function independently and learn independently to achieve competitive, cooperative, or hybrid policies.

Similar to DDPG, its policies are:

$$
\boldsymbol{\mu} = \left\{ \mu_{1}, \mu_{2}, \mu_{3}, \ldots, \mu_{N} \right\}
$$

The parameters of the policies are:

$$
\boldsymbol{\theta} = \left\{ \theta_{1}, \theta_{2}, \theta_{3}, \cdots, \theta_{N} \right\}
$$

The gradient of the objective function is:

$$
\nabla_{\theta_{i}} J\left(\boldsymbol{\mu}_{i}\right) = \left[ \left. \mathbb{E}_{x, a \sim D} \nabla_{\theta_{i}} \boldsymbol{\mu}_{i} \left( a_{i} \mid o_{i} \right) \nabla_{a_{i}} Q_{i}^{\mu} \left( x, a_{1}, \ldots, a_{N} \right) \right|_{a_{i} = \boldsymbol{\mu}_{i} \left( o_{i} \right)} \right]
$$

The experience replay buffer $D$ contains $\left(x, x^{\prime}, a_{1}, \cdots, a_{N}, r_{1}, \cdots, r_{N}\right)$, which records the experience of all agents. The loss function of the actor network is:

$$
L\left(\boldsymbol{\mu}_{i}\right) = -Q_{i}^{\mu}\left(x, a_{1}, \cdots, a_{N}\right)
$$

Accordingly, the critical network $Q_{i}^{\mu}$ is updated as follows:

$$
\begin{aligned}
L\left(\theta_{i}\right) & = \mathbb{E}_{x, a, r, x^{\prime}} \left[ \left( Q_{i}^{\mu} \left( x, a_{1}, \ldots, a_{N} \right) - y \right)^{2} \right] \\
y & = r_{i} + \left. \gamma Q_{i}^{\mu^{\prime}} \left( x^{\prime}, a_{1}^{\prime}, \ldots, a_{N}^{\prime} \right) \right|_{a_{j}^{\prime} = \mu_{j}^{\prime} \left( o_{j} \right)}
\end{aligned}
$$

where $\boldsymbol{\mu}^{\prime} = \left\{ \mu_{\theta_{1}^{\prime}} \ldots, \boldsymbol{\mu}_{\theta_{N}^{\prime}} \right\}$ is the policy target network and $\theta_{i}^{\prime}$ is the parameter of network $i$.

The MADDPG algorithm provides a common centralized training and distributed execution framework in multi-agent systems, as shown in Figure 3. However, the input dimension of the critical network will expand rapidly with the increase in the number of agents. Therefore, MADDPG cannot be applied to large-scale agent scenarios directly. Meanwhile, MADDPG may fail when the training and application scenarios are different. Based on the above discussion, this paper will solve the problems with the MADDPG algorithm and propose an extensible UAV swarm task assignment algorithm.

MADDPG ç®—æ³•åœ¨å¤šä»£ç†ç³»ç»Ÿä¸­æä¾›äº†ä¸€ä¸ªé€šç”¨çš„é›†ä¸­å¼è®­ç»ƒå’Œåˆ†å¸ƒå¼æ‰§è¡Œæ¡†æ¶ï¼Œå¦‚å›¾ 3 æ‰€ç¤ºã€‚ä½†æ˜¯ï¼Œå…³é”®ç½‘ç»œçš„è¾“å…¥ç»´åº¦å°†éšç€ä»£ç†æ•°é‡çš„å¢åŠ è€Œè¿…é€Ÿæ‰©å±•ã€‚å› æ­¤ï¼ŒMADDPG ä¸èƒ½ç›´æ¥åº”ç”¨äºå¤§è§„æ¨¡ä»£ç†æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œå½“è®­ç»ƒåœºæ™¯å’Œåº”ç”¨åœºæ™¯ä¸åŒæ—¶ï¼ŒMADDPG å¯èƒ½ä¼šå¤±è´¥ã€‚åŸºäºä»¥ä¸Šè®¨è®ºï¼Œæœ¬æ–‡å°†è§£å†³ MADDPG ç®—æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æå‡ºä¸€ç§å¯æ‰©å±•çš„æ— äººæœºé›†ç¾¤ä»»åŠ¡åˆ†é…ç®—æ³•ã€‚

## 3 Extensible Task Assignment Algorithm of UAV Swarm

This section designs a scalable UAV swarm task assignment algorithm based on the following scenarios, which is trained on a small number of agents but can be directly applied to a larger UAV swarm system with guaranteed task assignment performance.

æœ¬èŠ‚åŸºäºä»¥ä¸‹åœºæ™¯è®¾è®¡äº†ä¸€ç§å¯æ‰©å±•çš„æ— äººæœºé›†ç¾¤ä»»åŠ¡åˆ†é…ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨å°‘é‡æ™ºèƒ½ä½“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å¯ä»¥ç›´æ¥åº”ç”¨äºæ›´å¤§çš„æ— äººæœºé›†ç¾¤ç³»ç»Ÿï¼Œå¹¶ä¿è¯ä»»åŠ¡åˆ†é…æ€§èƒ½ã€‚

- (1)The UAV swarm searches for an unknown number of targets in a given area, using the Boids [36] algorithms to avoid obstacles during exploration.
  - æ— äººæœºé›†ç¾¤åœ¨ç»™å®šåŒºåŸŸæœç´¢æœªçŸ¥æ•°é‡çš„ç›®æ ‡ï¼Œä½¿ç”¨ Boids [ 36] ç®—æ³•åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­é¿å¼€éšœç¢ç‰©ã€‚
- (2)The UAV is the ammunition to attack the detected target.
  - æ— äººæœºæ˜¯æ”»å‡»æ£€æµ‹åˆ°çš„ç›®æ ‡çš„å¼¹è¯ã€‚
- (3)Each target needs multiple UAVs to destroy.
  - æ¯ä¸ªç›®æ ‡éƒ½éœ€è¦å¤šæ¶æ— äººæœºæ¥æ‘§æ¯ã€‚

In this section, we design a local communication model and a mean simulation observation model to reduce the computational burden of the basic MADDPG algorithm. Meanwhile, we propose a parameter synchronization training mechanism, which guarantees that the training network can be used in more UAVs directly. To ensure the performance, this paper proposes a multi-task assignment decision process. The system framework of the proposed Ex-MADDPG algorithm is shown in Figure 4, where letters Aâ€“F indicate the UAV swarm and the stars indicate the targets.

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå±€éƒ¨é€šä¿¡æ¨¡å‹å’Œä¸€ä¸ªå‡å€¼æ¨¡æ‹Ÿè§‚å¯Ÿæ¨¡å‹ï¼Œä»¥å‡è½»åŸºæœ¬ MADDPG ç®—æ³•çš„è®¡ç®—è´Ÿæ‹…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°åŒæ­¥è®­ç»ƒæœºåˆ¶ï¼Œä¿è¯äº†è®­ç»ƒç½‘ç»œå¯ä»¥ç›´æ¥ç”¨äºæ›´å¤šçš„æ— äººæœºã€‚ä¸ºäº†ä¿è¯æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡åˆ†é…å†³ç­–è¿‡ç¨‹ã€‚æ‰€æå‡ºçš„ Ex-MADDPG ç®—æ³•çš„ç³»ç»Ÿæ¡†æ¶å¦‚å›¾ 4 æ‰€ç¤ºï¼Œå…¶ä¸­å­—æ¯ A-F è¡¨ç¤ºæ— äººæœºé›†ç¾¤ï¼Œæ˜Ÿæ˜Ÿè¡¨ç¤ºç›®æ ‡ã€‚

![Figure 4. Extensible Task-Assignment Algorithm System Framework.](./images/2023.04-Drones-DRL4SwarmTA/Fig4-Extensible-Task-Assignment-Algorithm-System-Framework.png)

### 3.1 Local Communication Model

å‡å€¼æ¨¡æ‹Ÿè§‚å¯Ÿæ¨¡å‹

To ensure that the algorithm can be used in a large-scale UAV swarm system, this paper assumes that each agent can only receive partial information from its neighboring agents. The local communication model followed by each agent is designed as shown in Equation (17):

ä¸ºäº†ç¡®ä¿è¯¥ç®—æ³•å¯ä»¥åœ¨å¤§è§„æ¨¡æ— äººæœºé›†ç¾¤ç³»ç»Ÿä¸­ä½¿ç”¨ï¼Œæœ¬æ–‡å‡è®¾æ¯ä¸ªä»£ç†åªèƒ½ä»å…¶ç›¸é‚»ä»£ç†æ¥æ”¶éƒ¨åˆ†ä¿¡æ¯ã€‚æ¯ä¸ªä»£ç†éµå¾ªçš„æœ¬åœ°é€šä¿¡æ¨¡å‹è®¾è®¡å¦‚å…¬å¼ ï¼ˆ 17ï¼‰ æ‰€ç¤ºï¼š

$$
c_{i}^{\text {out }}=\left[a_{t}, \operatorname{pos}_{i}, \operatorname{pos}_{i, \text { goal }}\right]
$$

$$
c_{i}^{\text {in }}=\left\{\begin{array}{ll}c_{j}^{\text {out }} & \text { agent }_{j} \text { in } C_{\text {agent }}^{i} \\ 0 & \text { agent }_{j} \text { not in } C_{\text {agent }}^{i}\end{array}\right.
$$

![Figure 5. Schematic Diagram of Communication Mode.](./images/2023.04-Drones-DRL4SwarmTA/Fig5-Schematic-Diagram-of-Communication-Mode.png)

where $c_{i}^{\text{out}}$ is the communication message sent from the agent $i$; $a_{t}$ is a bool variable, indicating whether the agent $i$ is in the ready attack state. $pos_{i}$ is the position of the agent $i$; $\operatorname{pos}_{i, \text{goal}}$ is target positions found by the current agent $i$; $c_{j}^{\text{out}}$ is the release information of agent $j$; $c_{i}^{in}$ is the information received by the agent $i$; $C_{a_{\text{agent}}}$ refers to the communication range of agent $i$. An example of the local communication model is shown in Figure 5. The agent only receives messages within its communication range, such as $A$ and $B$. If two agents are not within each other's communication range, such as $B$ and $C$ in Figure 5, they will not communicate.

å…¶ä¸­ï¼Œ$c_{i}^{\text{out}}$ æ˜¯ä»æ™ºèƒ½ä½“ $i$ å‘é€çš„é€šä¿¡æ¶ˆæ¯ï¼›$a_{t}$ æ˜¯ä¸€ä¸ªå¸ƒå°”å˜é‡ï¼Œè¡¨ç¤ºæ™ºèƒ½ä½“ $i$ æ˜¯å¦å¤„äºå‡†å¤‡æ”»å‡»çŠ¶æ€ã€‚$pos_{i}$ æ˜¯æ™ºèƒ½ä½“ $i$ çš„ä½ç½®ï¼›$\operatorname{pos}_{i, \text{goal}}$ æ˜¯å½“å‰æ™ºèƒ½ä½“ $i$ æ‰¾åˆ°çš„ç›®æ ‡ä½ç½®ï¼›$c_{j}^{\text{out}}$ æ˜¯æ™ºèƒ½ä½“ $j$ çš„å‘å¸ƒä¿¡æ¯ï¼›$c_{i}^{in}$ æ˜¯æ™ºèƒ½ä½“ $i$ æ¥æ”¶åˆ°çš„ä¿¡æ¯ï¼›$C_{a_{\text{agent}}}$ è¡¨ç¤ºæ™ºèƒ½ä½“ $i$ çš„é€šä¿¡èŒƒå›´ã€‚å›¾ 5 å±•ç¤ºäº†ä¸€ä¸ªå±€éƒ¨é€šä¿¡æ¨¡å‹çš„ç¤ºä¾‹ã€‚æ™ºèƒ½ä½“ä»…æ¥æ”¶å…¶é€šä¿¡èŒƒå›´å†…çš„æ¶ˆæ¯ï¼Œä¾‹å¦‚ A å’Œ Bã€‚å¦‚æœä¸¤ä¸ªæ™ºèƒ½ä½“ä¸åœ¨å½¼æ­¤çš„é€šä¿¡èŒƒå›´å†…ï¼Œä¾‹å¦‚å›¾ 5 ä¸­çš„ B å’Œ Cï¼Œå®ƒä»¬å°†ä¸ä¼šè¿›è¡Œé€šä¿¡ã€‚

### 3.2 Mean Simulation Observation Model

Aiming at the problem that the observation dimension changes with the scale of the UAV swarm, which leads to the failure of the DRL algorithm, this paper proposes fixed-dimension observation values to solve this problem. Compared with the Long Short-Term Memory (LSTM) method proposed by Zhou Wenqin [37], fixed dimension observation values are more stable in environments with huge changes. The design observation model is a mean simulation observation model, as shown in Equation (19):

é’ˆå¯¹è§‚æµ‹ç»´åº¦éšæ— äººæœºé›†ç¾¤è§„æ¨¡å˜åŒ–å¯¼è‡´ DRL ç®—æ³•å¤±æ•ˆçš„é—®é¢˜ï¼Œè¯¥æ–‡æå‡ºå›ºå®šç»´åº¦è§‚æµ‹å€¼æ¥è§£å†³è¯¥é—®é¢˜ã€‚ä¸å‘¨æ–‡ç´[37]æå‡ºçš„é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œå›ºå®šç»´åº¦è§‚æµ‹å€¼åœ¨å˜åŒ–è¾ƒå¤§çš„ç¯å¢ƒä¸­æ›´åŠ ç¨³å®šã€‚è®¾è®¡è§‚æµ‹æ¨¡å‹æ˜¯å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹æ¨¡å‹ï¼Œå¦‚æ–¹ç¨‹ ï¼ˆ19ï¼‰ æ‰€ç¤ºï¼š

$$
obs _{i}=\left[\begin{array}{llll}n, & \operatorname{pos}_{i}, & \text { pos }_{\text {mean }}, & \operatorname{pos}_{\text {goal }}\end{array}\right]
$$

The agent receives the target assignment $n$ according to local communication and dynamically adjusts its assignment strategy according to $n$. Its position $\operatorname{pos}_{i}$, the average position of the surrounding agents $pos_{\text{mean}}$, and the target position $pos_{\text{goal}}$ allow the agent to complete the target assignment based on its own observations. Meanwhile, the dimensions of the observation will not change when the number of surrounding agents changes dynamically.

æ™ºèƒ½ä½“æ ¹æ®å±€éƒ¨é€šä¿¡æ¥æ”¶ç›®æ ‡åˆ†é… $n$ï¼Œå¹¶æ ¹æ® $n$ åŠ¨æ€è°ƒæ•´å…¶åˆ†é…ç­–ç•¥ã€‚å…¶ä½ç½® $\operatorname{pos}_{i}$ã€å‘¨å›´æ™ºèƒ½ä½“çš„å¹³å‡ä½ç½® $pos_{\text{mean}}$ ä»¥åŠç›®æ ‡ä½ç½® $pos_{\text{goal}}$ ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºè‡ªèº«è§‚å¯Ÿå®Œæˆç›®æ ‡åˆ†é…ã€‚åŒæ—¶ï¼Œå½“å‘¨å›´æ™ºèƒ½ä½“çš„æ•°é‡åŠ¨æ€å˜åŒ–æ—¶ï¼Œè§‚å¯Ÿçš„ç»´åº¦ä¸ä¼šå‘ç”Ÿå˜åŒ–ã€‚

At the same time, n is designed as a simulation quantity for large UAV swarms, where n is simulated as a random number. Through the parameter n, the situation of a large number of UAVs around a single UAV can be simulated. By training a small number of UAVs, an algorithm suitable for a large number of scenarios can be obtained. The mean simulation observation model effectively reduces the computing power and time consumed by training a large number of UAV task assignment algorithms and solves the disadvantage that the trained algorithms can only be applied to a fixed number. In subsequent experiments, it can be proved that the algorithm using the mean simulation observation model greatly improves the scalability of the MADDPG algorithm.

åŒæ—¶ï¼Œn è¢«è®¾è®¡ä¸ºå¤§å‹æ— äººæœºé›†ç¾¤çš„æ¨¡æ‹Ÿé‡ï¼Œå…¶ä¸­ n è¢«æ¨¡æ‹Ÿä¸ºéšæœºæ•°ã€‚é€šè¿‡å‚æ•° nï¼Œå¯ä»¥æ¨¡æ‹Ÿå¤§é‡æ— äººæœºå›´ç»•å•ä¸ªæ— äººæœºçš„æƒ…å†µã€‚é€šè¿‡è®­ç»ƒå°‘é‡æ— äººæœºï¼Œå¯ä»¥å¾—åˆ°é€‚åˆå¤§é‡åœºæ™¯çš„ç®—æ³•ã€‚å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹æ¨¡å‹æœ‰æ•ˆé™ä½äº†è®­ç»ƒå¤§é‡æ— äººæœºä»»åŠ¡åˆ†é…ç®—æ³•çš„è®¡ç®—èƒ½åŠ›å’Œæ—¶é—´æ¶ˆè€—ï¼Œè§£å†³äº†è®­ç»ƒç®—æ³•åªèƒ½åº”ç”¨äºå›ºå®šæ•°é‡çš„ç¼ºç‚¹ã€‚åœ¨éšåçš„å®éªŒä¸­å¯ä»¥è¯æ˜ï¼Œé‡‡ç”¨å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹æ¨¡å‹çš„ç®—æ³•å¤§å¤§æé«˜äº† MADDPG ç®—æ³•çš„å¯æ‰©å±•æ€§ã€‚

### 3.3 Swarm Synchronization Training

Swarm åŒæ­¥è®­ç»ƒ

In traditional multi-agent reinforcement learning training processes, the parameters of the agents are different, so the trained agents cannot be applied to systems of different scales. After the training is completed, the strategy of a single agent is often incomplete and needs the cooperation of other agent strategies. This training method can complete most multi-agent tasks. When considering the scalability, this training method will fail.

åœ¨ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“çš„å‚æ•°ä¸åŒï¼Œå› æ­¤è®­ç»ƒåçš„æ™ºèƒ½ä½“æ— æ³•åº”ç”¨äºä¸åŒè§„æ¨¡çš„ç³»ç»Ÿã€‚è®­ç»ƒå®Œæˆåï¼Œå•ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥å¾€å¾€æ˜¯ä¸å®Œæ•´çš„ï¼Œéœ€è¦å…¶ä»–æ™ºèƒ½ä½“ç­–ç•¥çš„é…åˆã€‚è¿™ç§è®­ç»ƒæ–¹æ³•å¯ä»¥å®Œæˆå¤§å¤šæ•°å¤šæ™ºèƒ½ä½“ä»»åŠ¡ã€‚å½“è€ƒè™‘å¯æ‰©å±•æ€§æ—¶ï¼Œè¿™ç§è®­ç»ƒæ–¹æ³•ä¼šå¤±è´¥ã€‚

Inspired by the characteristics of bee colony systems, this paper designs a training mechanism called a swarm synchronization training mechanism, which is shown in Figure 6, to achieve scalability. Unlike the traditional reinforcement learning training process, all agent parameters are synchronized every certain number of training steps. Under the mean value simulation observation model ğ’ğ’ƒğ’”ğ‘–, action value ğ´ğ‘ğ‘–, and the UAV swarm synchronization training mechanism, we obtain the gradient of the objective function:

å—èœ‚ç¾¤ç³»ç»Ÿç‰¹ç‚¹çš„å¯å‘ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§ç§°ä¸ºèœ‚ç¾¤åŒæ­¥è®­ç»ƒæœºåˆ¶çš„è®­ç»ƒæœºåˆ¶ï¼Œå¦‚å›¾ 6 æ‰€ç¤ºï¼Œä»¥å®ç°å¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸åŒï¼Œæ‰€æœ‰æ™ºèƒ½ä½“å‚æ•°æ¯ç»è¿‡ä¸€å®šæ•°é‡çš„è®­ç»ƒæ­¥éª¤å°±ä¼šåŒæ­¥ã€‚åœ¨å‡å€¼æ¨¡æ‹Ÿè§‚æµ‹æ¨¡å‹ ğ’ğ’ƒğ’”ğ‘–ã€ åŠ¨ä½œå€¼ ğ´ğ‘ğ‘– å’Œæ— äººæœºé›†ç¾¤åŒæ­¥è®­ç»ƒæœºåˆ¶ä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ï¼š

The gradient of the objective function is:

$$
\nabla_{\theta_{i}} J\left(\boldsymbol{\mu}_{i}\right) = \mathbb{E}_{x, A c \sim D} \left[ \left. \nabla_{\theta_{i}} \boldsymbol{\mu}_{i} \left( A c_{i} \mid \boldsymbol{obs}_{i} \right) \nabla_{A c_{i}} Q_{i}^{\mu} \left( x, A c_{1}, \ldots, A c_{N} \right) \right|_{A c_{i} = \boldsymbol{\mu}_{i} \left( \boldsymbol{obs}_{i} \right)} \right]
$$

where $\boldsymbol{x} = \left[ obs_{1}, \ldots, obs_{N} \right]$ is the collection of the observations for each agent. The loss function of the actor network is rewritten in Equation (21):

$$
L\left(\boldsymbol{\mu}_{i}\right) = -Q_{i}^{\mu}\left(x, A c_{1}, \cdots, A c_{N}\right)
$$

The update method of the critical network $Q_{i}^{\mu}$ is formulated as Equation (22):

$$
\begin{aligned}
L\left(\theta_{i}\right) & = \mathbb{E}_{x, A c, r, x^{\prime}} \left[ \left( Q_{i}^{\mu} \left( x, A c_{1}, \ldots, A c_{N} \right) - y \right)^{2} \right] \\
y & = \boldsymbol{r}_{i} + \left. \gamma Q_{i}^{\mu^{\prime}} \left( x^{\prime}, A c_{1}^{\prime}, \ldots, A c_{N}^{\prime} \right) \right|_{A c_{j}^{\prime} = \mu_{j}^{\prime} \left( \text{obs}_{j} \right)}
\end{aligned}
$$

where $\boldsymbol{\mu}^{\prime} = \left\{ \boldsymbol{\mu}_{\theta_{1}^{\prime}}, \ldots, \boldsymbol{\mu}_{\theta_{N}^{\prime}} \right\}$ is the actor target network, $\boldsymbol{\theta}^{\prime}$ is the actor target network parameter, $\omega$ is the critical network parameter, and $\omega^{\prime}$ is the critic target network parameter. After a certain number of steps, we synchronize the parameters of the actor and critic network:

$$
\begin{array}{c}
\boldsymbol{\theta}_{i \_new} = \frac{\sum_{j=1}^{N} \boldsymbol{\theta}_{j}}{N}, \quad \boldsymbol{\theta}_{i \_new}^{\prime} = \frac{\sum_{j=1}^{N} \boldsymbol{\theta}_{j}^{\prime}}{N} \\
\boldsymbol{\omega}_{i \_new} = \frac{\sum_{j=1}^{N} \boldsymbol{\omega}_{j}}{N}, \quad \boldsymbol{\omega}_{i \_new}^{\prime} = \frac{\sum_{j=1}^{N} \boldsymbol{\omega}_{j}^{\prime}}{N}
\end{array}
$$

![Figure 6. Swarm Synchronization Training Mechanism.](./images/2023.04-Drones-DRL4SwarmTA/Fig6-Swarm-Synchronization-Training-Mechanism.png)

When the training process is finished, all agents have the same â€œbrainâ€, which means the same parameters. The final strategy ğ can be directly applied to any scale of a UAV swarm system. Thus, Ex-MADDPG with a synchronization training mechanism solves the problem of applying the algorithm to large-scale agents.

å½“è®­ç»ƒè¿‡ç¨‹å®Œæˆåï¼Œæ‰€æœ‰ä»£ç†éƒ½æœ‰ç›¸åŒçš„ â€œå¤§è„‘â€ï¼Œè¿™æ„å‘³ç€ç›¸åŒçš„å‚æ•°ã€‚æœ€ç»ˆç­–ç•¥ ğ å¯ä»¥ç›´æ¥åº”ç”¨äºä»»ä½•è§„æ¨¡çš„æ— äººæœºé›†ç¾¤ç³»ç»Ÿã€‚å› æ­¤ï¼Œå…·æœ‰åŒæ­¥è®­ç»ƒæœºåˆ¶çš„ Ex-MADDPG è§£å†³äº†å°†ç®—æ³•åº”ç”¨äºå¤§è§„æ¨¡ä»£ç†çš„é—®é¢˜ã€‚

### 3.4 Extensible Multi-Decision Mechanism

To ensure the performance of dynamic task assignment, this paper proposes a multi-decision mechanism to adjust its decision in real time according to n, as shown in Figure 7. In this mechanism, all agents complete the first-round decision, i.e., ğ‘›=0, as shown in Equation (24), and then communicate with other involved agents (shown as the same color in Figure 7), and make an attack decision again, as shown in Equations (25) and (26):
ä¸ºäº†ä¿è¯åŠ¨æ€ä»»åŠ¡åˆ†é…çš„æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå†³ç­–æœºåˆ¶ï¼Œæ ¹æ® n å®æ—¶è°ƒæ•´å…¶å†³ç­–ï¼Œå¦‚å›¾ 7 æ‰€ç¤ºã€‚åœ¨è¿™ç§æœºåˆ¶ä¸­ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“éƒ½å®Œæˆç¬¬ä¸€è½®å†³ç­–ï¼Œ ğ‘›=0 å³ï¼Œå¦‚ç­‰å¼ ï¼ˆ24ï¼‰ æ‰€ç¤ºï¼Œç„¶åä¸å…¶ä»–å‚ä¸çš„æ™ºèƒ½ä½“é€šä¿¡ï¼ˆå¦‚å›¾ 7 ä¸­çš„ç›¸åŒé¢œè‰²æ‰€ç¤ºï¼‰ï¼Œå¹¶å†æ¬¡åšå‡ºæ”»å‡»å†³ç­–ï¼Œå¦‚ç­‰å¼ ï¼ˆ25ï¼‰ å’Œ ï¼ˆ 26ï¼‰ æ‰€ç¤ºï¼š

$$
\left[0\right., pos _{i}, pos _{\text {mean }} pos \left._{\text {goal }}\right] \rightarrow \mu_{i} \rightarrow A c_{i}
$$

$$
\sum_{i=1}^{N} A c_{i} \rightarrow n
$$

$$
\left[n\right., pos _{i}, pos _{\text {mean }} pos \left._{\text {goal }}\right] \rightarrow \mu_{i} \rightarrow A c_{i}
$$

![Figure 7. Scalable Multi-decision Mechanism.](./images/2023.04-Drones-DRL4SwarmTA/Fig7-Scalable-Multi-decision-Mechanism.png)

Remark 1. In practice, there may be more than one type of UAV to complete the mission and more than one target to attack. At the same time, the mission objective may have different priorities. It is impossible to train for every possible situation. The proposed multi-decision mechanism can easily accommodate these requirements by simply adding the appropriate decision conditions in Figure 7, such as the priority of the targets, the characteristics of attack targets, and so on.

Remark 1. åœ¨å®è·µä¸­ï¼Œå¯èƒ½æœ‰ä¸æ­¢ä¸€ç§ç±»å‹çš„æ— äººæœºæ¥å®Œæˆä»»åŠ¡ï¼Œå¹¶ä¸”æœ‰ä¸æ­¢ä¸€ç§ç›®æ ‡è¦æ”»å‡»ã€‚åŒæ—¶ï¼Œä»»åŠ¡ç›®æ ‡å¯èƒ½å…·æœ‰ä¸åŒçš„ä¼˜å…ˆçº§ã€‚ä¸å¯èƒ½é’ˆå¯¹æ‰€æœ‰å¯èƒ½çš„æƒ…å†µè¿›è¡Œè®­ç»ƒã€‚æ‰€æå‡ºçš„å¤šå†³ç­–æœºåˆ¶åªéœ€åœ¨å›¾ 7 ä¸­æ·»åŠ é€‚å½“çš„å†³ç­–æ¡ä»¶ï¼Œä¾‹å¦‚ç›®æ ‡çš„ä¼˜å…ˆçº§ã€æ”»å‡»ç›®æ ‡çš„ç‰¹å¾ç­‰ï¼Œå³å¯è½»æ¾æ»¡è¶³è¿™äº›è¦æ±‚ã€‚

By designing multi-decision mechanisms, the algorithm is more scalable and can handle much more complex dynamic missions.

é€šè¿‡è®¾è®¡å¤šå†³ç­–æœºåˆ¶ï¼Œè¯¥ç®—æ³•æ›´å…·å¯æ‰©å±•æ€§ï¼Œå¯ä»¥å¤„ç†æ›´å¤æ‚çš„åŠ¨æ€ä»»åŠ¡ã€‚

## 4 Simulation Experiments and Results

The simulation environment adopts the classical multi-agent simulation environment MPE, which is a set of 2D environments with discrete time and continuous space developed by OpenAI. This environment performs a series of tasks by controlling the motion of various role particles in 2D space [38]. At present, it is widely used in the simulation and verification of various multi-agent RL algorithms. We deployed the proposed algorithm on the PC platform with Intel Xeon Gold 5222 and NVIDIA GeForce RTX 2080Ti.

ä»¿çœŸç¯å¢ƒé‡‡ç”¨ç»å…¸çš„å¤šæ™ºèƒ½ä½“ä»¿çœŸç¯å¢ƒ MPEï¼ŒMPE æ˜¯ OpenAI å¼€å‘çš„ä¸€ç»„å…·æœ‰ç¦»æ•£æ—¶é—´å’Œè¿ç»­ç©ºé—´çš„ 2D ç¯å¢ƒã€‚è¯¥ç¯å¢ƒé€šè¿‡æ§åˆ¶ 2D ç©ºé—´ä¸­å„ç§è§’è‰²ç²’å­çš„è¿åŠ¨æ¥æ‰§è¡Œä¸€ç³»åˆ—ä»»åŠ¡ [ 38]ã€‚ç›®å‰ï¼Œå®ƒè¢«å¹¿æ³›åº”ç”¨äºå„ç§å¤šæ™ºèƒ½ä½“ RL ç®—æ³•çš„ä»¿çœŸå’ŒéªŒè¯ã€‚æˆ‘ä»¬åœ¨ PC å¹³å°ä¸Šéƒ¨ç½²äº†æ‰€æå‡ºçš„ç®—æ³•ï¼Œä½¿ç”¨çš„æ˜¯ Intel Xeon Gold 5222 å’Œ NVIDIA GeForce RTX 2080Tiã€‚

In this paper, the training scenario and the application scenario are made to be not exactly the same in order to illustrate the scalability of the proposed algorithm. Therefore, the training scenario and the application scenario will be discussed respectively.

åœ¨æœ¬æ–‡ä¸­ï¼Œè®­ç»ƒåœºæ™¯å’Œåº”ç”¨åœºæ™¯å¹¶ä¸å®Œå…¨ç›¸åŒï¼Œä»¥è¯´æ˜æ‰€æå‡ºçš„ç®—æ³•çš„å¯æ‰©å±•æ€§ã€‚å› æ­¤ï¼Œå°†åˆ†åˆ«è®¨è®ºè®­ç»ƒåœºæ™¯å’Œåº”ç”¨åœºæ™¯ã€‚

### 4.1 Training Experiment Scenario

We trained the algorithm with only four agents and deploy the result to any large system. During the training process, the agent moves and searches for the target in the scene. After the agent finds the target, it will communicate with the surrounding agents and make a decision.
æˆ‘ä»¬åªç”¨å››ä¸ªä»£ç†è®­ç»ƒäº†ç®—æ³•ï¼Œå¹¶å°†ç»“æœéƒ¨ç½²åˆ°ä»»ä½•å¤§å‹ç³»ç»Ÿã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç†åœ¨åœºæ™¯ä¸­ç§»åŠ¨å’Œæœç´¢ç›®æ ‡ã€‚Agent æ‰¾åˆ°ç›®æ ‡åï¼Œä¼šä¸å‘¨å›´çš„ Agent è¿›è¡Œæ²Ÿé€šå¹¶åšå‡ºå†³ç­–ã€‚
We set the following conditions and assumptions for the task-assignment training experiment:
æˆ‘ä»¬ä¸ºä»»åŠ¡åˆ†é…è®­ç»ƒå®éªŒè®¾ç½®äº†ä»¥ä¸‹æ¡ä»¶å’Œå‡è®¾ï¼š

(1) The UAV makes decisions based on the distance to the target and the average distance of the group to the target.
æ— äººæœºæ ¹æ®åˆ°ç›®æ ‡çš„è·ç¦»å’Œç¾¤ä½“åˆ°ç›®æ ‡çš„å¹³å‡è·ç¦»åšå‡ºå†³ç­–ã€‚
(2) The target needs at least two UAVs to destroy.
ç›®æ ‡è‡³å°‘éœ€è¦ä¸¤æ¶æ— äººæœºæ‰èƒ½æ‘§æ¯ã€‚
(3) The UAV only observes the target within its detection range.
æ— äººæœºä»…è§‚å¯Ÿå…¶æ£€æµ‹èŒƒå›´å†…çš„ç›®æ ‡ã€‚
(4) The UAV communicates only with agents within the communication range.
æ— äººæœºä»…ä¸é€šä¿¡èŒƒå›´å†…çš„ä»£ç†é€šä¿¡ã€‚

### 4.2 Construction of Training Model

During the training process, the training model is designed according to the task assignment conditions and specific simulation application scenarios in Section 4.1, including action, reward, mean simulation observation model, a swarm synchronization training mechanism, and a multi-decision mechanism.
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ ¹æ® 4.1 èŠ‚ä¸­çš„ä»»åŠ¡åˆ†é…æ¡ä»¶å’Œå…·ä½“ä»¿çœŸåº”ç”¨åœºæ™¯è®¾è®¡è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬åŠ¨ä½œã€å¥–åŠ±ã€å‡å€¼æ¨¡æ‹Ÿè§‚å¯Ÿæ¨¡å‹ã€ç¾¤ä½“åŒæ­¥è®­ç»ƒæœºåˆ¶å’Œå¤šå†³ç­–æœºåˆ¶ã€‚

#### 4.2.1 Action Value

In the process of UAV swarm task assignment, the UAV needs to perform actions such as obstacle avoidance and assignment according to the observation value. According to the task requirements and experimental assumptions, the design action value is shown in Equation (27):
åœ¨æ— äººæœºé›†ç¾¤ä»»åŠ¡åˆ†é…è¿‡ç¨‹ä¸­ï¼Œæ— äººæœºéœ€è¦æ ¹æ®è§‚æµ‹å€¼è¿›è¡Œé¿éšœã€åˆ†é…ç­‰åŠ¨ä½œã€‚æ ¹æ®ä»»åŠ¡è¦æ±‚å’Œå®éªŒå‡è®¾ï¼Œè®¾è®¡è¡ŒåŠ¨å€¼å¦‚å…¬å¼ ï¼ˆ 27ï¼‰ æ‰€ç¤ºï¼š

#### 4.2.2 Mean Simulation Observation

#### 4.2.3 Centralized and Distributed Reward Function

### 4.3 Validity of the Algorithm

![Figure 8. Task Loss.](./images/2023.04-Drones-DRL4SwarmTA/Fig8-Task-Loss.png)
![Figure 9. Task Target Difference.](./images/2023.04-Drones-DRL4SwarmTA/Fig9-Task-Target-Difference.png)

### 4.4 Extended Experiments

Based on the training scenario in Section 4.1, we expand the number of agents and targets. At the same time, to increase the detection rate of the target, the expanded scene is divided into three phases: takeoff, search, and decision. The agent will take off from the bottom left, pass through the formation, fly to the target area and hover, and complete the total task.

æ ¹æ® Section 4.1 ä¸­çš„è®­ç»ƒåœºæ™¯ï¼Œæˆ‘ä»¬æ‰©å±•äº† Agent å’Œ Target çš„æ•°é‡ã€‚åŒæ—¶ï¼Œä¸ºäº†æé«˜å¯¹ç›®æ ‡çš„æ£€å‡ºç‡ï¼Œå°†æ‰©å±•åçš„åœºæ™¯åˆ†ä¸ºèµ·é£ã€æœç´¢å’Œå†³ç­–ä¸‰ä¸ªé˜¶æ®µã€‚ä»£ç†å°†ä»å·¦ä¸‹è§’èµ·é£ï¼Œç©¿è¿‡ç¼–é˜Ÿï¼Œé£åˆ°ç›®æ ‡åŒºåŸŸå¹¶æ‚¬åœï¼Œå®Œæˆå…¨éƒ¨ä»»åŠ¡ã€‚

In this section, we design two kinds of extension experiments to demonstrate the superiority of our proposed algorithm.

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§æ‰©å±•å®éªŒæ¥è¯æ˜æˆ‘ä»¬æå‡ºçš„ç®—æ³•çš„ä¼˜è¶Šæ€§ã€‚

**Experiment 1: Number extension**

Considering that the attack ratio between agents and targets in the training process is 2:1, the number of agents and the number of targets are expanded according to this ratio. The experimental design conditions are as follows:

(1)Number of agents:number of targets = 2:1;
(2)The number of agents is between 8 and 56, with an interval of 4;
(3)The final decision distance of the agent is designed as 1.5. If the distance between the agent and the target exceeds this range, the agent will communicate and make a decision at specified intervals. Otherwise, it will attack the target directly.

**Experiment 2: Task extension**

The practical application may be different from the training, so the tasks may also need to be expanded. To ensure the attack effect, we add task redistribution requirements, which means that one target may require two or more UAVs to destroy.

(1)Number of agents:number of targets = 5:2;
(2)The number of agents is between 5 and 55, with an interval of 5;
(3)The final decision distance of the agent is chosen to be 1.5. If the distance between the agent and the target exceeds this range, the agent will communicate and make a decision at specified intervals. Otherwise, it will attack the target directly;
(4)The target attacked by two agents has an 80% chance of being destroyed, while the target attacked by three agents has a 100% chance of being destroyed.
To reduce the random effect, each experiment is performed 30 times.

### 4.5 Extended Performance Test

Based on the performance requirements of the extended extension experiment, the extended performance metrics must be redesigned to compare the performance changes during the two extended experiments. In the following experiment, the 5:2 in the legend is used to refer to Experiment 2 for algorithm comparison and analysis.

æ ¹æ®æ‰©å±•æ‰©å±•å®éªŒçš„æ€§èƒ½è¦æ±‚ï¼Œå¿…é¡»é‡æ–°è®¾è®¡æ‰©å±•æ€§èƒ½æŒ‡æ ‡ï¼Œä»¥æ¯”è¾ƒä¸¤ä¸ªæ‰©å±•å®éªŒæœŸé—´çš„æ€§èƒ½å˜åŒ–ã€‚åœ¨ä¸‹é¢çš„å®éªŒä¸­ï¼Œå›¾ä¾‹ä¸­çš„ 5ï¼š2 ç”¨äºå‚è€ƒå®éªŒ 2 è¿›è¡Œç®—æ³•å¯¹æ¯”åˆ†æã€‚

The running simulation process of the algorithm is shown in Figure 10, which is applied to the dynamic target-assignment scenario of 32 agents. In Figure 10, (1) is the swarm of intelligent agents flying from the takeoff area to the target area, and (2) is the agent that first detects the target and starts to make decisions and allocate the target. Other agents continue to search. Finally, (3) is the final assignment result. Each target is hit by two agents.

è¯¥ç®—æ³•çš„è¿è¡Œä»¿çœŸè¿‡ç¨‹å¦‚å›¾ 10 æ‰€ç¤ºï¼Œå®ƒåº”ç”¨äº 32 ä¸ªæ™ºèƒ½ä½“çš„åŠ¨æ€ç›®æ ‡åˆ†é…åœºæ™¯ã€‚åœ¨å›¾ 10 ä¸­ï¼Œï¼ˆ1ï¼‰ æ˜¯ä»èµ·é£åŒºåŸŸé£åˆ°ç›®æ ‡åŒºåŸŸçš„æ™ºèƒ½ä»£ç†ç¾¤ï¼Œï¼ˆ2ï¼‰ æ˜¯é¦–å…ˆæ£€æµ‹åˆ°ç›®æ ‡å¹¶å¼€å§‹åšå‡ºå†³ç­–å’Œåˆ†é…ç›®æ ‡çš„ä»£ç†ã€‚å…¶ä»–ä»£ç†ç»§ç»­æœç´¢ã€‚æœ€åï¼Œï¼ˆ3ï¼‰ æ˜¯æœ€ç»ˆçš„èµ‹å€¼ç»“æœã€‚æ¯ä¸ªç›®æ ‡éƒ½ç”±ä¸¤ä¸ªä»£ç†å‘½ä¸­ã€‚

![Figure 10. Algorithm Scalability Test Process.](./images/2023.04-Drones-DRL4SwarmTA/image-6.png)

For the above process, the following performance metrics are designed.

#### 4.5.1 Task Completion Rate

As shown in Figure 11, the improved MADDPG algorithm (ms-MADDPG) using only the mean simulation has a small scalability number. When the number of agents continues to increase, the task-completion rate of the ms-MADDPG algorithm decreases significantly, and it can only complete some tasks. The MADDPG algorithm has no scalability at all. When the number of applications is inconsistent with the number of training, the algorithm will not work, and the task completion rate is 0%.

å¦‚å›¾ 11 æ‰€ç¤ºï¼Œä»…ä½¿ç”¨å‡å€¼æ¨¡æ‹Ÿçš„æ”¹è¿›åçš„ MADDPG ç®—æ³• ï¼ˆms-MADDPGï¼‰ å…·æœ‰è¾ƒå°çš„å¯æ‰©å±•æ€§æ•°å­—ã€‚å½“ Agent æ•°é‡æŒç»­å¢åŠ æ—¶ï¼Œms-MADDPG ç®—æ³•çš„ä»»åŠ¡å®Œæˆç‡æ˜æ˜¾ä¸‹é™ï¼Œåªèƒ½å®Œæˆéƒ¨åˆ†ä»»åŠ¡ã€‚MADDPG ç®—æ³•æ ¹æœ¬æ²¡æœ‰å¯æ‰©å±•æ€§ã€‚å½“ç”³è¯·æ•°ä¸è®­ç»ƒæ•°ä¸ä¸€è‡´æ—¶ï¼Œç®—æ³•å°†ä¸èµ·ä½œç”¨ï¼Œä»»åŠ¡å®Œæˆç‡ä¸º 0%ã€‚

![Fig11-Task-Completion-Rate](./images/2023.04-Drones-DRL4SwarmTA/Fig11-Task-Completion-Rate.png)

In the comparison test between Experiment 1 and Experiment 2 (5:2), the Hungarian algorithm was found to be able to destroy the target at 100% in Experiment 1, but destroyed less than 90% of the target in Experiment 2. At the same time, Ex-MADDPG could destroy more than 90% of the target in two experiments within the range of the number of tests. In Experiment 2, the algorithm could detect the target in real time. If the target was not completely destroyed, the new optimal agent was immediately determined to attack it to ensure that the target esd destroyed, achieving a task-completion rate of nearly 100%. The Ex-MADDPG algorithm could maintain the task assignment performance under application numbers and task-assignment conditions.

åœ¨å®éªŒ 1 å’Œå®éªŒ 2 çš„æ¯”è¾ƒæµ‹è¯• ï¼ˆ5ï¼š2ï¼‰ ä¸­ï¼Œå‘ç°åŒˆç‰™åˆ©ç®—æ³•èƒ½å¤Ÿåœ¨å®éªŒ 1 100% ä¸­æ‘§æ¯ç›®æ ‡ï¼Œä½†æ‘§æ¯çš„ç›®æ ‡æ¯” 90% å®éªŒ 2 ä¸­çš„ç›®æ ‡å°‘ã€‚åŒæ—¶ï¼ŒEx-MADDPG å¯ä»¥åœ¨æµ‹è¯•æ¬¡æ•°èŒƒå›´å†…çš„ä¸¤æ¬¡å®éªŒä¸­æ‘§æ¯è¶…è¿‡ 90% ç›®æ ‡çš„ç›®æ ‡ã€‚åœ¨å®éªŒ 2 ä¸­ï¼Œè¯¥ç®—æ³•å¯ä»¥å®æ—¶æ£€æµ‹ç›®æ ‡ã€‚å¦‚æœç›®æ ‡æ²¡æœ‰è¢«å®Œå…¨æ‘§æ¯ï¼Œç«‹å³ç¡®å®šæ–°çš„æœ€ä¼˜æ™ºèƒ½ä½“å¯¹å…¶è¿›è¡Œæ”»å‡»ï¼Œç¡®ä¿ç›®æ ‡ ESD è¢«æ‘§æ¯ï¼Œå®ç°æ¥è¿‘ 100% çš„ä»»åŠ¡å®Œæˆç‡ã€‚Ex-MADDPG ç®—æ³•å¯ä»¥åœ¨åº”ç”¨ç¨‹åºæ•°é‡å’Œä»»åŠ¡åˆ†é…æ¡ä»¶ä¸‹ä¿æŒä»»åŠ¡åˆ†é…æ€§èƒ½ã€‚

#### 4.5.2 Task Loss

The design of task loss is the same as that of task loss in Section 4.3. This metric is the key metric for judging the distributional effect. The smaller the value, the more advantageous it is for the agent in the group, and the less time and distance it takes to execute the attack decision, which means the better the decision will be.

ä»»åŠ¡æŸå¤±çš„è®¾è®¡ä¸ç¬¬ 4.3 èŠ‚ä¸­çš„ä»»åŠ¡æŸå¤±ç›¸åŒã€‚è¯¥æŒ‡æ ‡æ˜¯åˆ¤æ–­åˆ†å¸ƒæ•ˆåº”çš„å…³é”®æŒ‡æ ‡ã€‚å€¼è¶Šå°ï¼Œå¯¹ç»„ä¸­çš„ä»£ç†è¶Šæœ‰åˆ©ï¼Œæ‰§è¡Œæ”»å‡»å†³ç­–æ‰€éœ€çš„æ—¶é—´å’Œè·ç¦»å°±è¶ŠçŸ­ï¼Œè¿™æ„å‘³ç€å†³ç­–å°±è¶Šå¥½ã€‚

Figure 12 shows that the Hungarian algorithm, as a traditional algorithm, had poor results in Experiment 1 and Experiment 2. In the Hungarian algorithm, all agents make decisions at the same time, resulting in poor task loss performance. The Ex-MADDPG algorithm can select the agents in the group that is closer to the target to attack and make better decisions for each decision. When the shortest decision distance is chosen as 1.5, the agent can only make the final attack decision when it is relatively close to the target, which can further reduce the task loss. In terms of task loss, whether in Experiment 1 or Experiment 2, Ex-MADDPG algorithm has obvious advantages and can make better decisions in the case of expansion. However, the MADDPG algorithm cannot complete the expansion experiment and count its task loss.

å›¾ 12 æ˜¾ç¤ºï¼ŒåŒˆç‰™åˆ©ç®—æ³•ä½œä¸ºä¸€ç§ä¼ ç»Ÿç®—æ³•ï¼Œåœ¨å®éªŒ 1 å’Œå®éªŒ 2 ä¸­æ•ˆæœä¸ä½³ã€‚åœ¨åŒˆç‰™åˆ©ç®—æ³•ä¸­ï¼Œæ‰€æœ‰ä»£ç†åŒæ—¶åšå‡ºå†³ç­–ï¼Œå¯¼è‡´ä»»åŠ¡ä¸¢å¤±æ€§èƒ½ä¸ä½³ã€‚Ex-MADDPG ç®—æ³•å¯ä»¥é€‰æ‹©ç»„ä¸­æ›´æ¥è¿‘æ”»å‡»ç›®æ ‡çš„ Agentï¼Œå¹¶ä¸ºæ¯ä¸ªå†³ç­–åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚å½“æœ€çŸ­å†³ç­–è·ç¦»é€‰æ‹©ä¸º 1.5 æ—¶ï¼Œæ™ºèƒ½ä½“åªæœ‰åœ¨è·ç¦»ç›®æ ‡æ¯”è¾ƒè¿‘çš„æƒ…å†µä¸‹æ‰èƒ½åšå‡ºæœ€ç»ˆçš„æ”»å‡»å†³ç­–ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘ä»»åŠ¡æŸå¤±ã€‚åœ¨ä»»åŠ¡ä¸¢å¤±æ–¹é¢ï¼Œæ— è®ºæ˜¯åœ¨ Experiment 1 è¿˜æ˜¯ Experiment 2 ä¸­ï¼ŒEx-MADDPG ç®—æ³•éƒ½å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œåœ¨æ‰©å±•çš„æƒ…å†µä¸‹å¯ä»¥åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚ä½†æ˜¯ï¼ŒMADDPG ç®—æ³•æ— æ³•å®Œæˆæ‰©å®¹å®éªŒå¹¶ç»Ÿè®¡å…¶ä»»åŠ¡ä¸¢å¤±ã€‚

![Fig12-Task-Loss](./images/2023.04-Drones-DRL4SwarmTA/Fig12-Task-Loss.png)

#### 4.5.3 Decision Time

In the actual operation process, real time is a very important indicator that determines whether the agent can react quickly to external changes and react in real time. Therefore, the time required to execute a decision is designed as one of the performance metrics of the algorithm.
åœ¨å®é™…æ“ä½œè¿‡ç¨‹ä¸­ï¼Œå®æ—¶æ€§æ˜¯å†³å®šæ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿå¯¹å¤–éƒ¨å˜åŒ–åšå‡ºå¿«é€Ÿååº”å¹¶å®æ—¶ååº”çš„ä¸€ä¸ªéå¸¸é‡è¦çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼Œæ‰§è¡Œå†³ç­–æ‰€éœ€çš„æ—¶é—´è¢«è®¾è®¡ä¸ºç®—æ³•çš„æ€§èƒ½æŒ‡æ ‡ä¹‹ä¸€ã€‚
As shown in Figure 13, whether in Experiment 1 or Experiment 2, the Hungarian algorithm, as a traditional algorithm, has a relatively short execution time in a small number of cases, but there is an obvious upward trend with the increase in agents. It can be predicted that when the scale of the agent is large, it will take a lot of time to obtain the decision results. The Ex-MADDPG algorithm is calculated by a neural network. The change in the number of surrounding agents has little impact on its input value, and the number of iterations needed to make decisions is small. Therefore, with the increase in the number of agents, its decision-making time showed a small upward trend. It can easily meet the real-time requirements of the scene. However, the ms-MADDPG algorithm has many iterations to make decisions, and it is difficult for to make decisions, so it consumes the most time. The MADDPG algorithm cannot count its decision time because it cannot complete the expansion experiment.
å¦‚å›¾ 13 æ‰€ç¤ºï¼Œæ— è®ºæ˜¯åœ¨å®éªŒ 1 è¿˜æ˜¯å®éªŒ 2 ä¸­ï¼ŒåŒˆç‰™åˆ©ç®—æ³•ä½œä¸ºä¼ ç»Ÿç®—æ³•ï¼Œåœ¨å°‘æ•°æƒ…å†µä¸‹å…·æœ‰ç›¸å¯¹è¾ƒçŸ­çš„æ‰§è¡Œæ—¶é—´ï¼Œä½†éšç€æ™ºèƒ½ä½“çš„å¢åŠ ï¼Œå­˜åœ¨æ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ã€‚å¯ä»¥é¢„è§ï¼Œå½“æ™ºèƒ½ä½“è§„æ¨¡è¾ƒå¤§æ—¶ï¼Œéœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´æ‰èƒ½è·å¾—å†³ç­–ç»“æœã€‚Ex-MADDPG ç®—æ³•ç”±ç¥ç»ç½‘ç»œè®¡ç®—ã€‚å‘¨å›´ä»£ç†æ•°é‡çš„å˜åŒ–å¯¹å…¶è¾“å…¥å€¼å½±å“å¾ˆå°ï¼Œå¹¶ä¸”åšå‡ºå†³ç­–æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°å¾ˆå°ã€‚å› æ­¤ï¼Œéšç€æ™ºèƒ½ä½“æ•°é‡çš„å¢åŠ ï¼Œå…¶å†³ç­–æ—¶é—´å‘ˆç°å°å¹…ä¸Šå‡è¶‹åŠ¿ã€‚å¯ä»¥è½»æ¾æ»¡è¶³åœºæ™¯çš„å®æ—¶æ€§è¦æ±‚ã€‚ä½†æ˜¯ï¼Œms-MADDPG ç®—æ³•éœ€è¦å¤šæ¬¡è¿­ä»£æ¥åšå‡ºå†³ç­–ï¼Œå¹¶ä¸”å¾ˆéš¾åšå‡ºå†³ç­–ï¼Œå› æ­¤å®ƒæ¶ˆè€—çš„æ—¶é—´æœ€å¤šã€‚MADDPG ç®—æ³•æ— æ³•è®¡ç®—å…¶å†³ç­–æ—¶é—´ï¼Œå› ä¸ºå®ƒæ— æ³•å®Œæˆæ‰©å±•å®éªŒã€‚

![Figure 13. Decision Time.](./images/2023.04-Drones-DRL4SwarmTA/Fig13-Decision-Time.png)

#### 4.5.4 Number of Communications

The number of agents that need to communicate refers to the number of other agents that each agent needs to communicate with when making decisions in a decision process. The more agents that need to communicate in a decision round, the more communication bandwidth is required to make decisions, the higher the hardware requirements for the agents, and the more difficult the algorithm is to implement.
éœ€è¦é€šä¿¡çš„ä»£ç†æ•°æ˜¯æŒ‡åœ¨å†³ç­–è¿‡ç¨‹ä¸­åšå‡ºå†³ç­–æ—¶ï¼Œæ¯ä¸ªä»£ç†éœ€è¦ä¸ä¹‹é€šä¿¡çš„å…¶ä»–ä»£ç†æ•°ã€‚åœ¨å†³ç­–è½®æ¬¡ä¸­éœ€è¦é€šä¿¡çš„ä»£ç†è¶Šå¤šï¼Œåšå‡ºå†³ç­–æ‰€éœ€çš„é€šä¿¡å¸¦å®½å°±è¶Šå¤šï¼Œå¯¹ä»£ç†çš„ç¡¬ä»¶è¦æ±‚å°±è¶Šé«˜ï¼Œç®—æ³•çš„å®ç°éš¾åº¦å°±è¶Šå¤§ã€‚
As shown in Figure 14, in Experiment 1 and Experiment 2, the Hungarian algorithm needed all scene information to make decisions, and each agent needed to communicate with all other agents. Therefore, the number of communications is equal to the size of the agent. The Ex-MADDPG algorithm only needs to communicate with nearby agents and can make decisions using part of the scene information. The required communication bandwidth is therefore greatly reduced. The improved ms-MADDPG algorithm using mean simulation requires a medium number of agents to make decisions. It can be found that the introduction of neural networks greatly reduces the amount of information required for decision-making and reduces the communication bandwidth.
å¦‚å›¾ 14 æ‰€ç¤ºï¼Œåœ¨å®éªŒ 1 å’Œå®éªŒ 2 ä¸­ï¼ŒåŒˆç‰™åˆ©ç®—æ³•éœ€è¦æ‰€æœ‰åœºæ™¯ä¿¡æ¯æ¥åšå‡ºå†³ç­–ï¼Œå¹¶ä¸”æ¯ä¸ªä»£ç†éƒ½éœ€è¦ä¸æ‰€æœ‰å…¶ä»–ä»£ç†è¿›è¡Œé€šä¿¡ã€‚å› æ­¤ï¼Œé€šä¿¡çš„æ•°é‡ç­‰äºä»£ç†çš„å¤§å°ã€‚Ex-MADDPG ç®—æ³•åªéœ€è¦ä¸é™„è¿‘çš„ä»£ç†è¿›è¡Œé€šä¿¡ï¼Œå°±å¯ä»¥ä½¿ç”¨éƒ¨åˆ†åœºæ™¯ä¿¡æ¯åšå‡ºå†³ç­–ã€‚å› æ­¤ï¼Œæ‰€éœ€çš„é€šä¿¡å¸¦å®½å¤§å¤§é™ä½ã€‚ä½¿ç”¨å‡å€¼æ¨¡æ‹Ÿçš„æ”¹è¿› ms-MADDPG ç®—æ³•éœ€è¦ä¸­ç­‰æ•°é‡çš„ä»£ç†æ¥åšå‡ºå†³ç­–ã€‚å¯ä»¥å‘ç°ï¼Œç¥ç»ç½‘ç»œçš„å¼•å…¥å¤§å¤§å‡å°‘äº†å†³ç­–æ‰€éœ€çš„ä¿¡æ¯é‡ï¼Œé™ä½äº†é€šä¿¡å¸¦å®½ã€‚

![Fig14-Number-of-Communications.](./images/2023.04-Drones-DRL4SwarmTA/Fig14-Number-of-Communications.png)

In conclusion, in Experiment 1 and Experiment 2 of the expansion experiment, the Ex-MADDPG algorithm was shown to be significantly superior to the traditional Hungarian algorithm and the MADDPG algorithm in terms of task completion rate, task loss, decision time, and the communication number, and could maintain stable performance during the expansion process and correctly complete the expected tasks.

ç»¼ä¸Šæ‰€è¿°ï¼Œåœ¨æ‰©å±•å®éªŒçš„å®éªŒ 1 å’Œå®éªŒ 2 ä¸­ï¼ŒEx-MADDPG ç®—æ³•åœ¨ä»»åŠ¡å®Œæˆç‡ã€ä»»åŠ¡ä¸¢å¤±ç‡ã€å†³ç­–æ—¶é—´å’Œé€šä¿¡æ•°é‡æ–¹é¢æ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„åŒˆç‰™åˆ©ç®—æ³•å’Œ MADDPG ç®—æ³•ï¼Œå¹¶ä¸”åœ¨æ‰©å±•è¿‡ç¨‹ä¸­èƒ½å¤Ÿä¿æŒç¨³å®šçš„æ€§èƒ½å¹¶æ­£ç¡®å®Œæˆé¢„æœŸä»»åŠ¡ã€‚

## 5 Experiments and Results

In this section, experiments are transferred from simulation to the real world in the context of task assignment in a UAV swarm target-attacking scenario. To validate its performance in practical task assignment, experiments in the real world were conducted with a group of nano-quadcopters named scit-minis (as shown in Figure 15) flying under the supervision of a NOKOV motion capture system. We deployed the proposed algorithm on the same PC platform as the simulation, but the algorithm ran separately for each UAV. The scit-mini is a nano-quadcopter such as crazyfile 2.0 [39], but with much more power and a longer endurance. Meanwhile, we used an open-source, unmanned vehicle Autopilot Software Suite called ArduPilot to make it easier to transfer the algorithm from simulation to the real world.
åœ¨æœ¬èŠ‚ä¸­ï¼Œåœ¨æ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»åœºæ™¯ä¸­çš„ä»»åŠ¡åˆ†é…ç¯å¢ƒä¸­ï¼Œå®éªŒä»æ¨¡æ‹Ÿè½¬ç§»åˆ°ç°å®ä¸–ç•Œã€‚ä¸ºäº†éªŒè¯å…¶åœ¨å®é™…ä»»åŠ¡åˆ†é…ä¸­çš„æ€§èƒ½ï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œä¸€ç»„åä¸º scit-minis çš„çº³ç±³å››è½´é£è¡Œå™¨ï¼ˆå¦‚å›¾ 15 æ‰€ç¤ºï¼‰åœ¨ NOKOV è¡ŒåŠ¨æ•æ‰ç³»ç»Ÿçš„ç›‘ç£ä¸‹é£è¡Œã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„ç®—æ³•éƒ¨ç½²åœ¨ä¸ä»¿çœŸç›¸åŒçš„ PC å¹³å°ä¸Šï¼Œä½†ç®—æ³•é’ˆå¯¹æ¯ä¸ªæ— äººæœºå•ç‹¬è¿è¡Œã€‚scit-mini æ˜¯ä¸€ç§çº³ç±³å››è½´é£è¡Œå™¨ï¼Œä¾‹å¦‚ crazyfile 2.0 [ 39]ï¼Œä½†å…·æœ‰æ›´å¤§çš„åŠ¨åŠ›å’Œæ›´é•¿çš„ç»­èˆªèƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåä¸º ArduPilot çš„å¼€æºæ— äººé©¾é©¶è½¦è¾† Autopilot è½¯ä»¶å¥—ä»¶ï¼Œä»¥ä¾¿æ›´è½»æ¾åœ°å°†ç®—æ³•ä»æ¨¡æ‹Ÿè½¬ç§»åˆ°ç°å®ä¸–ç•Œã€‚

![Figure 15-Diagram-of-system-components.](./images/2023.04-Drones-DRL4SwarmTA/Fig15-Diagram-of-system-components.png)

### 5.1 Architecture Overview

Similar to crazyswarm [39], our system architecture is outlined in Figure 15. We tracked the scit-mini with a motion capture system using passive spherical markers. Thanks to the sub-millimeter recognition accuracy of the motion capture system, we used the Iterative Closest Point (ICP) algorithm [40] to obtain the precise location of each scit-mini in the swarm in real time.
ä¸ crazyswarm [ 39] ç±»ä¼¼ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ¶æ„å¦‚å›¾ 15 æ‰€ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨æ— æºçƒå½¢æ ‡è®°çš„åŠ¨ä½œæ•æ‰ç³»ç»Ÿè·Ÿè¸ªäº† scit-miniã€‚ç”±äºåŠ¨ä½œæ•æ‰ç³»ç»Ÿçš„äºšæ¯«ç±³çº§è¯†åˆ«ç²¾åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿­ä»£æœ€è¿‘ç‚¹ ï¼ˆICPï¼‰ ç®—æ³• [ 40] æ¥å®æ—¶è·å–æ¯ä¸ª scit-mini åœ¨é›†ç¾¤ä¸­çš„ç²¾ç¡®ä½ç½®ã€‚
Unlike crazyswarm, the scit-mini communicates with a PC platform over WiFi that can transfer more data than Crazyradio PA. The control signals run at 50 Hz with an ROS2 a communication delay of about 10âˆ¼20 ms. However, the main onboard loop, like its attitude control loop, runs at 400 Hz, which can ensure the stable operation of the scit-mini. Each scit-mini obtains the control signals from its own Ex-MADDPG and boids through MAVROS with ROS2. We used only one computer in this experiment, but our system architecture supports multiple computers running simultaneously.
ä¸ crazyswarm ä¸åŒï¼Œscit-mini é€šè¿‡ WiFi ä¸ PC å¹³å°é€šä¿¡ï¼Œè¯¥å¹³å°å¯ä»¥ä¼ è¾“æ¯” Crazyradio PA æ›´å¤šçš„æ•°æ®ã€‚æ§åˆ¶ä¿¡å·ä»¥ 50 Hz çš„é¢‘ç‡è¿è¡Œï¼ŒROS2 çš„é€šä¿¡å»¶è¿Ÿçº¦ä¸º 10âˆ¼20 msã€‚ä½†æ˜¯ï¼Œä¸»æœºè½½å›è·¯å’Œå®ƒçš„å§¿æ€æ§åˆ¶å›è·¯ä¸€æ ·ï¼Œä»¥ 400 Hz çš„é¢‘ç‡è¿è¡Œï¼Œè¿™å¯ä»¥ä¿è¯ scit-mini çš„ç¨³å®šè¿è¡Œã€‚æ¯ä¸ª scit-mini éƒ½é€šè¿‡å¸¦æœ‰ ROS2 çš„ MAVROS ä»è‡ªå·±çš„ Ex-MADDPG å’Œ boids è·å–æ§åˆ¶ä¿¡å·ã€‚åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº†ä¸€å°è®¡ç®—æœºï¼Œä½†æ˜¯æˆ‘ä»¬çš„ç³»ç»Ÿæ¶æ„æ”¯æŒå¤šå°è®¡ç®—æœºåŒæ—¶è¿è¡Œã€‚

### 5.2 Flight Test

The actual test environment is shown in Figure 16, and the experimental area was divided into the take-off area and the target area. We tested our proposed Ex-MADDPG in a task-extension experiment, similar to Experiment 2 in Section 4.4, with nine scit-minis and three targets. The experimental procedure was as follows: nine scit-minis took off from the takeoff area shown in Figure 16(1) and then flew toward the target area, detected three targets, and executed the Ex-MADDPG algorithm. The experimental subject would fly over the target if it decides to attack it, and the rest of the scit-minis that do not decided to attack would continue to fly forward until they cross the target area. The scit-minis used the Boids algorithm to avoid collisions with each other.

å®é™…æµ‹è¯•ç¯å¢ƒå¦‚å›¾ 16 æ‰€ç¤ºï¼Œå®éªŒåŒºåŸŸåˆ†ä¸ºèµ·é£åŒºå’Œç›®æ ‡åŒºã€‚æˆ‘ä»¬åœ¨ä»»åŠ¡æ‰©å±•å®éªŒä¸­æµ‹è¯•äº†æˆ‘ä»¬æå‡ºçš„ Ex-MADDPGï¼Œç±»ä¼¼äºç¬¬ 4.4 èŠ‚ä¸­çš„å®éªŒ 2ï¼Œæœ‰ 9 ä¸ª scit-mini å’Œ 3 ä¸ªç›®æ ‡ã€‚å®éªŒè¿‡ç¨‹å¦‚ä¸‹ï¼š9 æ¶ scit-minis ä»å›¾ 16ï¼ˆ1ï¼‰ æ‰€ç¤ºçš„èµ·é£åŒºåŸŸèµ·é£ï¼Œç„¶åé£å‘ç›®æ ‡åŒºåŸŸï¼Œæ¢æµ‹åˆ°ä¸‰ä¸ªç›®æ ‡ï¼Œå¹¶æ‰§è¡Œ Ex-MADDPG ç®—æ³•ã€‚å¦‚æœå®éªŒå¯¹è±¡å†³å®šæ”»å‡»ç›®æ ‡ï¼Œå®ƒä¼šé£è¶Šç›®æ ‡ï¼Œè€Œå…¶ä½™æ²¡æœ‰å†³å®šæ”»å‡»çš„ scit-mini å°†ç»§ç»­å‘å‰é£è¡Œï¼Œç›´åˆ°ä»–ä»¬ç©¿è¿‡ç›®æ ‡åŒºåŸŸã€‚scit-mini ä½¿ç”¨ Boids ç®—æ³•æ¥é¿å…å½¼æ­¤å†²çªã€‚

![Fig16-Flight-Test](./images/2023.04-Drones-DRL4SwarmTA/Fig16-Flight-Test.png)

We selected two key steps of the experiment during the whole autonomous process, as shown in Figure 16(2),(3). Figure 16(2) shows that the scit-mini made the first decision to attack its target using Ex-MADDPG, and Figure 16(3) is the final result of the task assignment. As shown in Figure 16(3), one target was attacked by two scit-minis and two targets were attacked by three scit-minis. The videos of this experiment and more tests are available online: https://youtu.be/shA1Tu7VujM.

åœ¨æ•´ä¸ªè‡ªä¸»è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å®éªŒçš„ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼Œå¦‚å›¾ 16ï¼ˆ2ï¼‰ã€ï¼ˆ3ï¼‰ æ‰€ç¤ºã€‚å›¾ 16ï¼ˆ2ï¼‰ æ˜¾ç¤º scit-mini é¦–å…ˆå†³å®šä½¿ç”¨ Ex-MADDPG æ”»å‡»å…¶ç›®æ ‡ï¼Œå›¾ 16ï¼ˆ3ï¼‰ æ˜¯ä»»åŠ¡åˆ†é…çš„æœ€ç»ˆç»“æœã€‚å¦‚å›¾ 16ï¼ˆ3ï¼‰ æ‰€ç¤ºï¼Œä¸€ä¸ªç›®æ ‡è¢«ä¸¤ä¸ª scit-mini æ”»å‡»ï¼Œä¸¤ä¸ªç›®æ ‡è¢«ä¸‰ä¸ª scit-mini æ”»å‡»ã€‚è¯¥å®éªŒå’Œæ›´å¤šæµ‹è¯•çš„è§†é¢‘å¯åœ¨çº¿è·å–ï¼šhttps://youtu.be/shA1Tu7VujMã€‚

The experiment demonstrated that the proposed Ex-MADDPG algorithm can accomplish task assignment in UAV swarm target-attacking scenario efficiently in real-time, verifying its practicality and effectiveness.

å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ Ex-MADDPG ç®—æ³•èƒ½å¤Ÿå®æ—¶é«˜æ•ˆåœ°å®Œæˆæ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»åœºæ™¯ä¸­çš„ä»»åŠ¡åˆ†é…ï¼ŒéªŒè¯äº†å…¶å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## 6 Conclusions

This paper presents an improved algorithm Ex-MADDPG algorithm based on MADDPG to solve the problem of task assignment in a UAV swarm target-attacking scenario. This algorithm uses mean simulation observation and swarm-synchronization mechanisms to deploy in arbitrary-scale systems, training only a small number of agents. By designing the scalable multi-decision mechanism, this algorithm can maintain its performance in the process of expansion and achieve arbitrary expansion of the number of UAVs. At the same time, the algorithm can achieve task expansion and can complete similar tasks that differ from the training process. The Ex-MADDPG algorithm can be trained once and applied to a large number of task-assignment scenarios, effectively solving the problem of insufficient scalability of the traditional RL/DRL algorithm. Simulation results show that the Ex-MADDPG has obvious advantages over the Hungarian algorithm in terms of assignment performance, fault tolerance, and real-time capabilities. At the same time, the algorithm has good scalability and maintains performance under the condition of number and task expansion. Furthermore, the proposed method proves to be feasible and effective in UAV swarm target attack scenarios in both simulations and practical experiments.
è¯¥æ–‡é’ˆå¯¹æ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»åœºæ™¯ä¸‹çš„ä»»åŠ¡åˆ†é…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº MADDPG çš„æ”¹è¿›ç®—æ³• Ex-MADDPG ç®—æ³•ã€‚è¯¥ç®—æ³•ä½¿ç”¨å‡å€¼æ¨¡æ‹Ÿè§‚å¯Ÿå’Œç¾¤åŒæ­¥æœºåˆ¶åœ¨ä»»æ„è§„æ¨¡çš„ç³»ç»Ÿä¸­è¿›è¡Œéƒ¨ç½²ï¼Œä»…è®­ç»ƒå°‘é‡ä»£ç†ã€‚é€šè¿‡è®¾è®¡å¯æ‰©å±•çš„å¤šå†³ç­–æœºåˆ¶ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨æ‰©å±•è¿‡ç¨‹ä¸­ä¿æŒå…¶æ€§èƒ½ï¼Œå®ç°æ— äººæœºæ•°é‡çš„ä»»æ„æ‰©å±•ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•å¯ä»¥å®ç°ä»»åŠ¡æ‰©å±•ï¼Œå¯ä»¥å®Œæˆä¸è®­ç»ƒè¿‡ç¨‹ä¸åŒçš„ç›¸ä¼¼ä»»åŠ¡ã€‚Ex-MADDPG ç®—æ³•å¯ä»¥ä¸€æ¬¡è®­ç»ƒå¹¶åº”ç”¨äºå¤§é‡çš„ä»»åŠ¡åˆ†é…åœºæ™¯ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿ RL/DRL ç®—æ³•å¯æ‰©å±•æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒEx-MADDPG ç®—æ³•åœ¨èµ‹å€¼æ€§èƒ½ã€å®¹é”™æ€§å’Œå®æ—¶æ€§æ–¹é¢å‡ä¼˜äºåŒˆç‰™åˆ©ç®—æ³•ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œåœ¨æ•°é‡å’Œä»»åŠ¡æ‰©å±•çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€ææ–¹æ³•åœ¨æ— äººæœºé›†ç¾¤ç›®æ ‡æ”»å‡»åœºæ™¯ä¸‹çš„ä»¿çœŸå’Œå®é™…å®éªŒä¸­å‡è¢«è¯æ˜æ˜¯å¯è¡Œå’Œæœ‰æ•ˆçš„ã€‚
In this paper, we propose a scalable reinforcement learning algorithm to address the task assignment problem in variable scenarios, with a particular focus on UAV formation planning. While the current implementation uses the Boids algorithm for formation flying, the UAV formation algorithm is not presented in detail. Therefore, future work will concentrate on the design and implementation of advanced formation planning algorithms to improve the efficiency of target detection and task assignment.
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è§£å†³å¯å˜åœºæ™¯ä¸­çš„ä»»åŠ¡åˆ†é…é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨æ— äººæœºç¼–é˜Ÿè§„åˆ’ã€‚è™½ç„¶å½“å‰çš„å®ç°ä½¿ç”¨ Boids ç®—æ³•è¿›è¡Œç¼–é˜Ÿé£è¡Œï¼Œä½†å¹¶æœªè¯¦ç»†ä»‹ç»æ— äººæœºç¼–é˜Ÿç®—æ³•ã€‚å› æ­¤ï¼Œæœªæ¥çš„å·¥ä½œå°†é›†ä¸­åœ¨é«˜çº§ç¼–é˜Ÿè§„åˆ’ç®—æ³•çš„è®¾è®¡å’Œå®ç°ä¸Šï¼Œä»¥æé«˜ç›®æ ‡æ£€æµ‹å’Œä»»åŠ¡åˆ†é…çš„æ•ˆç‡ã€‚

Abbreviations

|   Short   |                           Full                            |
| :-------: | :-------------------------------------------------------: |
|    MPE    |             Multi-Agent ParticleÂ Environment              |
|    UAV    |                  Unmanned AerialÂ Vehicle                  |
|    GA     |                     GeneticÂ Algorithm                     |
|    SA     |                    SimulatedÂ Annealing                    |
|    ACO    |             Ant Colony OptimizationÂ algorithm             |
|    PSO    |           Particle Swarm OptimizationÂ algorithm           |
|    GW     |                         GreyÂ Wolf                         |
|    ICP    |                  Iterative ClosestÂ Point                  |
|    DRL    |                Deep ReinforcementÂ Learning                |
|    RL     |                  ReinforcementÂ Learning                   |
|    DL     |                       DeepÂ Learning                       |
|    DQN    |                      Deep QÂ Network                       |
|   MDPs    |                 Markov DecisionÂ Processes                 |
|   LSTM    |                  Long Short-TermÂ Memory                   |
|    PG     |                      PolicyÂ Gradient                      |
|   DDPG    |            Deep Deterministic PolicyÂ Gradient             |
|  MADDPG   |      Multi-Agent Deep Deterministic PolicyÂ Gradient       |
| ms-MADDPG |                   Mean SimulatedÂ MADDPG                   |
| Ex-MADDPG | Extensible Multi-Agent Deep Deterministic PolicyÂ Gradient |

## 7 References

[1]: https://doi.org/10.1177/0278364913496484

Korsah, G.A.; Stentz, A.; Dias, M.B. A comprehensive taxonomy for multi-robot task allocation.Int. J. Robot. Res.**2013**,32, 1495â€“1512. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+comprehensive+taxonomy+for+multi-robot+task+allocation&author=Korsah,+G.A.&author=Stentz,+A.&author=Dias,+M.B.&publication_year=2013&journal=Int.+J.+Robot.+Res.&volume=32&pages=1495â€“1512&doi=10.1177/0278364913496484)] [[CrossRef](https://doi.org/10.1177/0278364913496484)]

1. Korsah, G.A.; Stentz, A.; Dias, M.B. A comprehensive taxonomy for multi-robot task allocation.Int. J. Robot. Res.**2013**,32, 1495â€“1512. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+comprehensive+taxonomy+for+multi-robot+task+allocation&author=Korsah,+G.A.&author=Stentz,+A.&author=Dias,+M.B.&publication_year=2013&journal=Int.+J.+Robot.+Res.&volume=32&pages=1495â€“1512&doi=10.1177/0278364913496484)] [[CrossRef](https://doi.org/10.1177/0278364913496484)]
2. Ahner, D.K.; Parson, C.R. Optimal multi-stage allocation of weapons to targets using adaptive dynamic programming.Optim. Lett.**2015**,9, 1689â€“1701. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Optimal+multi-stage+allocation+of+weapons+to+targets+using+adaptive+dynamic+programming&author=Ahner,+D.K.&author=Parson,+C.R.&publication_year=2015&journal=Optim.+Lett.&volume=9&pages=1689â€“1701&doi=10.1007/s11590-014-0823-x)] [[CrossRef](https://doi.org/10.1007/s11590-014-0823-x)]
3. Zhao, Z.; Liu, S.; Zhou, M.C.; Abusorrah, A. Dual-objective mixed integer linear program and memetic algorithm for an industrial group scheduling problem.IEEE/CAA J. Autom. Sin.**2020**,8, 1199â€“1209. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Dual-objective+mixed+integer+linear+program+and+memetic+algorithm+for+an+industrial+group+scheduling+problem&author=Zhao,+Z.&author=Liu,+S.&author=Zhou,+M.C.&author=Abusorrah,+A.&publication_year=2020&journal=IEEE/CAA+J.+Autom.+Sin.&volume=8&pages=1199â€“1209)]
4. Crouse, D.F. On implementing 2d rectangular assignment algorithms.IEEE Trans. Aerosp. Electron. Syst.**2016**,52, 1679â€“1696. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=On+implementing+2d+rectangular+assignment+algorithms&author=Crouse,+D.F.&publication_year=2016&journal=IEEE+Trans.+Aerosp.+Electron.+Syst.&volume=52&pages=1679â€“1696&doi=10.1109/TAES.2016.140952)] [[CrossRef](https://doi.org/10.1109/TAES.2016.140952)]
5. Holland, J.H.Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence; MIT Press: Cambridge, MA, USA, 1992; Volume 52, pp. 1679â€“1696. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Adaptation+in+Natural+and+Artificial+Systems:+An+Introductory+Analysis+with+Applications+to+Biology,+Control,+and+Artificial+Intelligence&author=Holland,+J.H.&publication_year=1992)]
6. Tanha, M.; Shirvani, M.H.; Rahmani, A.M. A hybrid meta-heuristic task scheduling algorithm based on genetic and thermodynamic simulated annealing algorithms in cloud computing environments.Neural Comput. Appl.**2021**,33, 16951â€“16984. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+hybrid+meta-heuristic+task+scheduling+algorithm+based+on+genetic+and+thermodynamic+simulated+annealing+algorithms+in+cloud+computing+environments&author=Tanha,+M.&author=Shirvani,+M.H.&author=Rahmani,+A.M.&publication_year=2021&journal=Neural+Comput.+Appl.&volume=33&pages=16951â€“16984)]
7. Wu, X.; Yin, Y.; Xu, L.; Wu, X.; Meng, F.; Zhen, R. Multi-uav task allocation based on improved genetic algorithm.IEEE Access**2021**,52, 100369â€“100379. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Multi-uav+task+allocation+based+on+improved+genetic+algorithm&author=Wu,+X.&author=Yin,+Y.&author=Xu,+L.&author=Wu,+X.&author=Meng,+F.&author=Zhen,+R.&publication_year=2021&journal=IEEE+Access&volume=52&pages=100369â€“100379)]
8. Martin, J.G.; Frejo, J.R.D.; GarcÃ­a, R.A.; Camacho, E.F. Multi-robot task allocation problem with multiple nonlinear criteria using branch and bound and genetic algorithms.Intell. Serv. Robot.**2021**,14, 707â€“727. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Multi-robot+task+allocation+problem+with+multiple+nonlinear+criteria+using+branch+and+bound+and+genetic+algorithms&author=Martin,+J.G.&author=Frejo,+J.R.D.&author=GarcÃ­a,+R.A.&author=Camacho,+E.F.&publication_year=2021&journal=Intell.+Serv.+Robot.&volume=14&pages=707â€“727&doi=10.1007/s11370-021-00393-4)] [[CrossRef](https://doi.org/10.1007/s11370-021-00393-4)]
9. Abidin Ã‡il, Z.; Mete, S.; Serin, F. Robotic disassembly line balancing problem: A mathematical model and ant colony optimization approach.Appl. Math. Model.**2020**,86, 335â€“348. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Robotic+disassembly+line+balancing+problem:+A+mathematical+model+and+ant+colony+optimization+approach&author=Abidin+Ã‡il,+Z.&author=Mete,+S.&author=Serin,+F.&publication_year=2020&journal=Appl.+Math.+Model.&volume=86&pages=335â€“348)]
10. Gao, S.; Wu, J.; Ai, J. Multi-uav reconnaissance task allocation for heterogeneous targets using grouping ant colony optimization algorithm.Soft Comput.**2021**,25, 7155â€“7167. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Multi-uav+reconnaissance+task+allocation+for+heterogeneous+targets+using+grouping+ant+colony+optimization+algorithm&author=Gao,+S.&author=Wu,+J.&author=Ai,+J.&publication_year=2021&journal=Soft+Comput.&volume=25&pages=7155â€“7167&doi=10.1007/s00500-021-05675-8)] [[CrossRef](https://doi.org/10.1007/s00500-021-05675-8)]
11. Du, P.; Tang, Z.; Sun, Y. An object-oriented multi-role ant colony optimization algorithm for solving TSP problem.Control Decis.**2014**,29, 1729â€“1736. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=An+object-oriented+multi-role+ant+colony+optimization+algorithm+for+solving+TSP+problem&author=Du,+P.&author=Tang,+Z.&author=Sun,+Y.&publication_year=2014&journal=Control+Decis.&volume=29&pages=1729â€“1736&doi=10.13195/j.kzyjc.2013.1173)] [[CrossRef](https://doi.org/10.13195/j.kzyjc.2013.1173)]
12. Wei, C.; Ji, Z.; Cai, B. Particle swarm optimization for cooperative multi-robot task allocation: A multi-objective approach.IEEE Robot. Autom. Lett.**2020**,5, 2530â€“2537. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Particle+swarm+optimization+for+cooperative+multi-robot+task+allocation:+A+multi-objective+approach&author=Wei,+C.&author=Ji,+Z.&author=Cai,+B.&publication_year=2020&journal=IEEE+Robot.+Autom.+Lett.&volume=5&pages=2530â€“2537&doi=10.1109/LRA.2020.2972894)] [[CrossRef](https://doi.org/10.1109/LRA.2020.2972894)]
13. Li, W.; Zhang, W. Method of tasks allocation of multi-UAVs based on particles swarm optimization.Control Decis.**2010**,25, 1359â€“1363. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Method+of+tasks+allocation+of+multi-UAVs+based+on+particles+swarm+optimization&author=Li,+W.&author=Zhang,+W.&publication_year=2010&journal=Control+Decis.&volume=25&pages=1359â€“1363&doi=10.13195/j.cd.2010.09.82.liw.023)] [[CrossRef](https://doi.org/10.13195/j.cd.2010.09.82.liw.023)]
14. Chen, X.; Liu, Y.; Yin, L.; Qi, L. Cooperative task assignment and track planning for multi-uav attack mobile targets.J. Intell. Robot. Syst.**2020**,100, 1383â€“1400. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Cooperative+task+assignment+and+track+planning+for+multi-uav+attack+mobile+targets&author=Chen,+X.&author=Liu,+Y.&author=Yin,+L.&author=Qi,+L.&publication_year=2020&journal=J.+Intell.+Robot.+Syst.&volume=100&pages=1383â€“1400)]
15. Zhao, M.; Li, D. Collaborative task allocation of heterogeneous multi-unmanned platform based on a hybrid improved contract net algorithm.IEEE Access**2021**,29, 78936â€“78946. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Collaborative+task+allocation+of+heterogeneous+multi-unmanned+platform+based+on+a+hybrid+improved+contract+net+algorithm&author=Zhao,+M.&author=Li,+D.&publication_year=2021&journal=IEEE+Access&volume=29&pages=78936â€“78946&doi=10.1109/ACCESS.2021.3084238)] [[CrossRef](https://doi.org/10.1109/ACCESS.2021.3084238)]
16. Chen, P.; Yan, F.; Liu, Z.; Cheng, G. Communication-constrained task allocation of heterogeneous UAVs.Acta Aeronaut.**2021**,42, 313â€“326. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Communication-constrained+task+allocation+of+heterogeneous+UAVs&author=Chen,+P.&author=Yan,+F.&author=Liu,+Z.&author=Cheng,+G.&publication_year=2021&journal=Acta+Aeronaut.&volume=42&pages=313â€“326)]
17. Bertsekas, D.P. The auction algorithm: A distributed relaxation method for the assignment problem.Ann. Oper. Res.**1988**,14, 105â€“123. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+auction+algorithm:+A+distributed+relaxation+method+for+the+assignment+problem&author=Bertsekas,+D.P.&publication_year=1988&journal=Ann.+Oper.+Res.&volume=14&pages=105â€“123&doi=10.1007/BF02186476)] [[CrossRef](https://doi.org/10.1007/BF02186476)]
18. Di, B.; Zhou, R.; Ding, Q. Distributed coordinated heterogeneous task allocation for unmanned aerial vehicles.Control Decis.**2013**,28, 274â€“278. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Distributed+coordinated+heterogeneous+task+allocation+for+unmanned+aerial+vehicles&author=Di,+B.&author=Zhou,+R.&author=Ding,+Q.&publication_year=2013&journal=Control+Decis.&volume=28&pages=274â€“278&doi=10.13195/j.cd.2013.02.117.dib.010)] [[CrossRef](https://doi.org/10.13195/j.cd.2013.02.117.dib.010)]
19. Liao, M.; Chen, Z. Dynamic target assignment method based on multi-agent decentralized cooperative auction.J. Beijing Univ. Aeronaut. Astronaut.**2007**,33, 180â€“183. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Dynamic+target+assignment+method+based+on+multi-agent+decentralized+cooperative+auction&author=Liao,+M.&author=Chen,+Z.&publication_year=2007&journal=J.+Beijing+Univ.+Aeronaut.+Astronaut.&volume=33&pages=180â€“183&doi=10.13700/j.bh.1001-5965.2007.02.012)] [[CrossRef](https://doi.org/10.13700/j.bh.1001-5965.2007.02.012)]
20. Li, X.; Liang, Y. An optimal online distributed auction algorithm for multi-uav task allocation. InLISS 2021; Springer: Singapore, 2013; Volume 28, pp. 537â€“548. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=An+optimal+online+distributed+auction+algorithm+for+multi-uav+task+allocation&author=Li,+X.&author=Liang,+Y.&publication_year=2013&pages=537â€“548)]
21. Duo, N.; Lv, Q.; Lin, H.; Wei, H. Step into High-Dimensional and Continuous Action Space:A Survey on Applications of Deep Reinforcement Learning to Robotics.Control Decis.**2019**,41, 276â€“288. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Step+into+High-Dimensional+and+Continuous+Action+Space:A+Survey+on+Applications+of+Deep+Reinforcement+Learning+to+Robotics&author=Duo,+N.&author=Lv,+Q.&author=Lin,+H.&author=Wei,+H.&publication_year=2019&journal=Control+Decis.&volume=41&pages=276â€“288&doi=10.13973/j.cnki.robot.180336)] [[CrossRef](https://doi.org/10.13973/j.cnki.robot.180336)]
22. Sun, H.; Hu, C.; Zhang, J. Deep reinforcement learning for motion planning of mobile robots.Control Decis.**2021**,36, 1281â€“1292. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Deep+reinforcement+learning+for+motion+planning+of+mobile+robots&author=Sun,+H.&author=Hu,+C.&author=Zhang,+J.&publication_year=2021&journal=Control+Decis.&volume=36&pages=1281â€“1292&doi=10.13195/j.kzyjc.2020.0470)] [[CrossRef](https://doi.org/10.13195/j.kzyjc.2020.0470)]
23. Wu, X.; Liu, S.; Yang, L.; Deng, W.-Q.; Jia, Z.-H. A Gait Control Method for Biped Robot on Slope Based on Deep Reinforcement Learning.Acta Autom. Sin.**2021**,47, 1976â€“1987. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+Gait+Control+Method+for+Biped+Robot+on+Slope+Based+on+Deep+Reinforcement+Learning&author=Wu,+X.&author=Liu,+S.&author=Yang,+L.&author=Deng,+W.-Q.&author=Jia,+Z.-H.&publication_year=2021&journal=Acta+Autom.+Sin.&volume=47&pages=1976â€“1987&doi=10.16383/j.aas.c190547)] [[CrossRef](https://doi.org/10.16383/j.aas.c190547)]
24. Shi, J.; Gao, Y.; Wang, W.; Yu, N.; Ioannou, P.A. Operating electric vehicle fleet for ride-hailing services with reinforcement learning.IEEE Trans. Intell. Transp. Syst.**2019**,21, 4822â€“4834. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Operating+electric+vehicle+fleet+for+ride-hailing+services+with+reinforcement+learning&author=Shi,+J.&author=Gao,+Y.&author=Wang,+W.&author=Yu,+N.&author=Ioannou,+P.A.&publication_year=2019&journal=IEEE+Trans.+Intell.+Transp.+Syst.&volume=21&pages=4822â€“4834&doi=10.1109/TITS.2019.2947408)] [[CrossRef](https://doi.org/10.1109/TITS.2019.2947408)]
25. Yin, Y.; Guo, Y.; Su, Q.; Wang, Z. Task Allocation of Multiple Unmanned Aerial Vehicles Based on Deep Transfer Reinforcement Learning.Drones**2022**,6, 215. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Task+Allocation+of+Multiple+Unmanned+Aerial+Vehicles+Based+on+Deep+Transfer+Reinforcement+Learning&author=Yin,+Y.&author=Guo,+Y.&author=Su,+Q.&author=Wang,+Z.&publication_year=2022&journal=Drones&volume=6&pages=215&doi=10.3390/drones6080215)] [[CrossRef](https://doi.org/10.3390/drones6080215)]
26. Zhou, W.; Zhu, J.; Kuang, M. An unmanned air combat system based on swarm intelligence.Sci. Sin. Inf.**2020**,50, 363â€“374. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=An+unmanned+air+combat+system+based+on+swarm+intelligence&author=Zhou,+W.&author=Zhu,+J.&author=Kuang,+M.&publication_year=2020&journal=Sci.+Sin.+Inf.&volume=50&pages=363â€“374&doi=10.1360/SSI-2019-0196)] [[CrossRef](https://doi.org/10.1360/SSI-2019-0196)]
27. Chu, T.; Wang, J.; CodecÃ , L.; Li, Z. Multi-agent deep reinforcement learning for large-scale traffic signal control.IEEE Trans. Intell. Transp. Syst.**2019**,21, 1086â€“1095. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Multi-agent+deep+reinforcement+learning+for+large-scale+traffic+signal+control&author=Chu,+T.&author=Wang,+J.&author=CodecÃ ,+L.&author=Li,+Z.&publication_year=2019&journal=IEEE+Trans.+Intell.+Transp.+Syst.&volume=21&pages=1086â€“1095&doi=10.1109/TITS.2019.2901791)] [[CrossRef](https://doi.org/10.1109/TITS.2019.2901791)]
28. Shi, W.; Feng, Y.; Cheng, G.; Huang, H.; Huang, J.; Liu, Z.; He, W. Research on Multi-aircraft Cooperative Air Combat Method Based on Deep Reinforcement Learning.Acta Autom. Sin.**2021**,47, 1610â€“1623. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Research+on+Multi-aircraft+Cooperative+Air+Combat+Method+Based+on+Deep+Reinforcement+Learning&author=Shi,+W.&author=Feng,+Y.&author=Cheng,+G.&author=Huang,+H.&author=Huang,+J.&author=Liu,+Z.&author=He,+W.&publication_year=2021&journal=Acta+Autom.+Sin.&volume=47&pages=1610â€“1623&doi=10.16383/j.aas.c201059)] [[CrossRef](https://doi.org/10.16383/j.aas.c201059)]
29. Wang, L.; Wang, W.; Wang, Y.; Hou, S.; Qiao, Y.; Wu, T.; Tao, X. Feasibility of reinforcement learning for UAV-based target searching in a simulated communication denied environment.Sci. China Inf. Sci.**2020**,50, 375â€“395. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Feasibility+of+reinforcement+learning+for+UAV-based+target+searching+in+a+simulated+communication+denied+environment&author=Wang,+L.&author=Wang,+W.&author=Wang,+Y.&author=Hou,+S.&author=Qiao,+Y.&author=Wu,+T.&author=Tao,+X.&publication_year=2020&journal=Sci.+China+Inf.+Sci.&volume=50&pages=375â€“395)]
30. Ma, Y.; Fan, W.; Chang, T. Optimization Method of Unmanned Swarm Defensive Combat Scheme Based on Intelligent Algorithm.Acta Armamentarii**2022**,43, 1415â€“1425. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Optimization+Method+of+Unmanned+Swarm+Defensive+Combat+Scheme+Based+on+Intelligent+Algorithm&author=Ma,+Y.&author=Fan,+W.&author=Chang,+T.&publication_year=2022&journal=Acta+Armamentarii&volume=43&pages=1415â€“1425)]
31. Huang, T.; Cheng, G.; Huang, K.; Huang, J.; Liu, Z. Task assignment method of compound anti-drone based on DQN for multitype interception equipment.Control Decis.**2021**,37, 142â€“150. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Task+assignment+method+of+compound+anti-drone+based+on+DQN+for+multitype+interception+equipment&author=Huang,+T.&author=Cheng,+G.&author=Huang,+K.&author=Huang,+J.&author=Liu,+Z.&publication_year=2021&journal=Control+Decis.&volume=37&pages=142â€“150&doi=10.13195/j.kzyjc.2020.0787)] [[CrossRef](https://doi.org/10.13195/j.kzyjc.2020.0787)]
32. Watkins, C.J.C.H. Learning from Delayed Rewards. Ph.D. Thesis, Kingâ€™s College, London, UK, 1989. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Learning+from+Delayed+Rewards&author=Watkins,+C.J.C.H.&publication_year=1989)]
33. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; Riedmiller, M. Playing atari with deep reinforcement learning.arXiv**2013**, arXiv:1312.5602. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Playing+atari+with+deep+reinforcement+learning&author=Mnih,+V.&author=Kavukcuoglu,+K.&author=Silver,+D.&author=Graves,+A.&author=Antonoglou,+I.&author=Wierstra,+D.&author=Riedmiller,+M.&publication_year=2013&journal=arXiv)]
34. Timothy, L.P.; Jonathan, H.J.; Alexander, P.; Heess, N.M.O.; Erez, T.; Tassa, Y.; Silver, D.; Wierstra, D. Continuous control with deep reinforcement learning.arXiv**2015**, arXiv:1509.02971. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Continuous+control+with+deep+reinforcement+learning&author=Timothy,+L.P.&author=Jonathan,+H.J.&author=Alexander,+P.&author=Heess,+N.M.O.&author=Erez,+T.&author=Tassa,+Y.&author=Silver,+D.&author=Wierstra,+D.&publication_year=2015&journal=arXiv)]
35. Lowe, R.; Wu, Y.I.; Tamar, A.; Harb, J.; Pieter Abbeel, O.; Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments.Adv. Neural Inf. Process. Syst.**2017**,30, 1â€“12. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Multi-agent+actor-critic+for+mixed+cooperative-competitive+environments&author=Lowe,+R.&author=Wu,+Y.I.&author=Tamar,+A.&author=Harb,+J.&author=Pieter+Abbeel,+O.&author=Mordatch,+I.&publication_year=2017&journal=Adv.+Neural+Inf.+Process.+Syst.&volume=30&pages=1â€“12)]
36. Reynolds, C.W. Flocks, Herds and schools: A distributed behavioral model. In Proceedings of the SIGGRAPHâ€™87, Anaheim, CA, USA, 27â€“31 July 1987. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Flocks,+Herds+and+schools:+A+distributed+behavioral+model&conference=Proceedings+of+the+SIGGRAPHâ€™87&author=Reynolds,+C.W.&publication_year=1987)]
37. Bakker, B. Reinforcement learning with long short-term memory.Adv. Neural Inf. Process. Syst.**2001**,14, 1475â€“1482. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Reinforcement+learning+with+long+short-term+memory&author=Bakker,+B.&publication_year=2001&journal=Adv.+Neural+Inf.+Process.+Syst.&volume=14&pages=1475â€“1482)]
38. Mordatch, I.; Abbeel, P. Emergence of grounded compositional language in multi-agent populations. In Proceedings of the AAAI Conference on Artificial Intelligence 2018, New Orleans, LA, USA, 2â€“7 February 2018; Volume 32. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=Emergence+of+grounded+compositional+language+in+multi-agent+populations&conference=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+2018&author=Mordatch,+I.&author=Abbeel,+P.&publication_year=2018)]
39. Preiss, J.A.; Honig, W.; Sukhatme, G.S.; Ayanian, N. Crazyswarm: A large nano-quadcopter swarm. In Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), Singapore, 29 Mayâ€“3 June 2017; pp. 3299â€“3304. [[Google Scholar](<https://scholar.google.com/scholar_lookup?title=Crazyswarm:+A+large+nano-quadcopter+swarm&conference=Proceedings+of+the+2017+IEEE+International+Conference+on+Robotics+and+Automation+(ICRA)&author=Preiss,+J.A.&author=Honig,+W.&author=Sukhatme,+G.S.&author=Ayanian,+N.&publication_year=2017&pages=3299â€“3304&doi=10.1109/ICRA.2017.7989376>)] [[CrossRef](https://doi.org/10.1109/ICRA.2017.7989376)]
40. Besl, P.J.; McKay, N.D. A method for registration of 3-D shapes.IEEE Trans. Pattern Anal. Mach. Intell.**1992**,14, 239â€“256. [[Google Scholar](https://scholar.google.com/scholar_lookup?title=A+method+for+registration+of+3-D+shapes&author=Besl,+P.J.&author=McKay,+N.D.&publication_year=1992&journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&volume=14&pages=239â€“256&doi=10.1109/34.121791)] [[CrossRef](https://doi.org/10.1109/34.121791)]

## Task Allocation

#### Opinion-based Strategy for Distributed Multi-Robot Task Allocation in Swarms of Robots

[link](https://ieeexplore.ieee.org/document/10801579)

基于意见的机器人群分布式多机器人任务分配策略

Opinions of individuals in large groups evolve through interactions with neighbors and the environment, which can be modeled with opinion dynamics. In this paper, we propose a distributed opinion-based strategy for large-scale multi-robot task allocation utilizing the convergence behaviors of opinion dynamics. The strategy relies on the specialized opinion dynamics on the unit sphere for robot task selection. We investigate the convergence behaviors of opinion dynamics in the context of regions of attraction. Simulation results with a swarm of 200 homogeneous robots validate the effectiveness of our proposed strategy.

大群体中个人的观点通过与邻居和环境的互动而演变，这可以用意见动态来建模。在本文中，我们提出了一种基于分布式观点的策略，用于利用观点动力学的收敛行为进行大规模多机器人任务分配。该策略依赖于 unit sphere 上的专业意见动态来选择 robot 任务。我们研究了吸引力区域背景下意见动态的收敛行为。200 个同构机器人集群的仿真结果验证了我们提出的策略的有效性。

I. Introduction
II. Background
III. Problem Formulation
IV. Opinion-based Strategy
V. Convergence Behaviors
VI. Simulation Experiments and Discussions
VII. Conclusion and Future Work

Introduction 介绍

For homogeneous robots to collaborate on multiple tasks, they need to address the multi-robot task allocation problem. This problem can be modeled as a combinatorial optimization problem [1], it is challenging due to discontinuities in the solution space. To overcome this challenge, we propose an alternative approach by introducing opinions into robot states to represent task preferences. This models task allocation as a continuous-time convergence problem of opinions, eliminating the need for an explicit cost function.
对于同构机器人协作处理多个任务，它们需要解决多机器人任务分配问题。这个问题可以建模为组合优化问题 [1] ，由于解空间的不连续性，它具有挑战性。为了克服这一挑战，我们提出了一种替代方法，将 opinion 引入 robot 状态来表示任务偏好。这将任务分配建模为意见的连续时间收敛问题，无需显式成本函数。

Inspired by opinion dynamics in social networks, where individuals form consensus or dissensus through exchanging opinions [2], we leverage our previous work on convergence behaviors and stability [3]–​[5] to propose an opinion-based strategy for distributed multi-robot task allocation. Robots’ task preferences are modeled as vectors evolving continuously on the unit sphere, based on their prior information. Tasks are modeled as virtual robots with fixed opinions, participating in opinion exchanges with neighboring robots.
受社交网络中意见动态的启发，个人通过交换意见 [2] 形成共识或不共识，我们利用我们之前在收敛行为和稳定性 [3] 方面的工作—— [5] 提出了一种基于意见的分布式多机器人任务分配策略。机器人的任务首选项被建模为基于其先验信息在单位球体上不断演变的向量。任务被建模为具有固定意见的虚拟机器人，参与与相邻机器人的意见交流。

Driven by a specially designed interaction control, the robots can only communicate with local neighbors (no centralized communication or broadcasting) and their opinions converge to different stable equilibria, determining the task assignments among robots. Discussions on convergence behaviors based on regions of attraction are provided for stability guarantee. In simulations with 200 homogeneous robots and various initial scenarios, our strategy outperforms a local voting strategy, showing better performance and adaptivity in terms of average distance to selected tasks when robots have limited information about task locations.
在专门设计的交互控制驱动下，机器人只能与本地邻居通信（没有集中通信或广播），它们的意见会汇聚到不同的稳定均衡中，从而决定机器人之间的任务分配。提供了基于吸引力区域的收敛行为的讨论，以保证稳定性。在使用 200 个同构机器人和各种初始场景的模拟中，我们的策略优于本地投票策略，当机器人对任务位置的信息有限时，在与选定任务的平均距离方面表现出更好的性能和适应性。

Our major contributions in this paper are as follows: 1) Creating a scalable task allocation algorithm based on the convergence behaviors of specially designed opinion dynamics. 2) Providing detailed discussions on convergence behaviors with stability guarantee; 3) Validating the proposed strategy with comparisons to a local voting strategy.

我们在本文中的主要贡献如下：1） 基于专门设计的意见动态的收敛行为创建一个可扩展的任务分配算法。2） 在保证稳定性的情况下，对收敛行为进行详细讨论;3） 通过与当地投票策略的比较来验证拟议的策略。

#### Behavior Tree Capabilities for Dynamic Multi-Robot Task Allocation with Heterogeneous Robot Teams

异构机器人团队动态多机器人任务分配的行为树功能

[link](https://ieeexplore.ieee.org/document/10610515)

While individual robots are becoming increasingly capable, the complexity of expected missions increases exponentially in comparison. To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years. Making effective use of the robots and their unique skills in a team is challenging. Dynamic runtime conditions often make static task allocations infeasible, requiring a dynamic, capability-aware allocation of tasks to team members. To this end, we propose and implement a system that allows a user to specify missions using Behavior Trees (BTs), which can then, at runtime, be dynamically allocated to the current robot team. The system allows to statically model an individual robot’s capabilities within our ros_bt_py BT framework. It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team. The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments. To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics. Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition.

虽然单个机器人的能力越来越强，但相比之下，预期任务的复杂性呈指数级增长。为了应对这种复杂性，异构机器人团队近年来已成为一个重要的研究兴趣。在团队中有效利用机器人及其独特技能是一项挑战。动态运行时条件通常使静态任务分配不可行，需要将任务动态、功能感知分配给团队成员。为此，我们提出并实现了一个系统，该系统允许用户使用行为树 （BT） 指定任务，然后可以在运行时将其动态分配给当前的机器人团队。该系统允许在我们的 ros_bt_py BT 框架中对单个机器人的能力进行静态建模。它提供了一个运行时拍卖系统，可以将任务动态分配给当前团队中最有能力的机器人。该系统利用效用值和前提条件来确保分配提高整体任务执行质量，同时防止错误分配。为了评估该系统，我们模拟了一个由三个异构机器人组成的团队的发现和净化任务，并分析了利用率和整体任务时间作为指标。我们的结果表明，我们的系统可以提高团队的整体效率，同时允许直观的任务规范和团队组成的灵活性。

I.
Introduction
II.
Capabilities in Behavior Trees for MRTA
III.
Implementation
IV.
Evaluation
V.
Conclusions and Future Works

行为树作为一种模块化的、易于使用的方法，已经出现，用于指定机器人的行为[2], [3]。Ogren [4] 和 Marzinotto 等人 [5] -[9] 的著作提出了机器人行为树的基本概念。很早以前，它们也被提出用于多机器人系统。Colledanchise 等人 [10] 概述了如何利用 BT 控制多机器人设置。Jeong 等人[11]描述了一种利用单一 BT 控制多个移动机器人的集中式方法。出于类似目的，我们也提出了一种用于控制异构机器人团队的 BT 框架，称为 ros\_-bt_py[12]。虽然前面提到的方法允许远程执行静态、预定义的子树，但还没有一种方法能处理机器人团队中的异构或动态能力。针对 MRTAP 的非行为树方法更侧重于机器人能力建模方法。文献[13]和[14]中提出的 ASyMTRe 框架通过自动合成给定团队和任务集的传感器、处理器和执行器之间的数据流图来促进协作。在 CoMutaR 框架[15]中，这一原则被扩展到考虑资源限制和运行时的动态变化，但代价是相当大的建模开销。Guerin 等人[16]提出了一种与机器人无关的任务 BT 格式，以及一个机器人能力模型。该方法主要针对单个机器人的工业应用，在这些应用中，任务的描述应独立于执行任务的机器人。将任务分配给能力随时间下降的团队的方法在 [17] 中有所涉及。虽然这种方法的团队组成是动态的，但需要一组静态的可用能力。文献[18]解决了类似的问题，将能力和任务适用性建模为矢量，从而采用概率方法确定任务运行期间的最佳任务分配。

Capabilities in Behavior Trees for MRTA
MRTA 行为树中的功能

Our previously proposed Behavior Tree framework [12] implements common BT functionality such as flow control, ticking, and sub-trees to model individual capabilities. Additionally it can be used to design complete missions by enabling long running tasks and utility evaluation of sub-trees. Finally, the possibility to execute parts of the BT on a distant system enables multi-robot task allocation for a team of heterogeneous robots as the mission can be spread over a group of robots based on their individual skills. However, the proposed method relies on shovables. This special decorator node explicitly marks sub-trees suitable for remote execution, assigning them at the startup of the mission tree. This limits the possibility of changing the assignment during runtime, restricts reactivity of the parameters and results, and limits the complexity of the sub-trees. We therefore extended our previous work by specifically adding capabilities to the model and enable a dynamic distribution via auction of these within the team. All following definitions build upon the concepts & definitions introduced in [12].
我们之前提出的行为树框架 [12] 实现了常见的 BT 功能，例如流控制、更新和子树，以对各个功能进行建模。此外，它还可用于通过启用长时间运行的任务和子树的实用程序评估来设计完整的任务。最后，在远程系统上执行 BT 的某些部分的可能性使异构机器人团队能够进行多机器人任务分配，因为任务可以根据机器人的个人技能分配给一组机器人。然而，所提出的方法依赖于 shovables。这个特殊的装饰器节点显式标记适合远程执行的子树，并在任务树启动时分配它们。这限制了在运行时更改赋值的可能性，限制了参数和结果的反应性，并限制了子树的复杂性。因此，我们扩展了之前的工作，专门为模型添加了功能，并通过在团队内部拍卖这些功能来实现动态分配。以下所有定义都建立在中 [12] 引入的概念和定义之上。

#### Fast Task Allocation of Heterogeneous Robots With Temporal Logic and Inter-Task Constraints
具有时间逻辑和任务间约束的异构机器人的快速任务分配

This work develops a fast task allocation framework for heterogeneous multi-robot systems subject to both temporal logic and inter-task constraints. The considered inter-task constraints include unrelated tasks, compatible tasks, and exclusive tasks. To specify such inter-task relationships, we extend conventional atomic proposition to batch atomic propositions, which gives rise to the LTLT formula. The Task Batch Planning Decision Tree (TB-PDT) is then developed, which is a variant of conventional decision tree specialized for temporal logic and inter-task constraints. The TB-PDT is built incrementally to represent the task progress and does not require sophisticated product automaton, which significantly reduces the search space. Based on TB-PDT, the search algorithm, namely Intensive Inter-task Relationship Tree Search (IIRTS), is developed for the fast task allocation of heterogeneous multi-robot systems. It is shown that the solution time of finding a satisfactory task allocation grows almost quadratically with the number of automaton states. Extensive simulation and experiment demonstrate the validity, the effectiveness, and the transferability of IIRTS.
这项工作为受时间逻辑和任务间约束的异构多机器人系统开发了一个快速任务分配框架。考虑的任务间约束包括不相关的任务、兼容任务和独占任务。为了指定这种任务间关系，我们将传统的原子命题扩展到批量原子命题，这产生了 LTL T 公式。然后开发任务批处理规划决策树 （TB-PDT），它是传统决策树的变体，专门用于时间逻辑和任务间约束。TB-PDT 是增量构建的，用于表示任务进度，不需要复杂的产品自动化，这大大减少了搜索空间。基于 TB-PDT，开发了搜索算法，即强化任务间关系树搜索 （IIRTS），用于异构多机器人系统的快速任务分配。结果表明，找到令人满意的任务分配的求解时间几乎随着自动机状态的数量而呈二次方增长。广泛的模拟和实验证明了 IIRTS 的有效性、有效性和可转移性。

I.
Introduction
II.
Preliminaries and Problem Formulation
III.
Task Allocation
IV.
Experiment
V.
Conclusion

Introduction介绍

Imagine a medical scenario where a heterogeneous multi-robot system with different capabilities is desired to provide medical services in a hospital environment, as shown in Fig. 1. Example services, such as medicine delivery and navigation tasks, can be specified by temporal logic constraints. In addition to the task-level constraints, there might exist inter-task constraints. For instance, the robots that deliver medicine among patient rooms are not allowed to be reassigned with tasks operating in the therapeutic department due to potential infection risks. Such an inter-task constraint can indicate a class of mutually exclusive tasks. The inter-task constraints can also indicate compatible tasks, e.g., the medicine delivery tasks among patient rooms are compatible and the involved robots can be assigned freely based on the task requirement. However, few existing multi-robot task allocation (MRTA) problems jointly consider both temporal logic and inter-task constraints. In addition, the task is desired to be allocated fast to enable real-time implementation. Hence, this work is particularly motivated to develop a fast task allocation framework for the heterogeneous multi-robot system subject to both temporal logic and inter-task constraints.
想象一下这样一个医疗场景，需要一个具有不同能力的异构多机器人系统在医院环境中提供医疗服务，如图 1 所示。 1 。示例服务（例如药物递送和导航任务）可以通过时间逻辑约束来指定。除了任务级别的约束之外，还可能存在任务间的约束。例如，由于潜在的感染风险，不允许将病房内送药的机器人重新分配在治疗部门执行的任务。这样的任务间约束可以指示一类互斥的任务。任务间约束还可以表示兼容的任务，例如，病房之间的药物输送任务是兼容的，并且可以根据任务要求自由分配所涉及的机器人。然而，现有的多机器人任务分配 （MRTA） 问题很少同时考虑时间逻辑和任务间约束。此外，还需要快速分配任务以实现实时实施。因此，这项工作特别有动力为受时间逻辑和任务间约束的异构多机器人系统开发一个快速任务分配框架。

In this work, we develop a novel task allocation framework that can handle heterogeneous multi-robot systems under both temporal logic and inter-task constraints. Specifically, we consider three types of inter-task constraints, i.e., unrelated tasks, compatible tasks, and exclusive tasks. Compatible tasks are referred to the tasks with the same type and the same assignment requirements (e.g., the delivery tasks with the same delivery requirements among patient rooms), while the exclusive tasks restrict the task allocation to the involved robots (e.g., the batch of delivery robots in patient rooms cannot participate in any tasks in the therapeutic department). To specify such inter-task relationships, we extend conventional atomic proposition to batch atomic propositions, which gives rise to the LTLT formula. The Task Batch Planning Decision Tree (TB-PDT) is then developed, which is a variant of conventional decision tree specialized for temporal logic and inter-task constraints. Based on TB-PDT, the search algorithm, namely Intensive Inter-task Relationship Tree Search (IIRTS), is developed for fast task allocation that takes into account both temporal logic and inter-task constraints for the heterogeneous multi-robot system. Extensive simulation and experiment demonstrate the validity, the effectiveness, and the transferability of IIRTS.
在这项工作中，我们开发了一种新的任务分配框架，可以在时间逻辑和任务间约束下处理异构多机器人系统。具体来说，我们考虑了三种类型的任务间约束，即不相关任务、兼容任务和独占任务。兼容任务是指相同类型、相同分配要求的任务（例如，病房之间具有相同配送要求的配送任务），而独占任务则限制了对涉及机器人的任务分配（例如，病房中的这批配送机器人不能参与治疗科室的任何任务）。为了指定这种任务间关系，我们将传统的原子命题扩展到批量原子命题，这产生了 LTL T 公式。然后开发任务批处理规划决策树 （TB-PDT），它是传统决策树的变体，专门用于时间逻辑和任务间约束。基于 TB-PDT，开发了搜索算法，即密集任务间关系树搜索 （IIRTS），用于快速任务分配，同时考虑了异构多机器人系统的时间逻辑和任务间约束。广泛的模拟和实验证明了 IIRTS 的有效性、有效性和可转移性。

Contributions: First, the developed IIRTS is effective in handling both temporal logic and inter-task constraints. The TB-PDT encodes the automaton states generated by LTLT formula and task progression in a hierarchical tree, which is built incrementally during mission operation. Leveraging the tree structure, satisfactory task allocations can be searched effectively. Second, the developed IIRTS can search for a satisfactory planning much faster. By encoding the system and LTLT formula as node properties, the TB-PDT does not require sophisticated product automaton, which significantly reduces the search space. It is shown that the computation time of finding a satisfactory task allocation grows almost quadratically with the number of automaton states. In addition, the developed task allocation framework can be conveniently plugged in and used with existing path planning algorithms, such as Probabilistic Roadmaps Methods (PRM) [18], Rapidly exploring Random trees (RRT) [19], RRT* [20], etc, which indicates that our framework is highly flexible and adaptable for various scenarios.
贡献：首先，开发的 IIRTS 在处理时间逻辑和任务间约束方面都很有效。TB-PDT 将 LTL T 公式和任务进展生成的自动机状态编码在一个分层树中，该树是在任务运行期间逐步构建的。利用树结构，可以有效地搜索到令人满意的任务分配。其次，开发的 IIRTS 可以更快地搜索到令人满意的规划。通过将系统和 LTL T 公式编码为节点属性，TB-PDT 不需要复杂的产品自动机，这大大减少了搜索空间。结果表明，找到令人满意的任务分配的计算时间几乎随着自动机状态的数量而呈二次方增长。此外，开发的任务分配框架可以方便地插入并与现有的路径规划算法一起使用，例如概率路线图方法（PRM）、 [18] 快速探索随机树（RRT）、 [19] RRT* [20] 等，这表明我们的框架具有高度的灵活性和适应性，适用于各种场景。

#### Distributed Matching-By-Clone Hungarian-Based Algorithm for Task Allocation of Multiagent Systems
基于匈牙利的分布式克隆匹配算法，用于多智能体系统的任务分配

In this article, we present a novel approach, namely distributed matching-by-clone hungarian-based algorithm (DMCHBA), to multiagent task-allocation problems, in which the number of agents is smaller than the number of tasks. The proposed DMCHBA assumes that agents employ an implicit coordination mechanism and consists of two iterative phases, i.e., the communication phase and the assignment phase. In the communication phase, agents communicate with their connected neighbors and exchange their local knowledge base until they converge on the global knowledge base. In the assignment phase, each agent builds a squared cost matrix by cloning agents and adding pseudotasks when necessary, and applying the Hungarian method for task allocation. A local planning algorithm is then applied to identify the order of task execution for an agent. The proposed DMCHBA is proven to produce conflict-free assignments among agents in finite time. We compare the performance of DMCHBA with the consensus-based bundle algorithm, the distributed recursive Hungarian-based algorithms, and the cluster-based Hungarian algorithm (CBHA) in Monte-Carlo simulations with different numbers of agents and tasks. The numerical results reveal the superior convergence and optimality of DMCHBA over all other selected algorithms.
在本文中，我们提出了一种新颖的方法，即基于克隆匈牙利的分布式匹配算法 （DMCHBA），以解决多智能体任务分配问题，其中智能体的数量小于任务的数量。拟议的 DMCHBA 假设代理采用隐式协调机制，由两个迭代阶段组成，即通信阶段和分配阶段。在通信阶段，代理与他们连接的邻居通信并交换他们的本地知识库，直到他们汇聚到全球知识库。在分配阶段，每个代理通过克隆代理并在必要时添加伪任务，并应用 Hungarian 方法进行任务分配来构建一个平方成本矩阵。然后应用本地规划算法来识别代理的任务执行顺序。事实证明，拟议的 DMCHBA 可以在有限时间内在代理之间产生无冲突的任务。我们将 DMCHBA 的性能与基于共识的捆绑算法、基于匈牙利的分布式递归算法和基于集群的匈牙利算法 （CBHA） 在蒙特卡洛模拟中具有不同数量的代理和任务进行了比较。数值结果表明，DMCHBA 的收敛性和最优性优于所有其他选定的算法。

I.
Introduction
II.
Preliminaries
III.
Distributed Matching-By-Clone Hungarian-Based Algorithm
IV.
Conflict-Free Resolution of DMCHBA
V.
Convergence
VI.
Numerical Results and Discussion
VII.
Conclusion

#### Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation

[link](https://ieeexplore.ieee.org/document/10611094)

用于多机器人任务分配的 Bigraph 匹配加权与 Learnt 激励函数

Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots’ state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend.
大多数现实世界的多机器人任务分配 （MRTA） 问题都需要快速高效的决策，这通常是使用启发式辅助方法实现的，例如遗传算法、基于拍卖的方法和二分图匹配方法。与基于 MRTA 的端到端（学习）神经网络策略相比，这些方法通常采用一种形式，具有更好的可解释性。但是，如果问题太复杂，则推导出合适的启发式方法可能很乏味、有风险，并且在某些情况下不切实际。这就提出了一个问题：这些启发式方法可以学习吗？为此，本文特别开发了一个图强化学习 （GRL） 框架，以学习 MRTA 的二分图匹配方法的启发式或激励措施。具体来说，Capsule Attention 策略模型用于学习如何在将任务集连接到机器人集的二分图中对任务/机器人配对（边）进行加权。最初的胶囊注意力网络架构从根本上进行了修改，增加了机器人状态图的编码，以及两个基于 Multihead Attention 的解码器，其输出用于构建一个对数正态分布矩阵，从中可以得出正二元图权重。这种由 GRL 衍生的激励增强的新 bigraph 匹配方法的性能与使用专家指定的启发式方法的原始 bigraph 匹配方法相当，前者提供了显着的稳健性优势。在训练过程中，发现学习到的激励策略最初更接近专家指定的激励，然后略微偏离其趋势。

I.
Introduction
II.
MRTA - Collective Transport (MRTA-CT)
III.
Incentive (Weight) learning framework
IV.
Experimental Evaluation
V.
Conclusions

There exists a notable body of work on heuristic-based approaches to solving MRTA problems more efficiently, e.g., using genetic algorithm [5], graph-based methods [2], [6], [7], and auction-based methods [8]). Often the heuristic choices and setting in these approaches are driven by expert experience and intuition. Hence, although they provide some degree of explainability, they leave significant scope for improvement in performance. Moreover, heuristic-based methods are often poor at adapting to complex problem characteristics, or generalizing across different problem scenarios, without tedious hand-crafting of underlying heuristics. As a result, an emerging notion in (fast, near-optimal) decision-making is "can more effective heuristics be automatically (machine) learned from prior operational data or experience gathered by the agent/robot e.g., in simulation" [9]? This raises the fundamental question of whether learned heuristics can match and potentially surpass the performance of human-prescribed heuristics when generalizing across a wide range of problem scenario of similar or varying complexity. We explore this research question in the context of multi-robot task allocation problems of the following type Multi Robot Tasks-Single Task Robots (MR-ST) [10]. In doing so, this paper also provides initial evidence of the potential for exploiting the best of both worlds: explainable structure of graph matching techniques and automated generation of the necessary heuristics through reinforcement learning (RL).

在基于启发式的方法上存在大量工作，以更有效地解决 MRTA 问题，例如，使用遗传算法 [5] 、基于图的方法 [2] 、 [6] 、 [7] 和基于拍卖的方法 [8] ）。通常，这些方法中的启发式选择和设置是由专家经验和直觉驱动的。因此，尽管它们提供了一定程度的可解释性，但它们在性能方面仍有很大的改进空间。此外，基于启发式的方法通常不善于适应复杂的问题特征，或跨不同的问题场景进行泛化，而无需繁琐地手工制作底层启发式方法。因此，在（快速、近乎最佳）决策中，一个新兴的概念是“是否可以从代理/机器人收集的先前作数据或经验（例如，在模拟中）自动（机器）学习更有效的启发式方法” [9] ？这就提出了一个基本问题，即在对相似或不同复杂程度的广泛问题场景进行泛化时，学习的启发式方法是否可以匹配并可能超过人工规定的启发式方法的性能。我们在以下类型的多机器人任务分配问题的背景下探讨了这个研究问题：多机器人任务-单任务机器人 （MR-ST）。 [10] 在此过程中，本文还提供了初步证据，证明可以充分利用两全其美：图形匹配技术的可解释结构和通过强化学习 （RL） 自动生成必要的启发式方法。

Over the past few years, Graph Reinforcement Learning (GRL) methods have emerged as a promising approach to solving combinatorial optimization problems encountered in single and multi-agent planning applications [10], [19] –​[28]. While these methods have shown to generate solutions that can generalize across scenarios drawn from the same distribution and can be executed near instantaneously, they are considered to be black-box methods (thus lacking explainability) and usually do not provide any sort of guarantees.
在过去的几年里，图强化学习 （GRL） 方法已成为解决单智能体和多智能体规划应用中 [10] 遇到的组合优化问题的一种很有前途的方法。 [19] [28] 虽然这些方法已证明可以生成可以在来自同一分布的方案中泛化并且可以几乎立即执行的解决方案，但它们被认为是黑盒方法（因此缺乏可解释性），通常不提供任何形式的保证。

Key Contributions: The overall objective of this paper is to identify an approach to learning the incentive function that can be used by maximum weighted bigraph matching to perform multi-robot task allocation, with performance that is comparable to or better than reported i) bigraph matching techniques that use expert heuristics, and ii) purely RL based solutions. Thus the main contributions of this paper include: 1) Identify the inputs, outputs and structure of the graph neural network (GNN) model that will serve as the incentive function; 2) Integrate the GNN-based incentive with the bigraph matching process in a way that the GNN can be trained by policy gradient approaches over simulated MRTA experiences; 3) Analyze the (dis)similarity of the learned incentives to that of the expert-derived incentives.

主要贡献：本文的总体目标是**确定一种学习激励函数的方法**，该方法可供最大加权双元匹配用于执行多机器人任务分配，其性能与所报道的 i） 使用专家启发式的双元字节匹配技术相当或更好，以及 ii） 纯粹基于 RL 的解决方案。因此，本文的主要贡献包括：1） 确定将用作激励函数的图神经网络 （GNN） 模型的输入、输出和结构;2） 将基于 GNN 的激励与双字母匹配过程相结合，以便 GNN 可以通过模拟 MRTA 体验的政策梯度方法进行训练;3） 分析习得的激励措施与专家衍生的激励措施的相似性（不）相似性。

Paper Outline: The next section summarizes the MRTA problem, its MDP formulation, and the bigraph representation of the MRTA process. Section III describes our proposed GNN architecture for incentive learning and computing the final action (robot/task allocations). Section IV discusses numerical experiments on MRTA problems of different sizes, comparing learning and non-learning methods and analyzing computing time. Section V concludes the paper.
论文大纲：下一节总结了 MRTA 问题、其 MDP 公式以及 MRTA 过程的 bigraph 表示。 Section III 描述了我们提出的用于激励学习和计算最终动作（机器人/任务分配）的 GNN 架构。 Section IV 讨论不同大小的 MRTA 问题的数值实验，比较学习和非学习方法并分析计算时间。 Section V 论文总结道。

#### Dynamic Coalition Formation and Routing for Multirobot Task Allocation via Reinforcement Learning

- [code](https://github.com/marmotlab/DCMRTA)
- [link](https://ieeexplore.ieee.org/document/10611244/)
通过强化学习实现多机器人任务分配的动态联盟形成和路由

Many multi-robot deployments, such as automated construction of buildings, distributed search, or cooperative mapping, often require agents to intelligently coordinate their trajectories and form coalition over a large domain, to complete spatially distributed tasks as quickly as possible. We focus on scenarios involving homogeneous robots, but where tasks vary in the number of agents required to start them. For example, construction robots may need to collaboratively air-lift heavy objects at different locations (e.g., prefabricated rooms, crates of material/equipment), where the weight of each payload defines the required coalition size. To balance the total travel time of the agents and their waiting time (before task initiation), agents need to carefully sequence tasks but also dynamically form/disband coalitions. While simpler problems can be approached using heuristics or optimization, these methods struggle with more complex instances involving large task-to-agent ratios, where frequent coalition changes are needed. In this work, we propose to let agents learn to iteratively build cooperative schedules to solve such problems, by casting the problem in the reinforcement learning framework. Our approach relies on an attention-based neural network, allowing agents to reason about the current state of the system to sequence movement decisions that optimize short-term coalition formation and long-term task scheduling. We further propose a novel leader-follower technique to boost cooperation learning and compare our performance to conventional baselines in a wide variety of scenarios. There, our method closely matches or outperforms the baselines; in particular, it yields higher-quality solutions and is at least 2 orders of magnitude faster than exact solver in cases where frequent coalition updates are required.
许多多机器人部署，例如建筑物的自动建造、分布式搜索或协作测绘，通常需要代理智能地协调其轨迹并在一个大域上形成联盟，以尽快完成空间分布式任务。我们专注于涉及同构机器人的场景，但其中的任务在启动它们所需的代理数量方面有所不同。例如，建筑机器人可能需要在不同位置（例如，预制室、成箱的材料/设备）协同空运重物，其中每个有效载荷的重量决定了所需的联盟大小。为了平衡代理的总旅行时间和他们的等待时间（在任务启动之前），代理需要仔细排序任务，但也需要动态地组建/解散联盟。虽然可以使用启发式或优化来解决更简单的问题，但这些方法难以处理涉及大任务与代理比率的更复杂的实例，在这些情况下需要频繁的联盟更改。在这项工作中，我们建议通过在强化学习框架中铸造问题，让代理学习迭代构建协作时间表来解决这些问题。我们的方法依赖于基于注意力的神经网络，允许代理对系统的当前状态进行推理，以对优化短期联盟形成和长期任务调度的运动决策进行排序。我们进一步提出了一种新的领导者-追随者技术，以促进合作学习，并将我们的表现与各种场景中的传统基线进行比较。 在那里，我们的方法与基线非常匹配或优于基线;特别是，它可以产生更高质量的解，并且在需要频繁更新联盟的情况下，它比 EXACT SOLVER 至少快 2 个数量级。

I.
Introduction
II.
Related Work
III.
Problem formulation
IV.
METHODOLOGY
V.
Experiments
VI.
Conclusion

In this work, we propose a novel reinforcement learning (RL) approach through which agents (i.e., robots) can obtain decentralized policies to solve the ST-MR-TA problem. Specifically, agents learn to reason about their position, the status of all tasks, as well as the position and short-term intent of other agents, to make reactive movement decisions (i.e., which task to travel to and complete next). By choosing to converge to/diverge from tasks, agents naturally form coalitions and disband during task selection, iteratively constructing cooperative schedules. To train this collaboration in such a large multi-agent state-action space, we further propose a mechanism to reduce the decision complexity during training. Specifically, upon task completion, a random "leader" agent is selected from that coalition; this agent selects the next task, and all other "follower" agents are automatically driven to follow it to that new task (up to task requirements). Any agents left then select their next task in the same manner. In doing so, agent decisions are naturally linked, thus boosting cooperation learning. During policy execution, however, each agent uses its own learned policy to make individual decisions (no leader-follow mechanism), which our results show still yields similar performance. Using our approach, agents are able to collaboratively construct high-quality/near-optimal routes much faster than existing optimization methods relying on pre-training and low inference time, making our approach appealing for deployments in dynamic scenarios where frequent replanning might be necessary. We conduct experiments and compare our approach with state-of-the-art methods on randomly generated instances ranging from 10 to 40 agents and 20 to 200 tasks. Our results show that our approach can match or outperform an exact, MIP-based solver [9] while reducing computation times by at least two orders of magnitude, and outperforms heuristics methods in most cases where frequent coalition updates are required.

在本研究中，我们提出了一种新颖的强化学习（RL）方法，通过该方法，智能体（即机器人）可以获得去中心化的策略来解决 ST-MR-TA 问题。具体而言，智能体学会推理自身位置、所有任务的状态以及其他智能体的位置和短期意图，以做出反应性移动决策（即前往并完成哪个任务）。通过选择汇聚或分散于任务，智能体在任务选择过程中自然地形成联盟并在任务完成后解散，迭代构建合作调度计划。为了在如此庞大的多智能体状态-动作空间中训练这种协作，我们进一步提出了一种机制来降低训练期间的决策复杂度。具体来说，在任务完成后，从该联盟中随机选择一个“领导者”智能体；该智能体选择下一个任务，所有其他“跟随者”智能体则自动被驱动跟随其前往新任务（直至满足任务要求）。剩余的智能体随后以相同方式选择其下一个任务。通过这种方式，智能体的决策自然关联，从而促进了合作学习。 在策略执行过程中，每个代理使用其自身学习到的策略做出独立决策（无领导者跟随机制），我们的结果表明这仍能产生相似的性能。采用我们的方法，代理能够比依赖预训练和低推理时间的现有优化方法更快地协作构建高质量/接近最优的路径，这使得我们的方法在可能需要频繁重新规划的动态场景部署中具有吸引力。我们进行了实验，并将我们的方法与最先进的方法在随机生成的实例上进行了比较，这些实例涉及 10 到 40 个代理和 20 到 200 个任务。我们的结果表明，我们的方法能够匹配或超越基于 MIP 的精确求解器 [9] ，同时将计算时间减少至少两个数量级，并且在大多数需要频繁联盟更新的情况下优于启发式方法。

Related Work

A. Task allocation and task scheduling

Korsah et al. [12] expanded on the taxonomy introduced in [4] by incorporating interrelated utilities and constraints of robots and tasks in MRTA problems. This work falls under the category of ST-MR-TA with cross-schedule dependencies (XD) referred to as XD[ST-MR-TA], which indicates that the suitability of an agent for a particular task is not solely determined by its own schedule but also relies on the schedules of other agents within the system. Existing methods to solve XD[ST-MR-TA] problems can be generally categorized into integer programming [10], [13], [9], heuristics [7], [6], and auction-based methods [14], [8]. For example, Korsah et al. [10] first proposed a MIP-based anytime task allocation and scheduling planner for problems with cross-schedule dependencies, where different tasks may have synchronization constraints for initiation and time-window requirements for completion. Recently, Fu et al. proposed a scheduling framework utilizing MIP for heterogeneous robot teams [9], which addresses task decomposition, assignment, and scheduling problems while accounting for uncertainty in agents' abilities and task requirements. However, such MIP-based approaches do not scale well to large problems, as their computing times can reach/exceed tens of minutes on realistic instances, also limiting their application in dynamic scenarios. Different from exact solvers, Ferreira et al. [7] proposed a heuristic method in which they first decompose the task into robot actions and solve the problem as a Vehicle Routing Problem (VRP), thus existing VRP algorithms [15], [16], [17] could be applied. In addition, auction-based methods [14], [8] are often decentralized. However, agents tend to be competitive or greedy when placing their bids, which can result in degraded performance. For more complex cases beyond task decomposition and uncertainty, some studies address constraints like task precedence (i.e., order or priority of task execution) for ST-MR-TA problems [18], [19], [20], [21]. While beyond the scope of our current work, it is worth noting that our agents make reactive decisions while considering task executability. By dynamically changing task availability, agents have the potential to naturally address additional constraints such as task precedence.
Korsah 等人 [12] 在 [4] 中引入的分类法基础上进行了扩展，将机器人和任务在 MRTA 问题中的相互关联的效用和约束纳入其中。这项工作属于具有跨调度依赖关系（XD）的 ST-MR-TA 类别，称为 XD[ST-MR-TA]，这表明一个代理对特定任务的适用性不仅由其自身的调度决定，还依赖于系统中其他代理的调度。解决 XD[ST-MR-TA]问题的现有方法通常可以分为整数规划 [10] 、 [13] 、 [9] ，启发式方法 [7] 、 [6] ，以及基于拍卖的方法 [14] 、 [8] 。例如，Korsah 等人 [10] 首次提出了一种基于 MIP 的随时任务分配和调度规划器，用于处理具有跨调度依赖关系的问题，其中不同任务可能具有启动的同步约束和完成的时间窗口要求。最近，Fu 等人提出了一种利用 MIP 的调度框架，用于异构机器人团队 [9] ，该框架解决了任务分解、分配和调度问题，同时考虑了代理能力和任务需求的不确定性。 然而，此类基于 MIP 的方法在处理大规模问题时扩展性不佳，因为在实际情况中，其计算时间可能达到或超过数十分钟，这也限制了它们在动态场景中的应用。与精确求解器不同，Ferreira 等人 [7] 提出了一种启发式方法，他们首先将任务分解为机器人动作，并将问题作为车辆路径问题（VRP）来解决，从而可以应用现有的 VRP 算法 [15] 、 [16] 、 [17] 。此外，基于拍卖的方法 [14] 、 [8] 通常是去中心化的。然而，代理在出价时往往具有竞争性或贪婪性，这可能导致性能下降。对于超出任务分解和不确定性的更复杂情况，一些研究针对 ST-MR-TA 问题解决了诸如任务优先级（即任务执行的顺序或优先级）等约束 [18] 、 [19] 、 [20] 、 [21] 。虽然超出了我们当前工作的范围，但值得注意的是，我们的代理在考虑任务可执行性的同时做出反应性决策。通过动态改变任务可用性，代理有可能自然地解决诸如任务优先级之类的额外约束。

B. Learning-based routing and scheduling
Traditional optimization methods, while efficient for solving small-scale problems and yielding exact/near-optimal solutions, encounter significant challenges when applied to large-scale instances as the computation time tends to experience exponential growth with problem size. Therefore, many recent approaches have proposed to rely on deep reinforcement learning (dRL) to solve such problems in a data-driven way for rapidly finding high-quality solutions. Narzri et al. first proposed an end-to-end VRP framework [22], which relies on learned attention mechanisms to solve such optimization problems. Cao et al. [23] further proposed an RL-based decentralized sequential decision-making method for the multiple traveling salesman problem (mTSP). These approaches exhibit great scalability and are able to generate near-optimal solutions through a Transformer-style network with very short computation time, particularly for large instances. Recently, Agrawal et al. [24] addressed task assignment in warehouse using an attention dRL approach, but only focused on single-agent tasks without cooperation. Similarly, some works [25], [19] proposed an RL-based scheduler to tackle scheduling problems under precedence constraints, which showed great generalization to different routing and scheduling problems. However, these works only consider agents who work as individuals, and challenges in coalition formation have not been tackled yet.

传统优化方法虽然在解决小规模问题并产生精确/接近最优解方面效率较高，但在应用于大规模实例时面临显著挑战，因为计算时间往往随问题规模呈指数级增长。因此，许多近期方法提出依赖深度强化学习（dRL）以数据驱动的方式解决此类问题，从而快速找到高质量解决方案。Narzri 等人首次提出了一个端到端的 VRP 框架 [22] ，该框架依赖于学习到的注意力机制来解决此类优化问题。Cao 等人 [23] 进一步提出了一种基于 RL 的去中心化顺序决策方法，用于解决多旅行商问题（mTSP）。这些方法展现出极佳的扩展性，并能够通过 Transformer 风格的网络在极短的计算时间内生成接近最优的解决方案，尤其适用于大规模实例。最近，Agrawal 等人 [24] 使用注意力 dRL 方法解决了仓库中的任务分配问题，但仅关注单代理任务，未涉及合作。 同样，一些研究 [25] 、 [19] 提出了基于强化学习（RL）的调度器来解决具有优先约束的调度问题，这些调度器在不同路由和调度问题上展现了良好的泛化能力。然而，这些研究仅考虑了作为个体工作的代理，尚未解决联盟形成中的挑战。

Problem Formulation

We consider a team of n homogeneous agents A = {a1, a2,…, an} starting from a depot k0. These agents need to complete m tasks {k1,k2, …,km} spatially distributed within a given 2D domain, and finally return to the depot k0. Without loss of generality, the domain is normalized to the [0,1] × [0,1] ⊂ ℝ2 area. The depot and tasks are represented as a node set K = {k0, k1,k2,…, km}, each node kj ∈ K located at position (xkj,ykj). Each task kj is associated a requirement ckj and duration wkj, where ckj∈N+ indicates the number of agents required to start it, and wkj∈R+ the time needed for such a coalition (once assembled) to complete the task. Agents are always in one of three states: 1) waiting for other agents to initiate a task, 2) executing a task, or 3) traveling from one task to another, which is performed through a straight line (Euclidean distance between tasks) at constant speed v. A solution is a set of individual agent tours Φ={ϕa1,…,ϕan} that allows agents to complete all tasks based on their requirements and return to the depot, where each tour ϕai=(k0,ki1,ki2,…,k0) is a set of non-repeating node (except for the depot). The makespan of a solution is the longest tour duration among all agents, where individual tour duration is calculated as the sum of the agent's task execution times, travel times, but also waiting times (between its arrival to a task and that of its last taskmate, which depend on the tours of other agents). our objective is to find a solution with minimal makespan.

我们考虑一个由 n 个同质代理 A = {a 1 , a 2 ,…, a n }组成的团队，从仓库 k 0 出发。这些代理需要完成在给定二维区域内空间分布的 m 个任务{k 1 ,k 2 , …,k m }，并最终返回仓库 k 0 。不失一般性，该区域被归一化为[0,1] × [0,1] ⊂ ℝ 2 的面积。仓库和任务被表示为一个节点集 K = {k 0 , k 1 ,k 2 ,…, k m }，每个节点 k ∈ K 位于位置 (xkj,ykj) 。每个任务 k 关联一个需求 ckj 和持续时间 wkj ，其中 ckj∈N+ 表示启动任务所需的代理数量， wkj∈R+ 表示该联盟（一旦组建）完成任务所需的时间。代理始终处于以下三种状态之一：1) 等待其他代理启动任务，2) 执行任务，或 3) 以恒定速度 v 通过直线（任务之间的欧几里得距离）从一个任务移动到另一个任务。解决方案是一组单独的代理行程 Φ={ϕa1,…,ϕan} ，允许代理根据其需求完成所有任务并返回仓库，其中每个行程 ϕai=(k0,ki1,ki2,…,k0) 是一组不重复的节点（仓库除外）。 一个解的完工时间（makespan）是所有代理中最长的行程持续时间，其中单个行程持续时间计算为代理的任务执行时间、旅行时间以及等待时间（从到达任务到其最后一个任务伙伴到达之间的时间，这取决于其他代理的行程）的总和。我们的目标是找到一个具有最小完工时间的解。

METHODOLOGY

In this section, we frame our multi-robot task allocation as an RL problem and provide network and training specifics.
在本节中，我们将多机器人任务分配问题构建为一个强化学习（RL）问题，并提供网络和训练的具体细节。

A. Multi-Robot Scheduling as an RL problem

We frame our problem as a decentralized sequential decision-making problem. Each agent ai sequentially chooses its next task upon completion of its current one, thus iteratively constructing its own tour ϕai until all tasks have been completed and all agents return to the depot. Agents select their next tasks in a sequentially-conditional manner, considering the actions already taken by all other agents. To do so, after an agent makes a decision and travels to the next task, we directly update its location to that of the selected task, which informs the entire team of its decision. In our problem, agents often make decisions asynchronously, as the task completion time is rarely identical among parallelly running tasks. However, all agents in the same coalition will make decisions at the same time after completing their common task. In this case, agents make decisions in the order of their arrivals at that task.
我们将问题框架化为一个去中心化的顺序决策问题。每个代理 a i 在完成当前任务后依次选择下一个任务，从而迭代地构建自己的行程 ϕai ，直到所有任务完成且所有代理返回基地。代理以顺序条件的方式选择下一个任务，考虑所有其他代理已采取的行动。为此，在代理做出决策并前往下一个任务后，我们直接将其位置更新为所选任务的位置，这向整个团队通报了其决策。在我们的问题中，代理通常异步做出决策，因为并行运行的任务完成时间很少相同。然而，同一联盟中的所有代理在完成共同任务后将同时做出决策。在这种情况下，代理按照到达该任务的顺序做出决策。

To speed up the training and simplify decision-making, we introduce a leader-follower decision framework. In this framework, leaders are iteratively chosen to make decisions for a (sub)group of fellow agents. For instance, if a coalition of five agents completes a task, a leader will be randomly selected to decide the next task which requires a particular number of agents (e.g., three). Subsequently, a subset of agents in the original coalition (here, the leader and two other randomly picked agents) will move on to this next task, while the remaining agents (here, two) will choose their next task in the same leader-follower way, conditioned on the decisions made by the previous agents. By reducing the number of decisions during training, agents learn policies with lower variance and greater efficiency. In addition, learned policies can be utilized in a decentralized manner where each agent makes individual decisions.

为了加速训练并简化决策过程，我们引入了一种领导者-跟随者决策框架。在此框架中，领导者被迭代选出，为（子）群体中的其他代理做出决策。例如，如果一个由五个代理组成的联盟完成了一项任务，将随机选出一位领导者来决定下一个需要特定数量代理（如三个）的任务。随后，原联盟中的一部分代理（此处为领导者及另外两位随机挑选的代理）将转向这一新任务，而其余代理（此处为两位）则依据先前代理的决策，以相同的领导者-跟随者方式选择其下一个任务。通过减少训练期间的决策次数，代理能够学习到方差更小、效率更高的策略。此外，所学策略可在去中心化模式下应用，即每个代理独立做出决策。

B. RL formulation
1) Observation
The observation of agent ai at time t is oti={Gti,Ati,Mt}, and includes three components: i) an augmented graph Gti of all tasks associated with a*, ii) the current state of all agentsAti, and iii) a binary mask Mt. In our problem, we define tasks on a complete planar graph G = (K,E), where the vertices K is the set of tasks (and depot) nodes and the edges are denoted as (ki,kj) ∈ E, ∀ki = kj, where ki,kj ∈ K. An augmented graph Gti=(Kti,E) is used to describe the state of all tasks with respect to agent ai. Each vertex in Gti is denoted as ktji=(xkj−xai,ykj−yai,ckj,wkj,ckj−gkj), where j∈{0,1,2,…,m},(xkj,ykj) and (xai,yai) are the coordinates, ckj is the task requirement, wkj the task duration, and gkj the number of agents that currently have been assigned to this task. The depot is a special case, for which kt0i=(xk0−xai,yk0−yai,0,0,0).Ati contains information that pertains to the state of each agent with respect to the ego agent ai with atji=(xaj−xai,yaj−yai,baj,daj,eaj,naj)∈Ati, where j ∈ {1,2, …,n}, baj is the time required for agent aj to reach its next task (or 0 if the agent is currently at a task), daj is the time agent aj has spent waiting on the current task so far (0 if the agent is traveling/executing a task), eaj is the remaining time to complete agent aj's current task (0 if traveling/waiting), while naj∈{0,1} indicates whether the agent has already selected a task. The binary mask Mt ∈ ℝm+1 is shared among all agents, where each component represents whether the corresponding task has already been completed (1) or not (0).
在时间 t 时，代理 a i 的观察为 oti={Gti,Ati,Mt} ，并包含三个部分：i) 与 a*相关的所有任务的增强图 Gti ，ii) 所有代理的当前状态 Ati ，以及 iii) 一个二进制掩码 M t 。在我们的问题中，我们在一个完整的平面图 G = (K,E)上定义任务，其中顶点 K 是任务（和仓库）节点的集合，边表示为(k i ,k) ∈ E, ∀k i = k，其中 k i ,k ∈ K。增强图 Gti=(Kti,E) 用于描述所有任务相对于代理 a i 的状态。 Gti 中的每个顶点表示为 ktji=(xkj−xai,ykj−yai,ckj,wkj,ckj−gkj) ，其中 j∈{0,1,2,…,m},(xkj,ykj) 和 (xai,yai) 是坐标， ckj 是任务需求， wkj 是任务持续时间， gkj 是当前已分配给此任务的代理数量。 仓库是一个特殊情况， kt0i=(xk0−xai,yk0−yai,0,0,0).Ati 包含了关于每个智能体相对于自我智能体 a i 的状态信息，其中 j ∈ {1,2, …,n}， baj 是智能体 a 到达其下一个任务所需的时间（如果智能体当前在任务中则为 0）， daj 是智能体 a 在当前任务上已经等待的时间（如果智能体正在移动/执行任务则为 0）， eaj 是完成智能体 a 当前任务的剩余时间（如果正在移动/等待则为 0），而 naj∈{0,1} 表示智能体是否已经选择了一个任务。二进制掩码 M t ∈ ℝ m+1 在所有智能体之间共享，其中每个分量表示相应的任务是否已经完成（1 表示已完成，0 表示未完成）。

2) Action
At each decision step t of agent ai, given its current observation oti, our decentralized neural network parameterized by θ outputs a stochastic policy pθ(a∣oti)=pθ(τt=j∈E∣oti) over all tasks, where finished tasks are filtered using the mask Mt. We sample agent a*'s action from pθ(a∣oti) following a multinomial probability distribution during training, and select action greedily at inference time.
在智能体 a i 的每个决策步骤 t 中，给定其当前观测 oti ，我们由θ参数化的去中心化神经网络输出所有任务上的一个随机策略 pθ(a∣oti)=pθ(τt=j∈E∣oti) ，其中已完成的任务通过掩码 M t 进行过滤。在训练期间，我们按照多项式概率分布从 pθ(a∣oti) 中采样智能体 a*的动作，并在推理时贪婪地选择动作。

3) Reward
In our problem, the objective is to minimize the time needed for the team to complete all tasks and return to the depot (makespan). We simply define a sparse reward calculated at the end of the training episode as R(Φ) = -T, where T is the makespan. In addition, we incorporate a timeout for each episode to prevent deadlocks, where deadlocks may occur as agents cannot finish an episode due to poor coordination (which happens earlier on during training).
在我们的问题中，目标是使团队完成所有任务并返回仓库所需的时间最小化（即完工时间）。我们简单地定义了一个在训练回合结束时计算的稀疏奖励，即 R(Φ) = -T，其中 T 为完工时间。此外，我们为每个回合设置了超时机制，以防止死锁的发生，死锁可能因代理间协调不佳（这在训练初期尤为常见）而无法完成回合。

C. Policy network
We use an attention-based network with a encoder-decoder architecture to learn the agents' policies πθ(a∣oti), as depicted in Fig 2. The network parameters are shared among all the agents. We rely on a group of encoders to extract salient features, allowing agents to build context across the entire system. This shared context enhances communication among agents, enabling them to predict each other's intent and facilitate cooperation. Then, a group of decoders will leverage the learned representations from encoders (i.e., the features of position, task execution status, and travel times, etc.) and reason about potential benefits and costs associated with each task to help agents make informed decisions. Finally, a task selector outputs a probability for an agent to choose from an arbitrary number of tasks. We address scalability issues by leveraging the attention-based networks.
我们采用基于注意力的网络，其编码器-解码器架构用于学习代理的策略 πθ(a∣oti) ，如 Fig 2 所示。网络参数在所有代理之间共享。我们依赖一组编码器提取显著特征，使代理能够在整个系统中构建上下文。这种共享上下文增强了代理间的沟通，使它们能够预测彼此的意图并促进合作。接着，一组解码器将利用编码器学习到的表示（即位置、任务执行状态和旅行时间等特征），并推理与每项任务相关的潜在收益和成本，以帮助代理做出明智决策。最后，任务选择器输出代理从任意数量任务中选择的概率。我们通过利用基于注意力的网络来解决可扩展性问题。

1) Multi-head attention with gated linear unit
The multi-head attention layer [26] is a vital component of our network. We further integrate a gated mechanism [27] into the attention layer to obtain better representations. In each layer, the zth head takes query hq=[hq1,hq2,…,hqn]T and key-value pairs hk,v=[hk,v1,hk,v2,…,hk,vn]T, transformed by three learnable matrices, to calculate an attention vector αz:

多头注意力层 [26] 是我们网络的关键组成部分。我们进一步将门控机制 [27] 集成到注意力层中，以获得更好的表示。在每一层中，z th 头接收查询 hq=[hq1,hq2,…,hqn]T 和键值对 hk,v=[hk,v1,hk,v2,…,hk,vn]T ，通过三个可学习的矩阵进行转换，以计算注意力向量α z ：

where WQ ,WK ∈ ℝZ×dim × dim′ ,WV ∈ ℝZ×dim × dim, dim' = dim/Z, and Z is the number of heads (in practice, h = 8). The output vector will be first passed to layer normalization, followed by a feed-forward layer with a gated linear unit [27]. When hq and hk,v are the same, it is a self-attention layer as MHA(hq).

Conclusion
This work introduces a decentralized reinforcement learning approach for efficiently solving the XD[ST-MR-TA] problems, where each agent iteratively and reactively constructs its tour based on global observations and a learned attention over all tasks and agents. We further stabilize cooperative learning by implementing a leader-follower technique, which reduces decision complexity and help attain the final policy that is fully decentralized and applicable to problems of arbitrary sizes. Based on evaluation results, we show our approach exhibits excellent performance in scenarios with large task-to-agent ratios, whereas in smaller-/larger-scale problems, we are still within a few percent gap of the state-of-the-art baselines.
本研究提出了一种去中心化强化学习方法，旨在高效解决 XD[ST-MR-TA]问题，其中每个代理基于全局观察和对所有任务及代理的学习注意力，迭代且反应性地构建其路径。通过实施领导者-跟随者技术，我们进一步稳定了协作学习，该技术降低了决策复杂性，并有助于获得完全去中心化且适用于任意规模问题的最终策略。基于评估结果，我们展示了该方法在任务与代理比例较大的场景中表现出色，而在较小或较大规模的问题上，我们仍与最先进的基线方法保持几个百分点的差距。

In future work, we are interested in extending our approach to tackle more complex MRTA problems, such as heterogeneous agent teams and task precedence, which present more challenging scenarios for coalition formation and task decomposition problems.
在未来的工作中，我们有意扩展我们的方法以解决更复杂的多机器人任务分配（MRTA）问题，例如异构代理团队和任务优先级，这些问题为联盟形成和任务分解带来了更具挑战性的场景。
## 第一部分 人工智能基础

### 第 1 章 绪论

- 1.1 什么是人工智能
  - 1.1.1 类人行为：图灵测试方法
  - 1.1.2 类人思考：认知建模方法
  - 1.1.3 理性思考：“思维法则”方法
  - 1.1.4 理性行为：理性智能体方法
  - 1.1.5 益机
- 1.2 人工智能的基础
  - 1.2.1 哲学
  - 1.2.2 数学
  - 1.2.3 经济学
  - 1.2.4 神经科学
  - 1.2.5 心理学
  - 1.2.6 计算机工程
  - 1.2.7 控制理论与控制论
  - 1.2.8 语言学
- 1.3 人工智能的历史
  - 1.3.1 人工智能的诞生（1943—1956）
  - 1.3.2 早期热情高涨，期望无限（1952—1969）
  - 1.3.3 一些现实（1966—1973）
  - 1.3.4 专家系统（1969—1986）
  - 1.3.5 神经网络的回归（1986—现在）
  - 1.3.6 概率推理和机器学习（1987—现在）
  - 1.3.7 大数据（2001—现在）
  - 1.3.8 深度学习（2011—现在）
- 1.4 目前的先进技术
- 1.5 人工智能的风险和收益

### 第 2 章 智能体

- 2.1 智能体和环境
- 2.2 良好行为：理性的概念
  - 2.2.1 性能度量
  - 2.2.2 理性
  - 2.2.3 全知、学习和自主
- 2.3 环境的本质
  - 2.3.1 指定任务环境
  - 2.3.2 任务环境的属性
- 2.4 智能体的结构
  - 2.4.1 智能体程序
  - 2.4.2 简单反射型智能体
  - 2.4.3 基于模型的反射型智能体
  - 2.4.4 基于目标的智能体
  - 2.4.5 基于效用的智能体
  - 2.4.6 学习型智能体
  - 2.4.7 智能体程序的组件如何工作

## 第二部分 问题求解

### 第 3 章 通过搜索进行问题求解

- 3.1 问题求解智能体

  - 3.1.1 搜索问题和解
  - 3.1.2 问题形式化

- 3.2 问题示例

  - 3.2.1 标准化问题
  - 3.2.2 真实世界问题
  - 3.3 搜索算法
  - 3.3.1 最佳优先搜索
  - 3.3.2 搜索数据结构
  - 3.3.3 冗余路径
  - 3.3.4 问题求解性能评估

- 3.4 无信息搜索策略

  - 3.4.1 广度优先搜索
  - 3.4.2 Dijkstra 算法或一致代价搜索
  - 3.4.3 深度优先搜索与内存问题
  - 3.4.4 深度受限和迭代加深搜索
  - 3.4.5 双向搜索
  - 3.4.6 无信息搜索算法对比

- 3.5 有信息（启发式）搜索策略

  - 3.5.1 贪心最佳优先搜索
  - 3.5.2 A\*搜索
  - 3.5.3 搜索等值线
  - 3.5.4 满意搜索：不可容许的启发式函数与加权 A\*搜索
  - 3.5.5 内存受限搜索
  - 3.5.6 双向启发式搜索

- 3.6 启发式函数

  - 3.6.1 启发式函数的准确性对性能的影响
  - 3.6.2 从松弛问题出发生成启发式函数
  - 3.6.3 从子问题出发生成启发式函数：模式数据库
  - 3.6.4 使用地标生成启发式函数
  - 3.6.5 学习以更好地搜索
  - 3.6.6 从经验中学习启发式函数

### 第 4 章 复杂环境中的搜索

- 4.1 局部搜索和最优化问题
  - 4.1.1 爬山搜索
  - 4.1.2 模拟退火
  - 4.1.3 局部束搜索
  - 4.1.4 进化算法
- 4.2 连续空间中的局部搜索
- 4.3 使用非确定性动作的搜索
  - 4.3.1 不稳定的真空吸尘器世界
  - 4.3.2 与或搜索树
  - 4.3.3 反复尝试
- 4.4 部分可观测环境中的搜索
  - 4.4.1 无观测信息的搜索
  - 4.4.2 部分可观测环境中的搜索
  - 4.4.3 求解部分可观测问题
  - 4.4.4 部分可观测环境中的智能体
- 4.5 在线搜索智能体和未知环境
  - 4.5.1 在线搜索问题
  - 4.5.2 在线搜索智能体
  - 4.5.3 在线局部搜索
  - 4.5.4 在线搜索中的学习

### 第 5 章 对抗搜索和博弈

5.1 博弈论

5.2 博弈中的优化决策

5.2.1 极小化极大搜索算法
5.2.2 多人博弈中的最优决策

5.2.3 剪枝

5.2.4 移动顺序

5.3 启发式树搜索

5.3.1 评价函数
5.3.2 截断搜索
5.3.3 前向剪枝
5.3.4 搜索和查表
5.4 蒙特卡罗树搜索

5.5 随机博弈

机会博弈的评价函数

5.6 部分可观测博弈

5.6.1 四国军棋：部分可观测的国际象棋
5.6.2 纸牌游戏

5.7 博弈搜索算法的局限性

### 第 6 章 约束满足问题

6.1 定义约束满足问题

6.1.1 问题示例：地图着色
6.1.2 问题示例：车间作业调度
6.1.3 CSP 形式体系的变体

6.2 约束传播：CSP 中的推断

6.2.1 节点一致性
6.2.2 弧一致性
6.2.3 路径一致性
6.2.4 k 一致性
6.2.5 全局约束
6.2.6 数独

6.3 CSP 的回溯搜索

6.3.1 变量排序和值排序
6.3.2 交替进行搜索和推理
6.3.3 智能回溯：向后看
6.3.4 约束学习

6.4 CSP 的局部搜索

6.5 问题的结构

6.5.1 割集调整
6.5.2 树分解
6.5.3 值对称

## 第三部分 知识、推理和规划

### 第 7 章 逻辑智能体

7.1 基于知识的智能体

7.2 wumpus 世界

7.3 逻辑

7.4 命题逻辑：一种非常简单的逻辑

7.4.1 语法
7.4.2 语义
7.4.3 一个简单的知识库
7.4.4 一个简单的推断过程

7.5 命题定理证明

7.5.1 推断与证明
7.5.2 通过归结证明
7.5.3 霍恩子句与确定子句
7.5.4 前向链接与反向链接

7.6 高效命题模型检验

7.6.1 完备的回溯算法
7.6.2 局部搜索算法
7.6.3 随机 SAT 问题概览

7.7 基于命题逻辑的智能体

7.7.1 世界的当前状态
7.7.2 混合智能体
7.7.3 逻辑状态估计
7.7.4 用命题推断进行规划

### 第 8 章 一阶逻辑

- 8.1 回顾表示
- 8.1.1 思想的语言
- 8.1.2 结合形式语言和自然语言的优点
- 8.2 一阶逻辑的语法和语义
- 8.2.1 一阶逻辑模型
- 8.2.2 符号与解释
- 8.2.3 项
- 8.2.4 原子语句
- 8.2.5 复合语句
- 8.2.6 量词
- 8.2.7 等词
- 8.2.8 数据库语义
- 8.3 使用一阶逻辑
- 8.3.1 一阶逻辑的断言与查询
- 8.3.2 亲属关系论域
- 8.3.3 数、集合与列表
- 8.3.4 wumpus 世界
- 8.4 一阶逻辑中的知识工程
- 8.4.1 知识工程的过程
- 8.4.2 电子电路论域

### 第 9 章 一阶逻辑中的推断

- 9.1 命题推断与一阶推断
约简为命题推断
- 9.2 合一与一阶推断
- 9.2.1 合一
- 9.2.2 存储与检索
- 9.3 前向链接
- 9.3.1 一阶确定子句
- 9.3.2 简单的前向链接算法
- 9.3.3 高效前向链接
- 9.4 反向链接
- 9.4.1 反向链接算法
- 9.4.2 逻辑编程
- 9.4.3 冗余推断和无限循环
- 9.4.4 Prolog 的数据库语义
- 9.4.5 约束逻辑编程
- 9.5 归结
- 9.5.1 一阶逻辑的合取范式
- 9.5.2 归结推断规则
- 9.5.3 证明范例
- 9.5.4 归结的完备性
- 9.5.5 等词
- 9.5.6 归结策略

### 第 10 章 知识表示

- 10.1 本体论工程
- 10.2 类别与对象
- 10.2.1 物理组成
- 10.2.2 量度
- 10.2.3 对象：事物和物质
- 10.3 事件
- 10.3.1 时间
- 10.3.2 流和对象
- 10.4 精神对象和模态逻辑
其他模态逻辑
- 10.5 类别的推理系统
- 10.5.1 语义网络
- 10.5.2 描述逻辑
- 10.6 用缺省信息推理
- 10.6.1 限定与缺省逻辑
- 10.6.2 真值维护系统

### 第 11 章 自动规划

- 11.1 经典规划的定义
- 11.1.1 范例领域：航空货物运输
- 11.1.2 范例领域：备用轮胎问题
- 11.1.3 范例领域：积木世界
- 11.2 经典规划的算法
- 11.2.1 规划的前向状态空间搜索
- 11.2.2 规划的反向状态空间搜索
- 11.2.3 使用布尔可满足性规划
- 11.2.4 其他经典规划方法
- 11.3 规划的启发式方法
- 11.3.1 领域无关剪枝
- 11.3.2 规划中的状态抽象
- 11.4 分层规划
- 11.4.1 高层动作
- 11.4.2 搜索基元解
- 11.4.3 搜索抽象解
- 11.5 非确定性域的规划和行动
- 11.5.1 无传感器规划
- 11.5.2 应变规划
- 11.5.3 在线规划
- 11.6 时间、调度和资源
- 11.6.1 时间约束和资源约束的表示
- 11.6.2 解决调度问题
- 11.7 规划方法分析

## 第四部分 不确定知识和不确定推理

### 第 12 章 不确定性的量化

- 12.1 不确定性下的动作
  - 12.1.1 不确定性概述
  - 12.1.2 不确定性与理性决策
- 12.2 基本概率记号
  - 12.2.1 概率是关于什么的
  - 12.2.2 概率断言中的命题语言
  - 12.2.3 概率公理及其合理性
- 12.3 使用完全联合分布进行推断
- 12.4 独立性
- 12.5 贝叶斯法则及其应用
  - 12.5.1 应用贝叶斯法则：简单实例
  - 12.5.2 应用贝叶斯法则：合并证据
- 12.6 朴素贝叶斯模型
  - 使用朴素贝叶斯进行文本分类
- 12.7 重游 wumpus 世界

### 第 13 章 概率推理

- 13.1 不确定域的知识表示
- 13.2 贝叶斯网络的语义
  - 13.2.1 贝叶斯网络中的条件独立性关系
  - 13.2.2 条件分布的高效表示
  - 13.2.3 连续变量的贝叶斯网络
  - 13.2.4 案例研究：汽车保险
- 13.3 贝叶斯网络中的精确推断
  - 13.3.1 通过枚举进行推断
  - 13.3.2 变量消元算法
  - 13.3.3 精确推断的复杂性
  - 13.3.4 聚类算法
- 13.4 贝叶斯网络中的近似推理
  - 13.4.1 直接采样方法
  - 13.4.2 通过马尔可夫链模拟进行推断
  - 13.4.3 编译近似推断
- 13.5 因果网络
  - 13.5.1 表示动作：do 操作
  - 13.5.2 后门准则

### 第 14 章 时间上的概率推理

- 14.1 时间与不确定性
  - 14.1.1 状态与观测
  - 14.1.2 转移模型与传感器模型
- 14.2 时序模型中的推断
  - 14.2.1 滤波与预测
  - 14.2.2 平滑
  - 14.2.3 寻找最可能序列
- 14.3 隐马尔可夫模型
  - 14.3.1 简化矩阵算法
  - 14.3.2 隐马尔可夫模型示例：定位
- 14.4 卡尔曼滤波器
  - 14.4.1 更新高斯分布
  - 14.4.2 简单的一维示例
  - 14.4.3 一般情况
  - 14.4.4 卡尔曼滤波的适用范围
- 14.5 动态贝叶斯网络
  - 14.5.1 构建动态贝叶斯网络
  - 14.5.2 动态贝叶斯网络中的精确推断
  - 14.5.3 动态贝叶斯网络中的近似推断

### 第 15 章 概率编程

- 15.1 关系概率模型
  - 15.1.1 语法与语义
  - 15.1.2 实例：评定玩家的技能等级
  - 15.1.3 关系概率模型中的推断
- 15.2 开宇宙概率模型
  - 15.2.1 语义与语法
  - 15.2.2 开宇宙概率模型的推断
  - 15.2.3 示例
- 15.3 追踪复杂世界
  - 15.3.1 示例：多目标跟踪
  - 15.3.2 示例：交通监控
- 15.4 作为概率模型的程序
  - 15.4.1 示例：文本阅读
  - 15.4.2 语法与语义
  - 15.4.3 推断结果
  - 15.4.4 结合马尔可夫模型改进生成程序
  - 15.4.5 生成程序的推断

### 第 16 章 做简单决策

- 16.1 在不确定性下结合信念与愿望
  - 16.2 效用理论基础
  - 16.2.1 理性偏好的约束
  - 16.2.2 理性偏好导致效用
- 16.3 效用函数
  - 16.3.1 效用评估和效用尺度
  - 16.3.2 金钱的效用
  - 16.3.3 期望效用与决策后失望
  - 16.3.4 人类判断与非理性
- 16.4 多属性效用函数
  - 16.4.1 占优
  - 16.4.2 偏好结构与多属性效用
- 16.5 决策网络
  - 16.5.1 使用决策网络表示决策问题
  - 16.5.2 评估决策网络
- 16.6 信息价值
  - 16.6.1 简单示例
  - 16.6.2 完美信息的一般公式
  - 16.6.3 价值信息的性质
  - 16.6.4 信息收集智能体的实现
  - 16.6.5 非短视信息收集
  - 16.6.6 敏感性分析与健壮决策
- 16.7 未知偏好
  - 16.7.1 个人偏好的不确定性
  - 16.7.2 顺从人类

### 第 17 章 做复杂决策

- 17.1 序贯决策问题
  - 17.1.1 时间上的效用
  - 17.1.2 最优策略与状态效用
  - 17.1.3 奖励规模
  - 17.1.4 表示 MDP
- 17.2 MDP 的算法
  - 17.2.1 价值迭代
  - 17.2.2 策略迭代
  - 17.2.3 线性规划
  - 17.2.4 MDP 的在线算法
- 17.3 老虎机问题
  - 17.3.1 计算基廷斯指数
  - 17.3.2 伯努利老虎机
  - 17.3.3 近似最优老虎机策略
  - 17.3.4 不可索引变体
- 17.4 部分可观测 MDP
  - POMDP 的定义
- 17.5 求解 POMDP 的算法
  - 17.5.1 POMDP 的价值迭代
  - 17.5.2 POMDP 的在线算法

### 第 18 章 多智能体决策

- 18.1 多智能体环境的特性
  - 18.1.1 单个决策者
  - 18.1.2 多决策者
  - 18.1.3 多智能体规划
  - 18.1.4 多智能体规划：合作与协调
- 18.2 非合作博弈论
  - 18.2.1 单步博弈：正则形式博弈
  - 18.2.2 社会福利
  - 18.2.3 重复博弈
  - 18.2.4 序贯博弈：扩展形式
  - 18.2.5 不确定收益与辅助博弈
- 18.3 合作博弈论
  - 18.3.1 联盟结构与结果
  - 18.3.2 合作博弈中的策略
  - 18.3.3 合作博弈中的计算
- 18.4 做集体决策
  - 18.4.1 在合同网中分配任务
  - 18.4.2 通过拍卖分配稀缺资源
  - 18.4.3 投票
  - 18.4.4 议价

## 第五部分 机器学习

### 第 19 章 样例学习

- 19.1 学习的形式
- 19.2 监督学习
问题示例：餐厅等待问题
- 19.3 决策树学习
- 19.3.1 决策树的表达能力
- 19.3.2 从样例中学习决策树
- 19.3.3 选择测试属性
- 19.3.4 泛化与过拟合
- 19.3.5 拓展决策树的适用范围
- 19.4 模型选择与模型优化
- 19.4.1 模型选择
- 19.4.2 从错误率到损失函数
- 19.4.3 正则化
- 19.4.4 超参数调整
- 19.5 学习理论
PAC 学习示例：学习决策列表
- 19.6 线性回归与分类
- 19.6.1 单变量线性回归
- 19.6.2 梯度下降
- 19.6.3 多变量线性回归
- 19.6.4 带有硬阈值的线性分类器
- 19.6.5 基于逻辑斯谛回归的线性分类器
- 19.7 非参数模型
- 19.7.1 最近邻模型
- 19.7.2 使用 k-d 树寻找最近邻
- 19.7.3 局部敏感哈希
- 19.7.4 非参数回归
- 19.7.5 支持向量机
- 19.7.6 核技巧
- 19.8 集成学习
- 19.8.1 自助聚合法
- 19.8.2 随机森林法
- 19.8.3 堆叠法
- 19.8.4 自适应提升法
- 19.8.5 梯度提升法
- 19.8.6 在线学习
- 19.9 开发机器学习系统
- 19.9.1 问题形式化
- 19.9.2 数据收集、评估和管理
- 19.9.3 模型选择与训练
- 19.9.4 信任、可解释性、可说明性
- 19.9.5 操作、监控和维护

### 第 20 章 概率模型学习

- 20.1 统计学习
- 20.2 完全数据学习
- 20.2.1 最大似然参数学习：离散模型
- 20.2.2 朴素贝叶斯模型
- 20.2.3 生成模型和判别模型
- 20.2.4 最大似然参数学习：连续模型
- 20.2.5 贝叶斯参数学习
- 20.2.6 贝叶斯线性回归
- 20.2.7 贝叶斯网络结构学习
- 20.2.8 非参数模型密度估计
- 20.3 隐变量学习：EM 算法
- 20.3.1 无监督聚类：学习混合高斯
- 20.3.2 学习带隐变量的贝叶斯网络参数值
- 20.3.3 学习隐马尔可夫模型
- 20.3.4 EM 算法的一般形式
- 20.3.5 学习带隐变量的贝叶斯网络结构

### 第 21 章 深度学习

- 21.1 简单前馈网络
- 21.1.1 网络作为复杂函数
- 21.1.2 梯度与学习
- 21.2 深度学习的计算图
- 21.2.1 输入编码
- 21.2.2 输出层与损失函数
- 21.2.3 隐藏层
- 21.3 卷积网络
- 21.3.1 池化与下采样
- 21.3.2 卷积神经网络的张量运算
- 21.3.3 残差网络
- 21.4 学习算法
- 21.4.1 计算图中的梯度计算
- 21.4.2 批量归一化
- 21.5 泛化
- 21.5.1 选择正确的网络架构
- 21.5.2 神经架构搜索
- 21.5.3 权重衰减
- 21.5.4 暂退法
- 21.6 循环神经网络
- 21.6.1 训练基本的循环神经网络
- 21.6.2 长短期记忆 RNN
- 21.7 无监督学习与迁移学习
- 21.7.1 无监督学习
- 21.7.2 迁移学习和多任务学习
- 21.8 应用
- 21.8.1 视觉
- 21.8.2 自然语言处理
- 21.8.3 强化学习

### 第 22 章 强化学习

22.1 从奖励中学习
22.2 被动强化学习
22.2.1 直接效用估计
22.2.2 自适应动态规划
22.2.3 时序差分学习
22.3 主动强化学习
22.3.1 探索
22.3.2 安全探索
22.3.3 时序差分 Q 学习
22.4 强化学习中的泛化
22.4.1 近似直接效用估计
22.4.2 近似时序差分学习
22.4.3 深度强化学习
22.4.4 奖励函数设计
22.4.5 分层强化学习
22.5 策略搜索
22.6 学徒学习与逆强化学习
22.7 强化学习的应用
22.7.1 在电子游戏中的应用
22.7.2 在机器人控制中的应用

## 第六部分 沟通、感知和行动

### 第 23 章 自然语言处理

23.1 语言模型

23.1.1 词袋模型
23.1.2 n 元单词模型
23.1.3 其他 n 元模型
23.1.4 n 元模型的平滑
23.1.5 单词表示
23.1.6 词性标注
23.1.7 语言模型的比较
23.2 文法
E0 的词典
23.3 句法分析
23.3.1 依存分析
23.3.2 从样例中学习句法分析器
23.4 扩展文法
23.4.1 语义解释
23.4.2 学习语义文法
23.5 真实自然语言的复杂性
23.6 自然语言任务

### 第 24 章 自然语言处理中的深度学习

24.1 词嵌入
24.2 自然语言处理中的循环神经网络
24.2.1 使用循环神经网络的语言模型
24.2.2 用循环神经网络进行分类
24.2.3 自然语言处理任务中的 LSTM 模型
24.3 序列到序列模型
24.3.1 注意力
24.3.2 解码
24.4 Transformer 架构
24.4.1 自注意力
24.4.2 从自注意力到 Transformer
24.5 预训练和迁移学习
24.5.1 预训练词嵌入
24.5.2 预训练上下文表示
24.5.3 掩码语言模型
24.6 最高水平（SOTA）

### 第 25 章 计算机视觉

25.1 引言
25.2 图像形成
25.2.1 无透镜成像：针孔照相机
25.2.2 透镜系统
25.2.3 缩放正交投影
25.2.4 光线与明暗
25.2.5 颜色
25.3 简单图像特征
25.3.1 边缘
25.3.2 纹理
25.3.3 光流
25.3.4 自然图像分割
25.4 图像分类
25.4.1 基于卷积神经网络的图像分类
25.4.2 卷积神经网络对图像分类问题有效的原因
25.5 物体检测
25.6 三维世界
25.6.1 多个视图下的三维线索
25.6.2 双目立体视觉
25.6.3 移动摄像机给出的三维线索
25.6.4 单个视图的三维线索
25.7 计算机视觉的应用
25.7.1 理解人类行为
25.7.2 匹配图片与文字
25.7.3 多视图重建
25.7.4 单视图中的几何
25.7.5 生成图片
25.7.6 利用视觉控制运动

### 第 26 章 机器人学

26.1 机器人
26.2 机器人硬件
26.2.1 机器人的硬件层面分类
26.2.2 感知世界
26.2.3 产生运动
26.3 机器人学解决哪些问题
26.4 机器人感知
26.4.1 定位与地图构建
26.4.2 其他感知类型
26.4.3 机器人感知中的监督学习与无监督学习
26.5 规划与控制
26.5.1 构形空间
26.5.2 运动规划
26.5.3 轨迹跟踪控制
26.5.4 最优控制
26.6 规划不确定的运动
26.7 机器人学中的强化学习
26.7.1 利用模型
26.7.2 利用其他信息
26.8 人类与机器人
26.8.1 协调
26.8.2 学习做人类期望的事情
26.9 其他机器人框架
26.9.1 反应式控制器
26.9.2 包容架构
26.10 应用领域

## 第七部分 总结

### 第 27 章 人工智能的哲学、伦理和安全性

27.1 人工智能的极限
27.1.1 由非形式化得出的论据
27.1.2 由能力缺陷得出的论据
27.1.3 数学异议
27.1.4 衡量人工智能
27.2 机器能真正地思考吗
27.2.1 中文房间
27.2.2 意识与感质
27.3 人工智能的伦理
27.3.1 致命性自主武器
27.3.2 监控、安全与隐私
27.3.3 公平与偏见
27.3.4 信任与透明度
27.3.5 工作前景
27.3.6 机器人权利
27.3.7 人工智能安全性

### 第 28 章 人工智能的未来

28.1 人工智能组件
28.2 人工智能架构

## 附录 A 数学背景知识

A.1 复杂性分析和$O()$记号
A.1.1 渐近分析
A.1.2 NP 困难和固有的难题
A.2 向量，矩阵和线性代数
A.3 概率分布

读者服务

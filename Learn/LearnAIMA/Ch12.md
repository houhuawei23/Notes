# 第 12 章 不确定性的量化

本章表明概率论是不确定推理的合适基础，并简要介绍了它的应用。

- 不确定性的产生是由于惰性和无知。在复杂的、非确定性的或部分可观测的环境中，不确定性是不可避免的。
- 概率表达了智能体无法对一个语句的真值做出明确的判断。概率概括了智能体对于证据的信念。
- **决策论**结合了智能体的**信念**和**欲望**，将**最大期望效用**的动作定义为最佳动作。
  - 决策论 = 概率论 + 效用理论，MEU
- 基本的概率陈述包括简单命题和复杂命题上的先验概率（或无条件概率）和后验概率（或条件概率）。
- **概率公理**约束逻辑相关命题的概率。违背公理的智能体在某些情况下的行为必定是不理性的。
- **完全联合概率分布**为随机变量的每种完整赋值指定了概率。通常，完全联合概率分布过于庞大，以至于无法显式地创建和使用，但如果其可用时，它可以用于回答查询，只需要简单地将其中与查询命题对应的可能世界的条目相加即可。
- 随机变量子集间的**绝对独立性**允许将完全联合分布分解成小的联合分布，极大地降低它的复杂度。
- **贝叶斯法则**允许通过已知的条件概率去计算未知概率，条件概率通常在因果方向上。将贝叶斯法则应用于多条证据时会遇到与完全联合分布相同的规模扩展问题。
- 域中的直接因果关系带来的条件独立性允许完全联合分布被分解成小的条件分布。**朴素贝叶斯模型**假设**给定单原因变量时，所有结果变量具有条件独立性**。模型大小随结果个数线性增长。
- wumpus 世界的智能体可以计算世界中未观测的方面的概率，从而改进纯逻辑智能体的决策。条件独立性简化了这些计算。

---

- [(33 封私信 / 81 条消息) 相互独立和条件独立的关系？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/42080633)

## 概率定义

概率的公理化定义

概率是一个事件集合上的度量, 满足以下 3 条公理。

1. 每个事件的度量在 0 和 1 之间, 可写为$0 \leqslant P\left(X=x_{i}\right) \leqslant 1$，其中$X$是表示事件的随机变量,$x_{i}$是 X 的可能的值。一般来说, 随机变量用大写字母表示, 它们的值用小写字母表示。
2. 整个集合的度量为 1 , 即

$$
 \sum_{i=1}^{n} P\left(X=x_{i}\right)=1
$$

3. 不相交事件的并集的概率等于单个事件概率的和, 即

$$
P\left(X=x_{1} \vee X=x_{2}\right)=P\left(X=x_{1}\right)+ P\left(X=x_{2}\right)
$$

这里$x_{1}$和$x_{2}$是不相交的。

容斥原理：

$$
P(a \vee b) = P(a) + P(b) - P(a \wedge b)
$$

什么叫“概率公理的论据”？

为什么不有违背“概率公理”的信念集合呢？

因为违背概率公理的信念集合是不合理的，设想一个赌局，参加者按照他持有的信念状态下注，下注数量与自己持有的信念状态成正比。可以证明，对于按照违反概率公理的信念状态进行下注的智能体，一定存在某种赌局的组合，使得该智能体每次都会输钱。？？？

完全联合概率分布：所有随机变量的联合分布，即给定所有变量的所有取值组合的概率密度/概率

一个概率模型可以由完全联合概率分布完全确定

而每个命题的概率是该命题为真的可能世界的概率的求和，那么，如果已知完全联合概率分布，我们原则上可以计算任何命题的概率（因为我们已经指定/给出了每一个可能的样本点的概率，要做的只是找出满足要求的样本点，然后求和）。

实际上呢？

1. 完全联合概率分布实际上不可知/不知道
2. 完全概率分布的求和计算复杂度难以承受

## 使用完全联合概率分布进行推断

概率推断：使用完全联合概率分布作为知识库，给定观测证据，为每个查询命题计算后验概率。

边缘概率：抽取变量子集 or 单个变量的分布

边缘化/求和消元：对其他/不关注的变量在其值域上求和

$$
\mathbf{P}(\mathbf{Y})=\sum_{\mathbf{z}} \mathbf{P}(\mathbf{Y}, \mathbf{Z}=\mathbf{z})
$$

条件化：

$$
\mathbf{P}(\mathbf{Y})=\sum_{\mathbf{z}} \mathbf{P}(\mathbf{Y} \mid \mathbf{z}) P(\mathbf{z})
$$

通用推断过程：

- X 待查询单变量
- E 证据变量列表，e 为观测值列表
- Y 剩余为观测变量

$$
\mathbf{P}(X \mid \mathbf{e})=\alpha \mathbf{P}(X, \mathbf{e})=\alpha \sum_{\mathbf{y}} \mathbf{P}(X, \mathbf{e}, \mathbf{y})
$$

其中求和是针对所有可能的$\boldsymbol{y}$(也就是未观测变量$\boldsymbol{Y}$的值的所有可能组合)。注意，变量$X$、$\boldsymbol{E}$和$\boldsymbol{Y}$构成了域变量的完整集合, 所以$\boldsymbol{P}(X, \boldsymbol{e}, \boldsymbol{y})$仅仅是完全联合分布的一个概率子集。

-$\alpha$为归一化常数，要会合理利用以简化计算

#### 独立性 Independence

[Independence (probability theory) - Wikipedia](<https://en.wikipedia.org/wiki/Independence_(probability_theory)>)

独立性是“事理”上对两个随机事件的的关系进行评判，即不相关，互不影响，对方发生与否不影响我发生的概率，我发生与否也不影响对方发生的概率。

独立性定义为：

两个命题 a 和 b 是相互独立的，即 a 和 b 的概率满足如下关系：

$$
P(a \mid b)=P(a) \quad or \quad P(b \mid a)=P(b) \quad or \quad P(a \wedge b)=P(a) P(b)
$$

两个随机变量 X 和 Y 是独立的，当且仅当：

$$
\mathbf{P}(X \mid Y)=\mathbf{P}(X) \quad or \quad \mathbf{P}(Y \mid X)=\mathbf{P}(Y) \quad or \quad \mathbf{P}(X, Y)=\mathbf{P}(X) \mathbf{P}(Y)
$$

- “独立性”是一种定义，是一个断言，也就是说：
  - 如果两个事件独立，那么一定满足下述关系式；
  - 如果两个事件满足上述关系式，那么就称这两个事件相互独立。

多变量：

**两两独立** pairwise independent

有限事件的集合$\left\{A_{i}\right\}_{i=1}^{n}$中每对事件都是相互独立的，则称这些事件是两两独立的：

$$
\operatorname{Pr}\left(A_{i} \cap A_{j}\right)=\operatorname{Pr}\left(A_{i}\right) \operatorname{Pr}\left(A_{j}\right) \quad (~ \forall i,j \in \{ 1,\cdots,n\},~ i\neq j~)
$$

**相互独立** mutually independent

有限事件的集合$\left\{A_{i}\right\}_{i=1}^{n}$中，每个事件都与其他任何事件构成的交集独立，则这些事件是**相互独立** 的

对样本空间的**任意有限子集**序列$A_1, \cdots A_n$，有：

$$
\operatorname{Pr}\left(A_{1} \cap \cdots \cap A_{n}\right)=\operatorname{Pr}\left(A_{1}\right) \cdots \operatorname{Pr}\left(A_{n}\right)
$$

或：

$$
\operatorname{Pr}\left(\bigcap_{i=1}^{n} A_{i}\right)=\prod_{i=1}^{n} \operatorname{Pr}\left(A_{i}\right)
$$

#### 贝叶斯法则

$$
\mathbf{P}(Y \mid X)=\frac{\mathbf{P}(X \mid Y) \mathbf{P}(Y)}{\mathbf{P}(X)}
$$

有证据变量的形式：

$$
\mathbf{P}(Y \mid X, \mathbf{e})=\frac{\mathbf{P}(X \mid Y, \mathbf{e}) \mathbf{P}(Y \mid \mathbf{e})}{\mathbf{P}(X \mid \mathbf{e})}
$$

#### 条件独立性

一般定义：

$$
\mathbf{P}(X, Y \mid Z)=\mathbf{P}(X \mid Z) \mathbf{P}(Y \mid Z)
$$

- 绝对独立性 vs 条件独立性
  - 一般情况下，两者不能互推，即：**条件独立得不出独立，独立得不出条件独立**
  - 绝对独立的两个变量，在给定另一个变量后，可能会相关。
  - 相关的两个变量，在给定另一个变量后，可能会条件独立。

## 朴素贝叶斯模型

使用朴素贝叶斯进行文本分类

给定原因时，所有结果都是条件独立的，那么完全联合分布可写作：

$$
\mathbf{P}\left(\right. Cause,~ Effect_{1}, \ldots, Effect \left._{n}\right) = \mathbf{P}( Cause ) \prod_{i} \mathbf{P}\left(\right. Effect _{i} \mid Cause )
$$

经常作为在给定原因时，结果变量并不是严格独立的情况的近似。

给定一些观测结果：

$$
\mathbf{P}( Cause \mid \mathbf{e})=\alpha \sum_{\mathbf{y}} \mathbf{P}( Cause , \mathbf{e}, \mathbf{y})
$$

进一步：

$$
\begin{aligned} 
\mathbf{P}(\text { Cause } \mid \mathbf{e}) & =\alpha \sum_{\mathbf{y}} \mathbf{P}(\text { Cause }) \mathbf{P}(\mathbf{y} \mid \text { Cause })\left(\prod_{j} \mathbf{P}\left(e_{j} \mid \text { Cause }\right)\right) \\ & =\alpha \mathbf{P}(\text { Cause })\left(\prod_{j} \mathbf{P}\left(e_{j} \mid \text { Cause }\right)\right) \sum_{\mathbf{y}} \mathbf{P}(\mathbf{y} \mid \text { Cause }) \\ & =\alpha \mathbf{P}(\text { Cause }) \prod_{j} \mathbf{P}\left(e_{j} \mid \text { Cause }\right)
\end{aligned}
$$

对于每一个可能的原因，将原因的先验概率诚意在给定原因时所观测到的结果的条件概率，然后将结果归一化。

计算复杂度于观测到的结果数量呈线性关系？不依赖于为观测到的结果数量。
